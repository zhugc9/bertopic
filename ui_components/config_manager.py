"""
é…ç½®ç®¡ç†ç»„ä»¶ - SOTAå®ç°
======================
è´Ÿè´£å‚æ•°é…ç½®å’Œé¢„è®¾ç®¡ç†ï¼Œæ ¸å¿ƒæ™ºèƒ½è°ƒå‚åŠŸèƒ½
"""

import streamlit as st
import pandas as pd
import time
from pathlib import Path
from typing import Dict, Any, List
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥å¯¼å…¥topic_analyzeræ¨¡å—
current_dir = Path(__file__).parent.parent
sys.path.insert(0, str(current_dir))


class ConfigManager:
    """é…ç½®ç®¡ç†å™¨ - ç®€æ´é«˜æ•ˆ"""
    
    def __init__(self):
        self.presets = {
            "æ™ºèƒ½æ¨è": {
                "name": "æ™ºèƒ½æ¨è ğŸ¤–",
                "description": "åŸºäºæ‚¨çš„æ•°æ®è‡ªåŠ¨åˆ†æå¹¶ä¼˜åŒ–å‚æ•°",
                "auto_adjust": True,
                "requires_analysis": True
            },
            "æ‰‹åŠ¨é…ç½®": {
                "name": "æ‰‹åŠ¨é…ç½® âš™ï¸",
                "description": "è‡ªä¸»è°ƒèŠ‚æ‰€æœ‰å‚æ•°",
                "manual_config": True
            },
            "æ–°æ‰‹å‹å¥½": {
                "name": "æ–°æ‰‹å‹å¥½ ğŸŒ±", 
                "description": "ä¿å®ˆç¨³å®šçš„å‚æ•°",
                "params": {
                    "min_topic_size": 20,
                    "nr_topics": "auto",
                    "n_gram_range": [1, 2],
                    "umap_params": {"n_neighbors": 15, "n_components": 5},
                    "hdbscan_params": {"min_cluster_size": 20, "min_samples": 5}
                }
            },
            "ç»†ç²’åº¦åˆ†æ": {
                "name": "ç»†ç²’åº¦åˆ†æ ğŸ”",
                "description": "å‘ç°æ›´å¤šç»†èŠ‚ä¸»é¢˜",
                "params": {
                    "min_topic_size": 10,
                    "nr_topics": "auto",
                    "n_gram_range": [1, 3],
                    "umap_params": {"n_neighbors": 10, "n_components": 4},
                    "hdbscan_params": {"min_cluster_size": 10, "min_samples": 3}
                }
            }
        }
    
    def render_config_selector(self) -> str:
        """æ¸²æŸ“é…ç½®é€‰æ‹©å™¨"""
        st.markdown("#### âš™ï¸ åˆ†ææ¨¡å¼é€‰æ‹©")
        
        preset_names = list(self.presets.keys())
        current = st.session_state.get('current_preset', 'æ™ºèƒ½æ¨è')
        
        selected = st.selectbox(
            "é€‰æ‹©åˆ†æç±»å‹",
            preset_names,
            index=preset_names.index(current) if current in preset_names else 0,
            help="é€‰æ‹©é€‚åˆæ‚¨æ•°æ®çš„åˆ†ææ¨¡å¼"
        )
        
        # æ˜¾ç¤ºè¯´æ˜
        preset_info = self.presets[selected]
        st.info(f"ğŸ“ {preset_info['description']}")
        
        # æ ¹æ®é€‰æ‹©çš„æ¨¡å¼æ˜¾ç¤ºå¯¹åº”ç•Œé¢
        if selected == "æ™ºèƒ½æ¨è":
            self._render_smart_analysis()
        elif selected == "æ‰‹åŠ¨é…ç½®":
            self._render_manual_config()
        
        # æ›´æ–°session_state
        st.session_state.current_preset = selected
        
        return selected
    
    def get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'data_processing': {
                'text_column': 'Unit_Text',
                'merge_strategy': 'concat',
                'metadata_columns': [
                    # åŸºç¡€åˆ—ï¼ˆä¸¤ç§æ ¼å¼éƒ½å¯èƒ½æœ‰ï¼‰
                    'åºå·', 'æ—¥æœŸ', 'æ ‡é¢˜', 'é“¾æ¥', 'Tokenæ•°', 'text', 'tokenæ•°', 
                    'Unit_ID', 'Source', 'Macro_Chunk_ID', 'speaker', 'Unit_Text',
                    'seed_sentence', 'expansion_logic', 'Unit_Hash', 'processing_status', 
                    'Incident', 'Valence',
                    
                    # Frameåˆ†æåˆ—ï¼ˆä¸¤ç§æ ¼å¼ç•¥æœ‰ä¸åŒï¼‰
                    'Frame_ProblemDefinition', 'Frame_ProblemDefinition_Present',
                    'Frame_ResponsibilityAttribution', 'Frame_ResponsibilityAttribution_Present',
                    'Frame_MoralEvaluation', 'Frame_MoralEvaluation_Present',
                    'Frame_SolutionRecommendation', 'Frame_TreatmentRecommendation_Present',
                    'Frame_ActionStatement', 'Frame_ConflictAttribution_Present',
                    'Frame_CausalExplanation', 'Frame_CausalInterpretation_Present',
                    'Frame_HumanInterest_Present', 'Frame_EconomicConsequences_Present',
                    
                    # åˆ†æç»´åº¦åˆ—
                    'Evidence_Type', 'Attribution_Level', 'Temporal_Focus', 
                    'Primary_Actor_Type', 'Geographic_Scope', 'Relationship_Model_Definition', 
                    'Discourse_Type'
                ]
            },
            'bertopic_params': {
                'language': 'multilingual',
                'embedding_model': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                'min_topic_size': 15,
                'nr_topics': 'auto',
                'n_gram_range': [1, 2],
                'umap_params': {
                    'n_neighbors': 15,
                    'n_components': 5,
                    'min_dist': 0.0,
                    'metric': 'cosine',
                    'random_state': 42
                },
                'hdbscan_params': {
                    'min_cluster_size': 15,
                    'min_samples': 5,
                    'metric': 'euclidean',
                    'cluster_selection_method': 'eom',
                    'prediction_data': True
                },
                'expert_keyword_extraction': {
                    'enable_pos_patterns': True,
                    'pos_patterns': {
                        'zh': '<n.*|a.*>*<n.*>+',
                        'en': '<JJ.*>*<NN.*>+',
                        'ru': '<A.*>*<N.*>+'
                    },
                    'custom_stopwords_path': 'stopwords/politics_stopwords.txt',
                    'use_custom_stopwords': True,
                    'pos_language_detection': True
                }
            },
            'data_paths': {
                'media_data': '',
                'social_media_data': ''
            },
            'results_paths': {
                'output_dir': 'results',
                'model_dir': 'results/trained_model',
                'summary_file': 'results/topics_summary.csv',
                'summary_enhanced': 'results/topics_summary_enhanced.csv',
                'cross_lingual_file': 'results/cross_lingual_composition.csv',
                'evolution_file': 'results/dynamic_evolution_analysis.csv',
                'viz_file': 'results/topic_visualization.html',
                'source_analysis': 'results/topic_by_source.html',
                'timeline_analysis': 'results/topics_over_time.html',
                'frame_heatmap': 'results/topic_frame_heatmap.html',
                'academic_charts': {
                    'topic_distribution': 'results/academic_topic_distribution.png',
                    'topic_sizes': 'results/academic_topic_sizes.png',
                    'topic_evolution': 'results/academic_topic_evolution.png',
                    'cross_lingual': 'results/academic_cross_lingual.png'
                }
            },
            'visualization': {
                'figsize': [12, 8],
                'dpi': 100,
                'style': 'plotly',
                'color_scheme': 'viridis',
                'save_format': 'html'
            },
            'analysis': {
                'time_analysis': {
                    'enable': True,
                    'time_column': 'æ—¥æœŸ',
                    'bins': 10
                },
                'source_analysis': {
                    'enable': True,
                    'source_column': 'Source'
                },
                'frame_analysis': {
                    'enable': True,
                    'threshold': 0.1
                }
            },
            'system': {
                'random_seed': 42,
                'verbose': True,
                'save_intermediate': False,
                'use_gpu': False
            }
        }
    
    def apply_preset(self, preset_name: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """åº”ç”¨é¢„è®¾å‚æ•° - åŸºäºç°æœ‰æ¥å£"""
        if preset_name not in self.presets:
            return config
            
        preset = self.presets[preset_name]
        
        # æ™ºèƒ½æ¨èï¼šåº”ç”¨æ™ºèƒ½åˆ†æç»“æœ
        if preset_name == "æ™ºèƒ½æ¨è" and hasattr(st.session_state, 'smart_params'):
            smart_params = st.session_state.smart_params
            # è½¬æ¢IntelligentTunerçš„æ‰å¹³ç»“æ„ä¸ºconfigéœ€è¦çš„åµŒå¥—ç»“æ„
            converted_params = self._convert_smart_params(smart_params)
            # æ·±åº¦åˆå¹¶å‚æ•°ï¼Œç¡®ä¿åµŒå¥—å­—å…¸æ­£ç¡®åˆå¹¶
            self._deep_merge_params(config['bertopic_params'], converted_params)
            
        # æ‰‹åŠ¨é…ç½®ï¼šåº”ç”¨ç”¨æˆ·è®¾ç½®
        elif preset_name == "æ‰‹åŠ¨é…ç½®" and hasattr(st.session_state, 'manual_params'):
            manual_params = st.session_state.manual_params
            config['bertopic_params'].update(manual_params)
            
        # é¢„è®¾æ¨¡å¼ï¼šåº”ç”¨å›ºå®šå‚æ•°
        elif 'params' in preset:
            config['bertopic_params'].update(preset['params'])
        
        return config
    
    def _render_smart_analysis(self):
        """æ¸²æŸ“æ™ºèƒ½åˆ†æç•Œé¢ - KISSåŸåˆ™"""
        st.markdown("##### ğŸ¤– æ™ºèƒ½å‚æ•°åˆ†æ")
        
        if not st.session_state.get('uploaded_files'):
            st.warning("â³ è¯·å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶")
            return
        
        # ç¬¬1æ­¥ï¼šè®©ç”¨æˆ·é€‰æ‹©æ–‡æœ¬åˆ—
        text_column = self._select_text_column()
        if not text_column:
            return
        
        # ç¬¬2æ­¥ï¼šåˆ†ææŒ‰é’®
        col1, col2 = st.columns([3, 1])
        with col1:
            if st.button("ğŸ” åˆ†ææˆ‘çš„æ•°æ®", type="primary", use_container_width=True):
                self._execute_smart_analysis(text_column)
        with col2:
            if hasattr(st.session_state, 'smart_analysis_results'):
                st.success("âœ… å®Œæˆ")
        
        # ç¬¬3æ­¥ï¼šæ˜¾ç¤ºç»“æœ
        if hasattr(st.session_state, 'smart_analysis_results'):
            self._display_smart_results()
    
    def _render_manual_config(self):
        """æ¸²æŸ“æ‰‹åŠ¨é…ç½®ç•Œé¢ - è¯¦ç»†æŒ‡å¯¼ç‰ˆ"""
        st.markdown("##### âš™ï¸ æ‰‹åŠ¨å‚æ•°é…ç½®")
        st.info("ğŸ’¡ æ ¹æ®æ‚¨çš„æ•°æ®ç‰¹ç‚¹è°ƒæ•´å‚æ•°ã€‚ä¸ç¡®å®šæ—¶å¯ä»¥å…ˆç”¨æ™ºèƒ½æ¨èä½œä¸ºå‚è€ƒã€‚")
        
        # è·å–æ•°æ®è§„æ¨¡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        total_docs = 0
        if st.session_state.get('uploaded_files'):
            try:
                for file_path in st.session_state.uploaded_files.values():
                    df = pd.read_excel(file_path)
                    total_docs += len(df)
            except:
                pass
        
        # 1. æ ¸å¿ƒä¸»é¢˜å‚æ•°
        with st.expander("ğŸ¯ æ ¸å¿ƒä¸»é¢˜å‚æ•°", expanded=True):
            st.markdown("**æ§åˆ¶ä¸»é¢˜æ•°é‡å’Œè´¨é‡çš„å…³é”®å‚æ•°**")
            
            col1, col2 = st.columns([3, 2])
            with col1:
                min_topic_size = st.slider(
                    "æœ€å°ä¸»é¢˜å¤§å°", 5, 100, 15,
                    help="æ¯ä¸ªä¸»é¢˜è‡³å°‘åŒ…å«çš„æ–‡æ¡£æ•°ã€‚å€¼è¶Šå°=ä¸»é¢˜è¶Šå¤šè¶Šç»†è‡´ï¼Œå€¼è¶Šå¤§=ä¸»é¢˜è¶Šå°‘è¶Šå®è§‚"
                )
            with col2:
                # åŠ¨æ€è®¡ç®—å»ºè®®å€¼
                if total_docs > 0:
                    suggested_min = max(5, int(total_docs * 0.01))
                    suggested_max = max(10, int(total_docs * 0.03))
                    st.info(f"ğŸ“Š æ‚¨çš„æ•°æ®({total_docs}æ¡)å»ºè®®å€¼: {suggested_min}-{suggested_max}")
                else:
                    st.info("ğŸ“Š ä¸€èˆ¬å»ºè®®: æ–‡æ¡£æ•°Ã—1-3%")
            
            # å‚æ•°æ•ˆæœè¯´æ˜
            st.markdown("**ğŸ“ˆ è°ƒèŠ‚æ•ˆæœ:**")
            if min_topic_size <= 8:
                st.warning("âš ï¸ å¾ˆå°(â‰¤8): ä¼šäº§ç”Ÿå¾ˆå¤šç»†ç¢ä¸»é¢˜ï¼Œå¯èƒ½æœ‰å™ªéŸ³")
            elif min_topic_size <= 15:
                st.success("âœ… å°(9-15): äº§ç”Ÿè¾ƒå¤šç»†è‡´ä¸»é¢˜ï¼Œé€‚åˆè¯¦ç»†åˆ†æ")
            elif min_topic_size <= 30:
                st.info("â„¹ï¸ ä¸­ç­‰(16-30): äº§ç”Ÿé€‚ä¸­æ•°é‡ä¸»é¢˜ï¼Œå¹³è¡¡ç»†èŠ‚ä¸æ¦‚æ‹¬")
            else:
                st.info("ğŸ”µ å¤§(>30): äº§ç”Ÿè¾ƒå°‘å®è§‚ä¸»é¢˜ï¼Œé€‚åˆé«˜å±‚æ¬¡æ¦‚è§ˆ")
        
        # 2. UMAPé™ç»´å‚æ•°
        with st.expander("ğŸ”„ UMAPé™ç»´å‚æ•°", expanded=True):
            st.markdown("**æ§åˆ¶æ–‡æ¡£åœ¨ä½ç»´ç©ºé—´ä¸­çš„åˆ†å¸ƒç»“æ„**")
            
            col1, col2 = st.columns(2)
            with col1:
                n_neighbors = st.slider(
                    "é‚»å±…æ•°é‡", 5, 50, 15,
                    help="æ¯ä¸ªç‚¹è€ƒè™‘å¤šå°‘ä¸ªé‚»å±…ã€‚å½±å“å±€éƒ¨vså…¨å±€ç»“æ„çš„å¹³è¡¡"
                )
                st.markdown("**ğŸ”— é‚»å±…æ•°æ•ˆæœ:**")
                if n_neighbors <= 10:
                    st.info("ğŸ” å°‘é‚»å±…(5-10): ä¿æŒå±€éƒ¨ç»†èŠ‚ï¼Œä½†å¯èƒ½è¿‡åº¦åˆ†å‰²")
                elif n_neighbors <= 20:
                    st.success("âš–ï¸ ä¸­ç­‰é‚»å±…(11-20): å¹³è¡¡å±€éƒ¨ä¸å…¨å±€ç»“æ„")
                else:
                    st.info("ğŸŒ å¤šé‚»å±…(21-50): å¼ºè°ƒå…¨å±€ç»“æ„ï¼Œæ›´ç¨³å®šä½†å¯èƒ½ä¸¢å¤±ç»†èŠ‚")
                    
            with col2:
                n_components = st.slider(
                    "é™ç»´ç»´åº¦", 2, 10, 5,
                    help="é™ç»´åçš„ç»´åº¦æ•°ã€‚å½±å“ä¿¡æ¯ä¿ç•™ç¨‹åº¦"
                )
                st.markdown("**ğŸ“ ç»´åº¦æ•ˆæœ:**")
                if n_components <= 3:
                    st.warning("âš ï¸ ä½ç»´(2-3): è®¡ç®—å¿«ä½†ä¿¡æ¯æŸå¤±å¤š")
                elif n_components <= 6:
                    st.success("âœ… ä¸­ç»´(4-6): å¹³è¡¡æ€§èƒ½ä¸ä¿¡æ¯ä¿ç•™")
                else:
                    st.info("ğŸ”µ é«˜ç»´(7-10): ä¿ç•™æ›´å¤šä¿¡æ¯ä½†è®¡ç®—æ…¢")
        
        # 3. HDBSCANèšç±»å‚æ•°
        with st.expander("ğŸ¯ HDBSCANèšç±»å‚æ•°", expanded=True):
            st.markdown("**æ§åˆ¶å¦‚ä½•å°†ç›¸ä¼¼æ–‡æ¡£èšé›†æˆä¸»é¢˜**")
            
            col1, col2 = st.columns(2)
            with col1:
                min_samples = st.slider(
                    "æœ€å°æ ·æœ¬æ•°", 1, 30, 5,
                    help="å½¢æˆèšç±»æ ¸å¿ƒéœ€è¦çš„æœ€å°‘ç‚¹æ•°ã€‚å½±å“èšç±»ä¸¥æ ¼ç¨‹åº¦"
                )
                st.markdown("**ğŸ¯ æ ·æœ¬æ•°æ•ˆæœ:**")
                if min_samples <= 3:
                    st.warning("âš ï¸ å®½æ¾(1-3): å®¹æ˜“å½¢æˆèšç±»ï¼Œä½†å¯èƒ½åŒ…å«å™ªéŸ³")
                elif min_samples <= 7:
                    st.success("âœ… é€‚ä¸­(4-7): å¹³è¡¡èšç±»è´¨é‡ä¸æ•°é‡")
                else:
                    st.info("ğŸ”µ ä¸¥æ ¼(8+): é«˜è´¨é‡èšç±»ï¼Œä½†å¯èƒ½é—æ¼è¾¹ç¼˜æ–‡æ¡£")
                    
            with col2:
                # æ˜¾ç¤ºæ¨èå…³ç³»
                recommended_samples = max(1, min_topic_size // 3)
                st.info(f"ğŸ’¡ æ¨èå€¼: {recommended_samples} (ä¸»é¢˜å¤§å°Ã·3)")
                st.markdown("**ğŸ”— å‚æ•°å…³ç³»:**")
                st.write(f"â€¢ æœ€å°æ ·æœ¬æ•°é€šå¸¸è®¾ä¸ºä¸»é¢˜å¤§å°çš„1/3")
                st.write(f"â€¢ å½“å‰è®¾ç½®: {min_samples} vs æ¨è {recommended_samples}")
                
        # 4. é«˜çº§å‚æ•°
        with st.expander("ğŸ”§ é«˜çº§å‚æ•°", expanded=False):
            st.markdown("**é«˜çº§ç”¨æˆ·å¯è°ƒèŠ‚çš„é¢å¤–å‚æ•°**")
            
            col1, col2 = st.columns(2)
            with col1:
                nr_topics_option = st.selectbox(
                    "ç›®æ ‡ä¸»é¢˜æ•°é‡",
                    ["auto", "10", "15", "20", "25", "30", "35", "40"],
                    help="auto=è‡ªåŠ¨ç¡®å®šï¼Œæˆ–æ‰‹åŠ¨æŒ‡å®šä¸»é¢˜æ•°é‡"
                )
                nr_topics = "auto" if nr_topics_option == "auto" else int(nr_topics_option)
                
                n_gram_min = st.selectbox("N-gramæœ€å°å€¼", [1, 2], index=0, help="å•è¯ç»„åˆçš„æœ€å°é•¿åº¦")
                
            with col2:
                n_gram_max = st.selectbox("N-gramæœ€å¤§å€¼", [1, 2, 3], index=1, help="å•è¯ç»„åˆçš„æœ€å¤§é•¿åº¦")
                
                # æ˜¾ç¤ºN-gramæ•ˆæœ
                if n_gram_min == 1 and n_gram_max == 1:
                    st.info("ğŸ”¤ å•è¯æ¨¡å¼: åªè€ƒè™‘å•ä¸ªè¯æ±‡")
                elif n_gram_min == 1 and n_gram_max == 2:
                    st.success("âœ… æ ‡å‡†æ¨¡å¼: å•è¯+åŒè¯ç»„åˆ(æ¨è)")
                elif n_gram_min == 1 and n_gram_max == 3:
                    st.info("ğŸ“š ä¸°å¯Œæ¨¡å¼: åŒ…å«ä¸‰è¯çŸ­è¯­(å­¦æœ¯æ–‡æœ¬)")
                else:
                    st.info("ğŸ”§ è‡ªå®šä¹‰æ¨¡å¼")
        
        # 5. å½“å‰å‚æ•°æ€»ç»“
        with st.expander("ğŸ“‹ å½“å‰å‚æ•°æ€»ç»“", expanded=False):
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**ğŸ¯ ä¸»é¢˜æ§åˆ¶:**")
                st.write(f"â€¢ æœ€å°ä¸»é¢˜å¤§å°: {min_topic_size}")
                st.write(f"â€¢ ç›®æ ‡ä¸»é¢˜æ•°: {nr_topics}")
                st.write(f"â€¢ N-gramèŒƒå›´: [{n_gram_min}, {n_gram_max}]")
                
            with col2:
                st.markdown("**âš™ï¸ ç®—æ³•å‚æ•°:**")
                st.write(f"â€¢ UMAPé‚»å±…æ•°: {n_neighbors}")
                st.write(f"â€¢ é™ç»´ç»´åº¦: {n_components}")
                st.write(f"â€¢ æœ€å°æ ·æœ¬æ•°: {min_samples}")
                
            # å‚æ•°åˆç†æ€§æ£€æŸ¥
            st.markdown("**ğŸ” å‚æ•°æ£€æŸ¥:**")
            warnings = []
            if min_samples > min_topic_size:
                warnings.append("âš ï¸ æœ€å°æ ·æœ¬æ•°å¤§äºä¸»é¢˜å¤§å°ï¼Œå¯èƒ½å¯¼è‡´æ— æ³•å½¢æˆä¸»é¢˜")
            if n_neighbors > 30 and total_docs < 1000:
                warnings.append("âš ï¸ å°æ•°æ®é›†ä½¿ç”¨è¿‡å¤šé‚»å±…å¯èƒ½å¯¼è‡´è¿‡åº¦å¹³æ»‘")
            if min_topic_size < 5:
                warnings.append("âš ï¸ ä¸»é¢˜å¤§å°è¿‡å°å¯èƒ½äº§ç”Ÿå™ªéŸ³ä¸»é¢˜")
                
            if warnings:
                for warning in warnings:
                    st.warning(warning)
            else:
                st.success("âœ… å‚æ•°è®¾ç½®åˆç†")
        
        # ä¿å­˜å®Œæ•´çš„æ‰‹åŠ¨é…ç½®
        manual_params = {
            'min_topic_size': min_topic_size,
            'nr_topics': nr_topics,
            'n_gram_range': [n_gram_min, n_gram_max],
            'umap_params': {
                'n_neighbors': n_neighbors,
                'n_components': n_components,
                'min_dist': 0.0,
                'metric': 'cosine',
                'random_state': 42
            },
            'hdbscan_params': {
                'min_cluster_size': min_topic_size,
                'min_samples': min_samples,
                'metric': 'euclidean',
                'cluster_selection_method': 'eom',
                'prediction_data': True
            }
        }
        
        st.session_state.manual_params = manual_params
    
    def _select_text_column(self) -> str:
        """è®©ç”¨æˆ·é€‰æ‹©æ–‡æœ¬åˆ— - KISSåŸåˆ™"""
        try:
            # è·å–ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„åˆ—å
            first_file = list(st.session_state.uploaded_files.values())[0]
            df = pd.read_excel(first_file)
            
            columns = df.columns.tolist()
            
            # æ™ºèƒ½æ¨èæ–‡æœ¬åˆ— - åŸºäºç”¨æˆ·åé¦ˆä¼˜åŒ–
            text_candidates = ['Unit_Text', 'text', 'Text', 'Incident', 'Content', 'æ–‡æœ¬']
            suggested = None
            for candidate in text_candidates:
                if candidate in columns:
                    suggested = candidate
                    break
            
            # ç”¨æˆ·é€‰æ‹©
            selected_col = st.selectbox(
                "é€‰æ‹©æ–‡æœ¬åˆ—",
                columns,
                index=columns.index(suggested) if suggested else 0,
                help="é€‰æ‹©åŒ…å«è¦åˆ†ææ–‡æœ¬çš„åˆ—"
            )
            
            return selected_col
            
        except Exception as e:
            st.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def _execute_smart_analysis(self, text_column: str):
        """æ‰§è¡Œæ™ºèƒ½åˆ†æ - è°ƒç”¨ç°æœ‰IntelligentTuneræ¥å£"""
        progress_bar = st.progress(0)
        status_text = st.empty()
        log_text = st.empty()
        
        try:
            # æ­¥éª¤1: åŠ è½½æ•°æ®
            status_text.info("ğŸ“ åŠ è½½æ•°æ®...")
            progress_bar.progress(0.2)
            
            documents = self._load_documents(text_column)
            log_text.success(f"âœ“ åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
            
            if not documents:
                st.error("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æœ¬æ•°æ®")
                return
            
            # æ­¥éª¤2: æ™ºèƒ½åˆ†æ
            status_text.info("ğŸ” åˆ†ææ–‡æœ¬ç‰¹å¾...")
            progress_bar.progress(0.5)
            
            # ä½¿ç”¨ç°æœ‰çš„IntelligentTuneræ¥å£
            from topic_analyzer.intelligent_tuner import IntelligentTuner
            tuner = IntelligentTuner()
            
            # æ™ºèƒ½é‡‡æ ·ç­–ç•¥ï¼šä¿æŒåŸå§‹æ•°æ®è§„æ¨¡ç”¨äºå‚æ•°è®¡ç®—ï¼Œä½†é™åˆ¶åˆ†ææ ·æœ¬æå‡é€Ÿåº¦
            total_docs = len(documents)
            # æµ·é‡æ•°æ®æ™ºèƒ½é‡‡æ ·ç­–ç•¥
            if total_docs >= 100000:
                # è¶…å¤§è§„æ¨¡ï¼šåˆ†å±‚é‡‡æ ·5000æ¡
                sample_size = 5000
                step = max(1, total_docs // sample_size)
                sample_docs = [documents[i] for i in range(0, total_docs, step)][:sample_size]
                log_text.info(f"â€¢ è¶…å¤§æ•°æ®é›†æ£€æµ‹ï¼šæ€»è®¡ {total_docs} æ¡ï¼Œåˆ†å±‚é‡‡æ · {len(sample_docs)} æ¡")
            elif total_docs >= 50000:
                # å¤§è§„æ¨¡ï¼šåˆ†å±‚é‡‡æ ·3000æ¡
                sample_size = 3000
                step = max(1, total_docs // sample_size)
                sample_docs = [documents[i] for i in range(0, total_docs, step)][:sample_size]
                log_text.info(f"â€¢ å¤§æ•°æ®é›†æ£€æµ‹ï¼šæ€»è®¡ {total_docs} æ¡ï¼Œåˆ†å±‚é‡‡æ · {len(sample_docs)} æ¡")
            elif total_docs > 10000:
                # ä¸­å¤§è§„æ¨¡ï¼šå‰2000æ¡é‡‡æ ·
                sample_docs = documents[:2000]
                log_text.info(f"â€¢ ä¸­å¤§æ•°æ®é›†æ£€æµ‹ï¼šæ€»è®¡ {total_docs} æ¡ï¼Œé‡‡æ · {len(sample_docs)} æ¡")
            else:
                # ä¸­å°è§„æ¨¡ï¼šå…¨é‡åˆ†æ
                sample_docs = documents
                log_text.info(f"â€¢ å…¨é‡åˆ†æ {len(sample_docs)} ä¸ªæ–‡æ¡£")
            
            # æ‰‹åŠ¨è®¾ç½®çœŸå®æ–‡æ¡£æ•°é‡åˆ°ç‰¹å¾ä¸­ï¼ˆç¡®ä¿å‚æ•°è®¡ç®—åŸºäºçœŸå®è§„æ¨¡ï¼‰
            log_text.info(f"â€¢ åŸºäºçœŸå®æ•°æ®è§„æ¨¡ {total_docs} æ¡è¿›è¡Œå‚æ•°ä¼˜åŒ–")
            
            # æ­¥éª¤3: å‚æ•°ä¼˜åŒ–
            status_text.info("âš™ï¸ ä¼˜åŒ–å‚æ•°...")
            progress_bar.progress(0.8)
            
            # æ‰§è¡Œæ™ºèƒ½åˆ†æ
            try:
                results = tuner.auto_tune(sample_docs)
                log_text.info(f"â€¢ IntelligentTunerè¿”å›ç±»å‹: {type(results)}")
                
                # æ£€æŸ¥è¿”å›ç»“æœç»“æ„
                if not isinstance(results, dict):
                    st.error("âŒ IntelligentTunerè¿”å›ç»“æœæ ¼å¼é”™è¯¯")
                    return
                
                if 'data_features' not in results or 'optimized_parameters' not in results:
                    st.error("âŒ ç¼ºå°‘å¿…è¦çš„åˆ†æç»“æœå­—æ®µ")
                    return
                
                # ä¿®æ­£ç‰¹å¾ä¸­çš„æ–‡æ¡£æ•°é‡ä¸ºçœŸå®æ•°é‡
                results['data_features']['total_docs'] = total_docs
                results['data_features']['sample_size'] = len(sample_docs)
                
                # æ™ºèƒ½ä¿®æ­£ï¼šç¡®ä¿å‚æ•°åŸºäºçœŸå®æ•°æ®è§„æ¨¡ï¼Œä½†ä¿ç•™æ™ºèƒ½ä¼˜åŒ–é€»è¾‘
                if total_docs != len(sample_docs):
                    log_text.info(f"â€¢ åŸºäºçœŸå®è§„æ¨¡ {total_docs} é‡æ–°ä¼˜åŒ–å‚æ•°")
                    # é‡æ–°è¿è¡Œå‚æ•°ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨çœŸå®æ•°æ®è§„æ¨¡
                    from topic_analyzer.intelligent_tuner import ParameterOptimizer
                    optimizer = ParameterOptimizer()
                    corrected_params = optimizer.optimize_parameters(results['data_features'])
                    # æ›´æ–°ä¼˜åŒ–åçš„å‚æ•°
                    results['optimized_parameters'].update(corrected_params)
                
                params = results['optimized_parameters']
                
            except Exception as tune_error:
                log_text.error(f"â€¢ IntelligentTunerè°ƒç”¨å¤±è´¥: {tune_error}")
                st.error("âŒ æ™ºèƒ½åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")
                return
            
            # è°ƒè¯•ä¿¡æ¯ - æ˜¾ç¤ºå®é™…è¿”å›çš„å‚æ•°é”®
            log_text.write(f"â€¢ è¿”å›çš„å‚æ•°é”®: {list(params.keys())}")
            
            # å®Œæˆ
            status_text.success("ğŸ‰ æ™ºèƒ½åˆ†æå®Œæˆï¼")
            progress_bar.progress(1.0)
            
            # ä¿å­˜ç»“æœ
            st.session_state.smart_analysis_results = results
            st.session_state.smart_params = params
            
            log_text.success("âœ… å‚æ•°å·²è‡ªåŠ¨ä¼˜åŒ–")
                
        except Exception as e:
            status_text.error(f"âŒ åˆ†æå¤±è´¥: {e}")
            log_text.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            # æ˜¾ç¤ºæ›´å¤šè°ƒè¯•ä¿¡æ¯
            import traceback
            log_text.text(traceback.format_exc())
    
    def _load_documents(self, text_column: str) -> List[str]:
        """åŠ è½½ç”¨æˆ·æ–‡æ¡£ - KISSåŸåˆ™"""
        documents = []
        
        for file_path in st.session_state.uploaded_files.values():
            try:
                df = pd.read_excel(file_path)
                texts = df[text_column].dropna().astype(str)
                texts = texts[texts.str.len() > 10].tolist()
                documents.extend(texts)
            except Exception:
                continue
        
        return documents
    
    def _display_smart_results(self):
        """æ˜¾ç¤ºæ™ºèƒ½åˆ†æç»“æœ"""
        results = st.session_state.smart_analysis_results
        
        st.markdown("##### ğŸ“Š æ•°æ®åˆ†æç»“æœ")
        
        # æ•°æ®æ¦‚è§ˆ
        features = results['data_features']
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("æ–‡æ¡£æ•°é‡", features.get('total_docs', 0))
        with col2:
            # è¯­è¨€æ˜ å°„
            lang_map = {'zh': 'ä¸­æ–‡', 'en': 'è‹±æ–‡', 'ru': 'ä¿„æ–‡'}
            dominant_lang = features.get('dominant_language', 'unknown')
            display_lang = lang_map.get(dominant_lang, dominant_lang)
            st.metric("ä¸»è¦è¯­è¨€", display_lang)
        with col3:
            complexity_map = {'low': 'ç®€å•', 'medium': 'ä¸­ç­‰', 'high': 'å¤æ‚'}
            complexity = features.get('estimated_complexity', 'unknown')
            display_complexity = complexity_map.get(complexity, complexity)
            st.metric("å¤æ‚åº¦", display_complexity)
        
        # æ¨èå‚æ•°
        st.markdown("##### ğŸ¯ æ¨èå‚æ•°")
        params = results['optimized_parameters']
        
        col1, col2 = st.columns(2)
        with col1:
            st.write(f"**æœ€å°ä¸»é¢˜å¤§å°:** {params.get('min_topic_size', 15)}")
            st.write(f"**UMAPé‚»å±…æ•°:** {params.get('umap_n_neighbors', 15)}")
        with col2:
            st.write(f"**é™ç»´ç»´åº¦:** {params.get('umap_n_components', 5)}")
            st.write(f"**æœ€å°æ ·æœ¬æ•°:** {params.get('hdbscan_min_samples', 5)}")
        
        # è¯¦ç»†æ¨èç†ç”± - ç§‘å­¦ä¾æ®è¯´æ˜
        with st.expander("ğŸ’¡ è¯¦ç»†æ¨èç†ç”±", expanded=True):
            self._display_detailed_reasoning(results['data_features'], params)
        
        st.info("ğŸ’¡ è¿™äº›å‚æ•°å°†è‡ªåŠ¨åº”ç”¨åˆ°åˆ†æä¸­")
    
    def _convert_smart_params(self, smart_params: Dict[str, Any]) -> Dict[str, Any]:
        """è½¬æ¢IntelligentTunerçš„æ‰å¹³å‚æ•°ç»“æ„ä¸ºconfigéœ€è¦çš„åµŒå¥—ç»“æ„"""
        converted = {}
        
        # åŸºç¡€å‚æ•°
        if 'min_topic_size' in smart_params:
            converted['min_topic_size'] = smart_params['min_topic_size']
        if 'nr_topics' in smart_params:
            converted['nr_topics'] = smart_params['nr_topics']
        if 'n_gram_range' in smart_params:
            converted['n_gram_range'] = smart_params['n_gram_range']
        
        # UMAPå‚æ•° - ä¿ç•™é»˜è®¤å€¼å¹¶è¦†ç›–æ™ºèƒ½è°ƒå‚ç»“æœ
        umap_params = {
            'min_dist': 0.0,
            'metric': 'cosine', 
            'random_state': 42
        }
        if 'umap_n_neighbors' in smart_params:
            umap_params['n_neighbors'] = smart_params['umap_n_neighbors']
        if 'umap_n_components' in smart_params:
            umap_params['n_components'] = smart_params['umap_n_components']
        converted['umap_params'] = umap_params
        
        # HDBSCANå‚æ•° - ä¿ç•™é»˜è®¤å€¼å¹¶è¦†ç›–æ™ºèƒ½è°ƒå‚ç»“æœ
        hdbscan_params = {
            'metric': 'euclidean',
            'cluster_selection_method': 'eom',
            'prediction_data': True
        }
        if 'hdbscan_min_cluster_size' in smart_params:
            hdbscan_params['min_cluster_size'] = smart_params['hdbscan_min_cluster_size']
        if 'hdbscan_min_samples' in smart_params:
            hdbscan_params['min_samples'] = smart_params['hdbscan_min_samples']
        converted['hdbscan_params'] = hdbscan_params
        
        return converted
    
    def _deep_merge_params(self, target: Dict[str, Any], source: Dict[str, Any]):
        """æ·±åº¦åˆå¹¶å‚æ•°å­—å…¸ï¼Œç¡®ä¿åµŒå¥—å­—å…¸æ­£ç¡®åˆå¹¶"""
        for key, value in source.items():
            if key in target and isinstance(target[key], dict) and isinstance(value, dict):
                # é€’å½’åˆå¹¶åµŒå¥—å­—å…¸
                self._deep_merge_params(target[key], value)
            else:
                # ç›´æ¥èµ‹å€¼
                target[key] = value
    
    def _display_detailed_reasoning(self, features: Dict[str, Any], params: Dict[str, Any]):
        """æ˜¾ç¤ºè¯¦ç»†çš„å‚æ•°æ¨èç†ç”± - ç§‘å­¦ä¾æ®"""
        total_docs = features.get('total_docs', 0)
        complexity = features.get('estimated_complexity', 'medium')
        dominant_lang = features.get('dominant_language', 'unknown')
        avg_length = features.get('avg_text_length', 0)
        vocab_diversity = features.get('vocabulary_diversity', 0)
        
        # 1. ä¸»é¢˜å¤§å°çš„å…·ä½“è®¡ç®—ä¾æ®
        if 'min_topic_size' in params:
            topic_size = params['min_topic_size']
            percentage = (topic_size / total_docs) * 100 if total_docs > 0 else 0
            st.markdown(f"**ğŸ¯ æœ€å°ä¸»é¢˜å¤§å° {topic_size}**")
            st.write(f"   â€¢ è®¡ç®—å…¬å¼ï¼š{total_docs}æ–‡æ¡£ Ã— {percentage:.1f}% = {topic_size}")
            st.write(f"   â€¢ ç§‘å­¦ä¾æ®ï¼š1-5%æ¯”ä¾‹ç¡®ä¿ä¸»é¢˜æœ‰è¶³å¤Ÿæ ·æœ¬ï¼Œé¿å…å™ªéŸ³ä¸»é¢˜")
            st.write("")
        
        # 2. UMAPé‚»å±…æ•°çš„å…·ä½“åŸç†
        if params.get('umap_n_neighbors'):
            neighbors = params['umap_n_neighbors']
            st.markdown(f"**ğŸ”— UMAPé‚»å±…æ•° {neighbors}**")
            if total_docs < 500:
                st.write("   â€¢ å°æ•°æ®é›†(<500)ï¼šç”¨è¾ƒå°‘é‚»å±…ä¿æŒå±€éƒ¨ç»“æ„ç²¾åº¦")
            elif total_docs < 2000:
                st.write("   â€¢ ä¸­ç­‰æ•°æ®é›†(500-2000)ï¼šå¹³è¡¡å±€éƒ¨ç»†èŠ‚ä¸å…¨å±€ç»“æ„")
            else:
                st.write("   â€¢ å¤§æ•°æ®é›†(>2000)ï¼šç”¨æ›´å¤šé‚»å±…æ•è·å…¨å±€æ¨¡å¼")
            st.write("")
        
        # 3. é™ç»´ç»´åº¦çš„é€‰æ‹©ç†ç”±
        if params.get('umap_n_components'):
            components = params['umap_n_components']
            st.markdown(f"**ğŸ“ é™ç»´ç»´åº¦ {components}**")
            if complexity == 'high':
                st.write("   â€¢ é«˜å¤æ‚åº¦æ–‡æœ¬ï¼šéœ€è¦æ›´å¤šç»´åº¦ä¿ç•™è¯­ä¹‰ä¿¡æ¯")
            elif complexity == 'low':
                st.write("   â€¢ ä½å¤æ‚åº¦æ–‡æœ¬ï¼šè¾ƒå°‘ç»´åº¦é¿å…è¿‡æ‹Ÿåˆ")
            else:
                st.write("   â€¢ ä¸­ç­‰å¤æ‚åº¦ï¼šæ ‡å‡†ç»´åº¦å¹³è¡¡æ€§èƒ½ä¸ç²¾åº¦")
            st.write("")
        
        # 4. è¯­è¨€ç‰¹å¾å½±å“
        lang_names = {'zh': 'ä¸­æ–‡', 'en': 'è‹±æ–‡', 'ru': 'ä¿„æ–‡'}
        lang_display = lang_names.get(dominant_lang, dominant_lang)
        st.markdown(f"**ğŸŒ è¯­è¨€ç‰¹å¾ä¼˜åŒ–ï¼ˆ{lang_display}ï¼‰**")
        
        if dominant_lang == 'zh':
            st.write("   â€¢ ä¸­æ–‡è¯æ±‡å¯†åº¦é«˜ï¼ŒN-gram=[1,2]ï¼Œä¸»é¢˜å¤§å°å¯é€‚å½“å‡å°")
        elif dominant_lang == 'ru':
            st.write("   â€¢ ä¿„æ–‡è¯æ±‡å˜åŒ–ä¸°å¯Œï¼ŒN-gram=[1,2]ï¼Œéœ€è¦æ›´å¤šæ ·æœ¬é¿å…ç¢ç‰‡åŒ–")
        elif dominant_lang == 'en':
            st.write("   â€¢ è‹±æ–‡æ”¯æŒçŸ­è¯­ï¼ŒN-gram=[1,3]ï¼Œæ ‡å‡†å‚æ•°è®¾ç½®")
        else:
            st.write(f"   â€¢ {lang_display}æ–‡æœ¬ï¼Œä½¿ç”¨é€šç”¨å‚æ•°é…ç½®")
        st.write("")
        
        # 5. æ•°æ®ç‰¹å¾ç»¼åˆåˆ†æ
        st.markdown("**ğŸ“Š æ•°æ®ç‰¹å¾ç»¼åˆåˆ†æ**")
        if avg_length > 0:
            if avg_length > 200:
                st.write(f"   â€¢ å¹³å‡æ–‡æœ¬é•¿åº¦ {avg_length:.0f}å­—ç¬¦ï¼šé•¿æ–‡æœ¬ä¿¡æ¯ä¸°å¯Œï¼Œå¯ç”¨è¾ƒå°ä¸»é¢˜")
            elif avg_length < 50:
                st.write(f"   â€¢ å¹³å‡æ–‡æœ¬é•¿åº¦ {avg_length:.0f}å­—ç¬¦ï¼šçŸ­æ–‡æœ¬éœ€è¦æ›´å¤§ä¸»é¢˜é¿å…ç¢ç‰‡åŒ–")
            else:
                st.write(f"   â€¢ å¹³å‡æ–‡æœ¬é•¿åº¦ {avg_length:.0f}å­—ç¬¦ï¼šä¸­ç­‰é•¿åº¦ï¼Œä½¿ç”¨æ ‡å‡†å‚æ•°")
        
        if vocab_diversity > 0:
            if vocab_diversity > 0.6:
                st.write(f"   â€¢ è¯æ±‡å¤šæ ·æ€§ {vocab_diversity:.2f}ï¼šé«˜å¤šæ ·æ€§éœ€è¦æ›´ç²¾ç»†çš„ä¸»é¢˜åˆ’åˆ†")
            elif vocab_diversity < 0.3:
                st.write(f"   â€¢ è¯æ±‡å¤šæ ·æ€§ {vocab_diversity:.2f}ï¼šä½å¤šæ ·æ€§ç”¨è¾ƒå¤§ä¸»é¢˜é¿å…é‡å¤")
            else:
                st.write(f"   â€¢ è¯æ±‡å¤šæ ·æ€§ {vocab_diversity:.2f}ï¼šä¸­ç­‰å¤šæ ·æ€§ï¼Œå¹³è¡¡è®¾ç½®")
        
        # 6. æ€»ç»“å»ºè®®
        st.markdown("**ğŸ¯ å‚æ•°è°ƒèŠ‚æ€»ç»“**")
        st.write(f"   â€¢ åŸºäº {total_docs} æ¡{lang_display}æ–‡æœ¬çš„æ·±åº¦åˆ†æ")
        st.write(f"   â€¢ {complexity}å¤æ‚åº¦æ•°æ®çš„ä¸“ä¸šå‚æ•°é…ç½®")
        st.write("   â€¢ æ‰€æœ‰å‚æ•°å‡åŸºäºæ•°æ®é©±åŠ¨çš„ç§‘å­¦è®¡ç®—")
    
