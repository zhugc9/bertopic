"""
åˆ†æè¿è¡Œç»„ä»¶ - SOTAå®ç°
=====================
é«˜æ•ˆçš„åˆ†ææ‰§è¡Œå’Œè¿›åº¦ç®¡ç†
"""

import streamlit as st
import time
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, Any


class AnalysisRunner:
    """åˆ†æè¿è¡Œå™¨ - ç®€æ´é«˜æ•ˆ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.results_dir = Path("results")
        self.results_dir.mkdir(exist_ok=True)
    
    def render_runner_interface(self):
        """æ¸²æŸ“è¿è¡Œç•Œé¢"""
        st.markdown("#### ğŸš€ å¼€å§‹åˆ†æ")
        
        # æ£€æŸ¥å‰ç½®æ¡ä»¶
        if not self._check_ready():
            st.warning("âš ï¸ è¯·å…ˆå®Œæˆå‰é¢çš„æ­¥éª¤")
            return
        
        # æ˜¾ç¤ºé…ç½®æ‘˜è¦
        self._show_config_summary()
        
        # è¿è¡ŒæŒ‰é’®
        if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary", use_container_width=True):
            self._run_analysis()
        
        # æ˜¾ç¤ºçŠ¶æ€
        self._show_status()
    
    def _check_ready(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å‡†å¤‡å°±ç»ª"""
        return (
            'uploaded_files' in st.session_state and 
            len(st.session_state.uploaded_files) > 0 and
            'current_preset' in st.session_state
        )
    
    def _show_config_summary(self):
        """æ˜¾ç¤ºé…ç½®æ‘˜è¦"""
        files_count = len(st.session_state.get('uploaded_files', {}))
        preset = st.session_state.get('current_preset', 'æœªè®¾ç½®')
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("æ•°æ®æ–‡ä»¶", f"{files_count} ä¸ª")
        with col2:
            st.metric("åˆ†ææ¨¡å¼", preset)
    
    def _run_analysis(self):
        """è¿è¡Œåˆ†æ"""
        # åˆå§‹åŒ–çŠ¶æ€
        st.session_state.analysis_running = True
        st.session_state.analysis_progress = 0
        
        # åˆ›å»ºè¿›åº¦æ˜¾ç¤º
        progress_container = st.empty()
        
        try:
            with progress_container.container():
                st.info("ğŸš€ åˆ†æå¼€å§‹...")
                progress_bar = st.progress(0)
            
            self._execute_analysis(progress_container, progress_bar)
            
        except Exception as e:
            st.error(f"âŒ åˆ†æå¤±è´¥: {e}")
        finally:
            st.session_state.analysis_running = False
    
    def _execute_analysis(self, progress_container, progress_bar):
        """æ‰§è¡Œåˆ†ææµç¨‹ - KISSåŸåˆ™ç²¾ç®€ç‰ˆ"""
        status_text = st.empty()
        
        try:
            # æ­¥éª¤1: åŠ è½½æ•°æ®
            status_text.info("ğŸ“ åŠ è½½æ•°æ®...")
            progress_bar.progress(0.3)
            self._load_data()
            
            # æ­¥éª¤2: è®­ç»ƒæ¨¡å‹
            status_text.info("ğŸ¤– è®­ç»ƒæ¨¡å‹...")
            progress_bar.progress(0.7)
            self._train_model()
            
            # æ­¥éª¤3: ç”Ÿæˆç»“æœ
            status_text.info("ğŸ“Š ç”Ÿæˆç»“æœ...")
            progress_bar.progress(0.9)
            self._generate_results()
            
            # å®Œæˆ
            status_text.success("ğŸ‰ åˆ†æå®Œæˆï¼")
            progress_bar.progress(1.0)
            st.balloons()
                
        except Exception as e:
            status_text.error(f"âŒ åˆ†æå¤±è´¥: {e}")
            st.write(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
    
    def _load_data(self):
        """åŠ è½½æ•°æ® - å®æ—¶åé¦ˆç‰ˆ"""
        from topic_analyzer.data_loader import DataLoader
        
        # å®æ—¶æ˜¾ç¤ºè¿›åº¦
        st.write("â€¢ é…ç½®æ•°æ®è·¯å¾„...")
        # ç¡®ä¿data_pathså­˜åœ¨
        if 'data_paths' not in self.config:
            self.config['data_paths'] = {}
        
        # è®¾ç½®ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„ï¼Œç¡®ä¿æœªä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„ä¸ä¼šå¯¼è‡´é”™è¯¯
        uploaded_files = st.session_state.get('uploaded_files', {})
        if 'media_data' in uploaded_files:
            self.config['data_paths']['media_data'] = uploaded_files['media_data']
            st.write(f"  âœ“ ä¸»æ•°æ®æ–‡ä»¶: {uploaded_files['media_data']}")
        else:
            # å¦‚æœæ²¡æœ‰media_dataï¼Œå°†å…¶è®¾ä¸ºç©ºï¼ŒDataLoaderä¼šè·³è¿‡
            self.config['data_paths']['media_data'] = ''
            
        if 'social_media_data' in uploaded_files:
            self.config['data_paths']['social_media_data'] = uploaded_files['social_media_data']
            st.write(f"  âœ“ è¡¥å……æ•°æ®æ–‡ä»¶: {uploaded_files['social_media_data']}")
        else:
            # å¦‚æœæ²¡æœ‰social_media_dataï¼Œå°†å…¶è®¾ä¸ºç©ºï¼ŒDataLoaderä¼šè·³è¿‡
            self.config['data_paths']['social_media_data'] = ''
        
        st.write("â€¢ ä¿å­˜ä¸´æ—¶é…ç½®...")
        temp_config = self.results_dir / "temp_config.yaml"
        with open(temp_config, 'w', encoding='utf-8') as f:
            yaml.dump(self.config, f, default_flow_style=False, allow_unicode=True)
        
        st.write("â€¢ å¼€å§‹åŠ è½½æ•°æ®...")
        # ä¿®å¤ï¼šDataLoaderæœŸæœ›é…ç½®å­—å…¸ï¼Œä¸æ˜¯æ–‡ä»¶è·¯å¾„
        loader = DataLoader(self.config)
        # ä¿®å¤ï¼šæ­£ç¡®è§£åŒ…DataLoaderè¿”å›çš„å…ƒç»„
        self.documents, self.metadata_df = loader.load_and_prepare_data()
        st.write(f"âœ“ åŠ è½½å®Œæˆï¼š{len(self.documents)} ä¸ªæ–‡æ¡£")
    
    def _train_model(self):
        """è®­ç»ƒæ¨¡å‹ - å®æ—¶åé¦ˆç‰ˆ"""
        from topic_analyzer.model import TopicAnalyzer
        
        st.write("â€¢ åˆå§‹åŒ–BERTopicæ¨¡å‹...")
        # ä¿®å¤ï¼šTopicAnalyzeræœŸæœ›é…ç½®å­—å…¸ï¼Œä¸æ˜¯æ–‡ä»¶è·¯å¾„
        analyzer = TopicAnalyzer(self.config)
        
        doc_count = len(self.documents)
        st.write(f"â€¢ å¼€å§‹è®­ç»ƒï¼ˆ{doc_count} ä¸ªæ–‡æ¡£ï¼‰...")
        
        import time
        start_time = time.time()
        
        # å®æ—¶çŠ¶æ€æ›´æ–°å®¹å™¨
        status_container = st.empty()
        
        # æ­¥éª¤1: åˆå§‹åŒ–
        status_container.info("ğŸ“Š æ­¥éª¤1/5: åˆå§‹åŒ–è®­ç»ƒç»„ä»¶...")
        st.write("  â€¢ åŠ è½½åµŒå…¥æ¨¡å‹...")
        time.sleep(1)
        
        # æ­¥éª¤2: æ–‡æ¡£åµŒå…¥
        status_container.info(f"ğŸš€ æ­¥éª¤2/5: ç”Ÿæˆæ–‡æ¡£åµŒå…¥å‘é‡ ({doc_count}ä¸ªæ–‡æ¡£)")
        st.write("  â€¢ æ­£åœ¨å¤„ç†æ–‡æ¡£åµŒå…¥ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
        
        # åˆ›å»ºå®æ—¶ç›‘æ§æ˜¾ç¤º
        progress_placeholder = st.empty()
        
        # æ‰§è¡Œå®é™…è®­ç»ƒ - è¿™é‡Œæ˜¯æœ€è€—æ—¶çš„éƒ¨åˆ†
        import threading
        training_complete = threading.Event()
        training_error = [None]  # ç”¨åˆ—è¡¨å­˜å‚¨å¼‚å¸¸ï¼Œå› ä¸ºçº¿ç¨‹æ— æ³•ç›´æ¥ä¿®æ”¹å¤–éƒ¨å˜é‡
        
        def training_task():
            """è®­ç»ƒä»»åŠ¡çº¿ç¨‹"""
            try:
                self.topic_model, self.topics = analyzer.train_bertopic_model(self.documents)
                training_complete.set()
            except Exception as e:
                training_error[0] = e
                training_complete.set()
        
        def progress_monitor():
            """è¿›åº¦ç›‘æ§çº¿ç¨‹ - æ¯åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡"""
            minutes_elapsed = 0
            while not training_complete.is_set():
                if minutes_elapsed == 0:
                    progress_placeholder.info("ğŸ”„ è®­ç»ƒè¿›è¡Œä¸­... (åˆšå¼€å§‹)")
                else:
                    progress_placeholder.info(f"ğŸ”„ è®­ç»ƒè¿›è¡Œä¸­... (å·²è¿è¡Œ {minutes_elapsed} åˆ†é’Ÿ)")
                
                # ç­‰å¾…60ç§’æˆ–è®­ç»ƒå®Œæˆ
                training_complete.wait(timeout=60)
                minutes_elapsed += 1
        
        # å¯åŠ¨è®­ç»ƒå’Œç›‘æ§çº¿ç¨‹
        training_thread = threading.Thread(target=training_task)
        monitor_thread = threading.Thread(target=progress_monitor)
        
        training_thread.daemon = True
        monitor_thread.daemon = True
        
        training_thread.start()
        monitor_thread.start()
        
        # ç­‰å¾…è®­ç»ƒå®Œæˆ
        training_thread.join()
        
        # æ£€æŸ¥è®­ç»ƒç»“æœ
        if training_error[0]:
            if isinstance(training_error[0], KeyboardInterrupt):
                status_container.warning("âš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
                st.warning("è®­ç»ƒå·²ä¸­æ–­ï¼Œæ²¡æœ‰ç”Ÿæˆå®Œæ•´ç»“æœ")
            else:
                status_container.error(f"âŒ è®­ç»ƒå¤±è´¥: {training_error[0]}")
                st.write(f"é”™è¯¯è¯¦æƒ…: {str(training_error[0])}")
                st.write("ğŸ’¡ æç¤ºï¼šå¯ä»¥æ£€æŸ¥ results/ ç›®å½•æŸ¥çœ‹æ˜¯å¦æœ‰éƒ¨åˆ†ç»“æœæ–‡ä»¶")
            raise training_error[0]
        
        # è®­ç»ƒæˆåŠŸï¼Œæ¸…é™¤è¿›åº¦ç›‘æ§
        progress_placeholder.empty()
        st.write("  â€¢ è®­ç»ƒå®Œæˆï¼Œå‡†å¤‡åç»­å¤„ç†...")
        
        # æ­¥éª¤3: é™ç»´å¤„ç†
        status_container.info("ğŸ”„ æ­¥éª¤3/5: UMAPé™ç»´å¤„ç†å®Œæˆ")
        st.write("  â€¢ é™ç»´å¤„ç†å®Œæˆ")
        time.sleep(0.5)
        
        # æ­¥éª¤4: èšç±»åˆ†æ
        status_container.info("ğŸ¯ æ­¥éª¤4/5: HDBSCANèšç±»åˆ†æå®Œæˆ")
        st.write("  â€¢ èšç±»åˆ†æå®Œæˆ")
        time.sleep(0.5)
        
        # æ­¥éª¤5: ä¸»é¢˜ä¼˜åŒ–
        status_container.info("âœ¨ æ­¥éª¤5/5: ä¸»é¢˜æ ‡ç­¾ç”Ÿæˆå®Œæˆ")
        st.write("  â€¢ ä¸»é¢˜æ ‡ç­¾ç”Ÿæˆå®Œæˆ")
        time.sleep(0.5)
        
        # å®Œæˆç»Ÿè®¡
        end_time = time.time()
        training_time = end_time - start_time
        
        topic_info = self.topic_model.get_topic_info()
        n_topics = len(topic_info) - 1
        
        st.success(f"âœ… è®­ç»ƒå®Œæˆï¼å‘ç° {n_topics} ä¸ªä¸»é¢˜ï¼Œç”¨æ—¶ {training_time:.1f}ç§’")
        
        # è¯¦ç»†ç»Ÿè®¡
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("å‘ç°ä¸»é¢˜", f"{n_topics} ä¸ª")
        with col2:
            st.metric("è®­ç»ƒæ—¶é—´", f"{training_time:.1f} ç§’")
        with col3:
            st.metric("å¤„ç†é€Ÿåº¦", f"{doc_count/training_time:.1f} æ–‡æ¡£/ç§’")
        
        st.write(f"âœ“ æ¨¡å‹è®­ç»ƒæˆåŠŸï¼š{n_topics} ä¸ªä¸»é¢˜")
    
    def _generate_results(self):
        """ç”Ÿæˆç»“æœ - å®æ—¶åé¦ˆç‰ˆ"""
        st.write("â€¢ æå–ä¸»é¢˜ä¿¡æ¯...")
        topic_info = self.topic_model.get_topic_info()
        
        st.write("â€¢ ä¿å­˜ç»“æœæ–‡ä»¶...")
        
        # é¦–å…ˆä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
        try:
            st.write("  â€¢ ä¿å­˜è®­ç»ƒæ¨¡å‹...")
            model_dir = self.results_dir / "trained_model"
            model_dir.mkdir(exist_ok=True)
            
            if hasattr(self, 'topic_model') and self.topic_model is not None:
                # ä½¿ç”¨ä¸åŒçš„ä¿å­˜æ–¹å¼é¿å…æƒé™é—®é¢˜
                import pickle
                model_file = model_dir / "bertopic_model.pkl"
                with open(model_file, 'wb') as f:
                    pickle.dump(self.topic_model, f)
                st.write(f"    âœ“ æ¨¡å‹å·²ä¿å­˜: {model_file}")
            else:
                st.warning("    âš ï¸ æ¨¡å‹å¯¹è±¡ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¿å­˜")
                
        except Exception as save_error:
            st.warning(f"    âš ï¸ æ¨¡å‹ä¿å­˜å¤±è´¥: {save_error}")
            st.write("    â€¢ ç»§ç»­ä¿å­˜å…¶ä»–ç»“æœæ–‡ä»¶...")
        
        # ä¿å­˜ä¸»é¢˜æ‘˜è¦
        summary_file = self.results_dir / "topics_summary.csv"
        topic_info.to_csv(summary_file, index=False, encoding='utf-8-sig')
        st.write(f"  âœ“ ä¸»é¢˜æ‘˜è¦: {summary_file}")
        
        # ä¿å­˜æ–‡æ¡£-ä¸»é¢˜æ˜ å°„
        try:
            import pandas as pd
            
            # ç¡®ä¿æ‰€æœ‰æ•°ç»„é•¿åº¦ä¸€è‡´
            n_docs = len(self.documents)
            n_topics = len(self.topics) if hasattr(self, 'topics') and self.topics is not None else 0
            
            st.write(f"  â€¢ å‡†å¤‡æ–‡æ¡£æ˜ å°„ ({n_docs}ä¸ªæ–‡æ¡£, {n_topics}ä¸ªä¸»é¢˜åˆ†é…)...")
            
            if n_topics > 0 and n_docs == n_topics:
                # ä¿å­˜æ‰€æœ‰æ–‡æ¡£ï¼Œä¸åšé™åˆ¶
                max_docs = n_docs  # ä¿å­˜å…¨éƒ¨æ–‡æ¡£
                
                doc_topic_df = pd.DataFrame({
                    'document_id': range(max_docs),
                    'document_text': [str(doc)[:200] + '...' if len(str(doc)) > 200 else str(doc) 
                                    for doc in self.documents[:max_docs]],  # é™åˆ¶æ–‡æœ¬é•¿åº¦
                    'topic': self.topics[:max_docs],
                    'topic_name': [f"Topic_{topic}" if topic != -1 else 'Outlier' 
                                  for topic in self.topics[:max_docs]]
                })
                
                doc_mapping_file = self.results_dir / "document_topic_mapping.csv"
                doc_topic_df.to_csv(doc_mapping_file, index=False, encoding='utf-8-sig')
                st.write(f"  âœ“ æ–‡æ¡£ä¸»é¢˜æ˜ å°„: {doc_mapping_file} ({max_docs}æ¡è®°å½•)")
            else:
                st.warning(f"  âš ï¸ æ•°æ®é•¿åº¦ä¸åŒ¹é… (æ–‡æ¡£:{n_docs}, ä¸»é¢˜:{n_topics})ï¼Œè·³è¿‡æ˜ å°„ä¿å­˜")
                
        except Exception as mapping_error:
            st.warning(f"  âš ï¸ æ–‡æ¡£æ˜ å°„ä¿å­˜å¤±è´¥: {mapping_error}")
            st.write("  â€¢ ç»§ç»­ä¿å­˜å…¶ä»–æ–‡ä»¶...")
        
        # ========== è°ƒç”¨ä¸“å®¶çº§æ¨¡å— ==========
        st.write("â€¢ è¿è¡Œä¸“å®¶çº§åˆ†æ...")
        
        try:
            from topic_analyzer.model import TopicAnalyzer
            analyzer = TopicAnalyzer(self.config)
            
            # 1. è·¨è¯­è¨€ä¸»é¢˜æˆåˆ†åˆ†æ
            st.write("  â€¢ è·¨è¯­è¨€æˆåˆ†åˆ†æ...")
            # å…ˆæ£€æµ‹æ–‡æ¡£è¯­è¨€
            document_languages = analyzer.cross_lingual_analyzer.analyze_document_languages(self.documents)
            # ç„¶ååˆ†æä¸»é¢˜è¯­è¨€æ„æˆ
            analyzer.cross_lingual_analyzer.analyze_topic_language_composition(
                self.topics, document_languages
            )
            st.write("  âœ“ è·¨è¯­è¨€åˆ†æå®Œæˆ")
            
            # 2. åŠ¨æ€ä¸»é¢˜æ¼”åŒ–åˆ†æ (å¦‚æœæœ‰æ—¥æœŸæ•°æ®)
            if hasattr(self, 'metadata_df') and self.metadata_df is not None:
                date_columns = ['æ—¥æœŸ', 'Date', 'date', 'timestamp']
                date_col = None
                for col in date_columns:
                    if col in self.metadata_df.columns:
                        date_col = col
                        break
                
                if date_col:
                    st.write("  â€¢ åŠ¨æ€æ¼”åŒ–åˆ†æ...")
                    # è½¬æ¢æ—¥æœŸåˆ—ä¸ºæ—¶é—´æˆ³
                    import pandas as pd
                    timestamps = pd.to_datetime(self.metadata_df[date_col], errors='coerce').tolist()
                    analyzer.evolution_analyzer.analyze_dynamic_topics(
                        self.topic_model, self.documents, timestamps
                    )
                    st.write("  âœ“ æ¼”åŒ–åˆ†æå®Œæˆ")
                else:
                    st.write("  âš ï¸ æœªæ‰¾åˆ°æ—¥æœŸåˆ—ï¼Œè·³è¿‡æ¼”åŒ–åˆ†æ")
            
            # 3. ç”Ÿæˆå­¦æœ¯çº§å›¾è¡¨
            st.write("  â€¢ ç”Ÿæˆå­¦æœ¯å›¾è¡¨...")
            analyzer.chart_generator.generate_all_academic_charts(
                self.topic_model, self.documents, self.topics, self.metadata_df
            )
            st.write("  âœ“ å­¦æœ¯å›¾è¡¨ç”Ÿæˆå®Œæˆ")
            
        except Exception as expert_error:
            st.warning(f"  âš ï¸ ä¸“å®¶çº§åˆ†æéƒ¨åˆ†å¤±è´¥: {expert_error}")
            st.write("  â€¢ ç»§ç»­ä¿å­˜åŸºç¡€ç»“æœ...")

        # ä¿å­˜åˆ†æå…ƒä¿¡æ¯
        metadata_file = self.results_dir / "analysis_metadata.json"
        import json
        metadata = {
            'n_topics': len(topic_info) - 1,
            'n_documents': len(self.documents),
            'timestamp': datetime.now().isoformat(),
            'preset_used': st.session_state.current_preset,
            'config_used': self.config,
            'output_files': {
                'topics_summary': str(summary_file),
                'document_mapping': str(doc_mapping_file),
                'trained_model': str(self.results_dir / "trained_model"),
                'analysis_metadata': str(metadata_file)
            }
        }
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        st.write(f"  âœ“ åˆ†æå…ƒæ•°æ®: {metadata_file}")
        
        # æ›´æ–°sessionçŠ¶æ€
        st.session_state.analysis_results = metadata
        
        # æ˜¾ç¤ºè¾“å‡ºæ–‡ä»¶æ€»ç»“
        st.success("ğŸ“ æ‰€æœ‰ç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ° results/ ç›®å½•ï¼š")
        st.write("```")
        st.write(f"results/")
        st.write(f"â”œâ”€â”€ topics_summary.csv                      # ä¸»é¢˜æ‘˜è¦")
        st.write(f"â”œâ”€â”€ document_topic_mapping.csv              # æ–‡æ¡£-ä¸»é¢˜æ˜ å°„")
        st.write(f"â”œâ”€â”€ cross_lingual_composition.csv           # è·¨è¯­è¨€æˆåˆ†åˆ†æ")
        st.write(f"â”œâ”€â”€ dynamic_evolution_analysis.csv          # åŠ¨æ€æ¼”åŒ–åˆ†æ")
        st.write(f"â”œâ”€â”€ analysis_metadata.json                  # åˆ†æå…ƒæ•°æ®")
        st.write(f"â”œâ”€â”€ academic_topic_distribution.png         # å­¦æœ¯çº§ä¸»é¢˜åˆ†å¸ƒå›¾")
        st.write(f"â”œâ”€â”€ academic_topic_sizes.png                # å­¦æœ¯çº§ä¸»é¢˜è§„æ¨¡å›¾")
        st.write(f"â”œâ”€â”€ academic_topic_evolution.png            # å­¦æœ¯çº§æ¼”åŒ–å›¾")
        st.write(f"â”œâ”€â”€ academic_cross_lingual.png              # å­¦æœ¯çº§è·¨è¯­è¨€å›¾")
        st.write(f"â””â”€â”€ trained_model/                          # è®­ç»ƒå¥½çš„æ¨¡å‹")
        st.write(f"    â””â”€â”€ bertopic_model.pkl                  # BERTopicæ¨¡å‹æ–‡ä»¶")
        st.write("```")
    
    
    def _show_status(self):
        """æ˜¾ç¤ºçŠ¶æ€"""
        if hasattr(st.session_state, 'analysis_results') and st.session_state.analysis_results:
            st.success("âœ… åˆ†æå·²å®Œæˆï¼")
        elif hasattr(st.session_state, 'analysis_running') and st.session_state.analysis_running:
            st.info("ğŸ”„ åˆ†æè¿›è¡Œä¸­...")
        
        if hasattr(st.session_state, 'analysis_progress'):
            st.progress(st.session_state.analysis_progress / 100)
