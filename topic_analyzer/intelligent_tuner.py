"""
æ™ºèƒ½å‚æ•°è°ƒä¼˜å™¨
==============
åŸºäºæ•°æ®ç‰¹å¾åˆ†æè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜BERTopicå‚æ•°
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, Tuple, List, Any
from pathlib import Path
import re
from collections import Counter
import jieba
from langdetect import detect
import statistics


class DataAnalyzer:
    """æ•°æ®ç‰¹å¾åˆ†æå™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
    def analyze_text_data(self, documents: List[str]) -> Dict[str, Any]:
        """
        åˆ†ææ–‡æœ¬æ•°æ®ç‰¹å¾
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            æ•°æ®ç‰¹å¾å­—å…¸
        """
        features = {}
        
        # åŸºç¡€ç»Ÿè®¡
        features['total_docs'] = len(documents)
        
        # æ–‡æœ¬é•¿åº¦åˆ†æ
        text_lengths = [len(doc) if doc else 0 for doc in documents]
        features['avg_text_length'] = np.mean(text_lengths)
        features['median_text_length'] = np.median(text_lengths)
        features['text_length_std'] = np.std(text_lengths)
        features['min_text_length'] = min(text_lengths)
        features['max_text_length'] = max(text_lengths)
        
        # è¯­è¨€åˆ†å¸ƒåˆ†æ
        features['language_distribution'] = self._analyze_languages(documents)
        features['dominant_language'] = max(features['language_distribution'].items(), 
                                          key=lambda x: x[1])[0]
        
        # è¯æ±‡ä¸°å¯Œåº¦åˆ†æ
        features['vocabulary_diversity'] = self._calculate_vocabulary_diversity(documents)
        
        # é‡å¤æ–‡æœ¬åˆ†æ
        features['duplicate_ratio'] = self._calculate_duplicate_ratio(documents)
        
        # æ–‡æœ¬å¯†åº¦åˆ†æï¼ˆè¯æ±‡å¯†åº¦ï¼‰
        features['lexical_density'] = self._calculate_lexical_density(documents)
        
        # ä¸»é¢˜å¤æ‚åº¦é¢„ä¼°
        features['estimated_complexity'] = self._estimate_topic_complexity(features)
        
        self.logger.info(f"æ•°æ®åˆ†æå®Œæˆ: {features['total_docs']} ä¸ªæ–‡æ¡£")
        return features
    
    def _analyze_languages(self, documents: List[str]) -> Dict[str, int]:
        """åˆ†æè¯­è¨€åˆ†å¸ƒ"""
        language_counts = Counter()
        
        # é‡‡æ ·åˆ†æï¼ˆå¤§æ•°æ®é›†åªåˆ†æå‰1000ä¸ªï¼‰
        sample_docs = documents[:1000] if len(documents) > 1000 else documents
        
        for doc in sample_docs:
            if not doc or len(doc.strip()) < 10:
                continue
                
            try:
                # æ£€æµ‹è¯­è¨€
                lang = detect(doc[:200])  # åªæ£€æµ‹å‰200å­—ç¬¦
                language_counts[lang] += 1
            except:
                # å¦‚æœæ£€æµ‹å¤±è´¥ï¼Œæ ¹æ®å­—ç¬¦ç‰¹å¾åˆ¤æ–­
                if re.search(r'[\u4e00-\u9fff]', doc):
                    language_counts['zh'] += 1
                elif re.search(r'[Ğ°-ÑÑ‘]', doc, re.IGNORECASE):
                    language_counts['ru'] += 1
                else:
                    language_counts['en'] += 1
        
        return dict(language_counts)
    
    def _calculate_vocabulary_diversity(self, documents: List[str]) -> float:
        """è®¡ç®—è¯æ±‡å¤šæ ·æ€§ï¼ˆTTR - Type-Token Ratioï¼‰"""
        all_words = []
        
        for doc in documents[:500]:  # é‡‡æ ·åˆ†æ
            if not doc:
                continue
                
            # æ ¹æ®è¯­è¨€é€‰æ‹©åˆ†è¯æ–¹å¼
            if re.search(r'[\u4e00-\u9fff]', doc):
                # ä¸­æ–‡åˆ†è¯
                words = list(jieba.cut(doc))
            else:
                # è‹±æ–‡/ä¿„æ–‡ç®€å•åˆ†è¯
                words = re.findall(r'\b\w+\b', doc.lower())
            
            all_words.extend([w for w in words if len(w) > 1])
        
        if not all_words:
            return 0.0
            
        unique_words = len(set(all_words))
        total_words = len(all_words)
        
        return unique_words / total_words if total_words > 0 else 0.0
    
    def _calculate_duplicate_ratio(self, documents: List[str]) -> float:
        """è®¡ç®—é‡å¤æ–‡æ¡£æ¯”ä¾‹"""
        if len(documents) < 2:
            return 0.0
            
        # æ ‡å‡†åŒ–æ–‡æ¡£ç”¨äºæ¯”è¾ƒ
        normalized_docs = []
        for doc in documents:
            if doc:
                # å»é™¤ç©ºç™½å­—ç¬¦å’Œæ ‡ç‚¹ï¼Œè½¬ä¸ºå°å†™
                normalized = re.sub(r'[^\w\s]', '', doc.lower().strip())
                normalized_docs.append(normalized)
            else:
                normalized_docs.append('')
        
        # è®¡ç®—é‡å¤æ¯”ä¾‹
        unique_docs = len(set(normalized_docs))
        total_docs = len(normalized_docs)
        
        return 1 - (unique_docs / total_docs) if total_docs > 0 else 0.0
    
    def _calculate_lexical_density(self, documents: List[str]) -> float:
        """è®¡ç®—è¯æ±‡å¯†åº¦ï¼ˆå†…å®¹è¯vsåŠŸèƒ½è¯æ¯”ä¾‹ï¼‰"""
        # åŠŸèƒ½è¯åˆ—è¡¨ï¼ˆä¸­è‹±ä¿„ï¼‰
        function_words = {
            'zh': {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'ä¸º', 'å¯¹', 'ä»', 'åˆ°'},
            'en': {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with'},
            'ru': {'Ğ¸', 'Ğ²', 'Ğ½Ğ°', 'Ñ', 'Ğ¿Ğ¾', 'Ğ´Ğ»Ñ', 'Ğ¾Ñ‚', 'Ğ´Ğ¾', 'Ğ·Ğ°', 'Ğ¿Ñ€Ğ¸', 'Ğ¾', 'Ğ¾Ğ±', 'Ñ‡Ñ‚Ğ¾', 'ĞºĞ°Ğº'}
        }
        
        content_words = 0
        function_word_count = 0
        
        for doc in documents[:200]:  # é‡‡æ ·åˆ†æ
            if not doc:
                continue
                
            if re.search(r'[\u4e00-\u9fff]', doc):
                # ä¸­æ–‡å¤„ç†
                words = list(jieba.cut(doc))
                for word in words:
                    if len(word) > 1:
                        if word in function_words['zh']:
                            function_word_count += 1
                        else:
                            content_words += 1
            else:
                # è‹±æ–‡/ä¿„æ–‡å¤„ç†
                words = re.findall(r'\b\w+\b', doc.lower())
                for word in words:
                    if word in function_words['en'] or word in function_words['ru']:
                        function_word_count += 1
                    else:
                        content_words += 1
        
        total_words = content_words + function_word_count
        return content_words / total_words if total_words > 0 else 0.5
    
    def _estimate_topic_complexity(self, features: Dict[str, Any]) -> str:
        """åŸºäºç‰¹å¾ä¼°è®¡ä¸»é¢˜å¤æ‚åº¦"""
        complexity_score = 0
        
        # åŸºäºæ–‡æ¡£æ•°é‡
        if features['total_docs'] > 5000:
            complexity_score += 2
        elif features['total_docs'] > 1000:
            complexity_score += 1
        
        # åŸºäºæ–‡æœ¬é•¿åº¦å˜å¼‚
        if features['text_length_std'] > features['avg_text_length'] * 0.5:
            complexity_score += 1
        
        # åŸºäºè¯æ±‡å¤šæ ·æ€§
        if features['vocabulary_diversity'] > 0.6:
            complexity_score += 2
        elif features['vocabulary_diversity'] > 0.4:
            complexity_score += 1
        
        # åŸºäºé‡å¤åº¦
        if features['duplicate_ratio'] > 0.3:
            complexity_score -= 1
        
        # åŸºäºè¯­è¨€å¤šæ ·æ€§
        if len(features['language_distribution']) > 1:
            complexity_score += 1
        
        if complexity_score >= 4:
            return 'high'
        elif complexity_score >= 2:
            return 'medium'
        else:
            return 'low'


class ParameterOptimizer:
    """æ™ºèƒ½å‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # å‚æ•°ä¼˜åŒ–è§„åˆ™åº“
        self.optimization_rules = {
            'min_topic_size': self._optimize_min_topic_size,
            'umap_n_neighbors': self._optimize_umap_neighbors,
            'umap_n_components': self._optimize_umap_components,
            'hdbscan_min_cluster_size': self._optimize_hdbscan_min_cluster,
            'hdbscan_min_samples': self._optimize_hdbscan_min_samples,
            'n_gram_range': self._optimize_ngram_range,
            'nr_topics': self._optimize_nr_topics
        }
    
    def optimize_parameters(self, data_features: Dict[str, Any]) -> Dict[str, Any]:
        """
        åŸºäºæ•°æ®ç‰¹å¾ä¼˜åŒ–å‚æ•°
        
        Args:
            data_features: æ•°æ®ç‰¹å¾å­—å…¸
            
        Returns:
            ä¼˜åŒ–åçš„å‚æ•°å­—å…¸
        """
        optimized_params = {}
        
        self.logger.info(f"å¼€å§‹æ™ºèƒ½å‚æ•°ä¼˜åŒ–ï¼Œæ•°æ®å¤æ‚åº¦: {data_features['estimated_complexity']}")
        
        # åº”ç”¨æ‰€æœ‰ä¼˜åŒ–è§„åˆ™
        for param_name, optimizer_func in self.optimization_rules.items():
            try:
                optimized_params[param_name] = optimizer_func(data_features)
                self.logger.debug(f"ä¼˜åŒ–å‚æ•° {param_name}: {optimized_params[param_name]}")
            except Exception as e:
                self.logger.warning(f"å‚æ•° {param_name} ä¼˜åŒ–å¤±è´¥: {e}")
        
        # å‚æ•°ä¸€è‡´æ€§æ£€æŸ¥å’Œè°ƒæ•´
        optimized_params = self._ensure_parameter_consistency(optimized_params, data_features)
        
        # æ·»åŠ æ¨èç†ç”±
        optimized_params['optimization_reasoning'] = self._generate_reasoning(data_features, optimized_params)
        
        self.logger.info("å‚æ•°ä¼˜åŒ–å®Œæˆ")
        return optimized_params
    
    def _optimize_min_topic_size(self, features: Dict[str, Any]) -> int:
        """ä¼˜åŒ–æœ€å°ä¸»é¢˜å¤§å° - æµ·é‡æ•°æ®ä¸“ç”¨ç®—æ³•"""
        total_docs = features['total_docs']
        complexity = features['estimated_complexity']
        duplicate_ratio = features['duplicate_ratio']
        dominant_lang = features.get('dominant_language', 'unknown')
        
        # æµ·é‡æ•°æ®åˆ†çº§å¤„ç†
        if total_docs >= 100000:
            # è¶…å¤§è§„æ¨¡ï¼š10ä¸‡+æ–‡æ¡£
            base_percentage = 0.0005  # 0.05%
            min_size = 50
        elif total_docs >= 50000:
            # å¤§è§„æ¨¡ï¼š5-10ä¸‡æ–‡æ¡£
            base_percentage = 0.001   # 0.1%
            min_size = 30
        elif total_docs >= 10000:
            # ä¸­å¤§è§„æ¨¡ï¼š1-5ä¸‡æ–‡æ¡£
            base_percentage = 0.002   # 0.2%
            min_size = 20
        elif total_docs >= 5000:
            # ä¸­ç­‰è§„æ¨¡ï¼š5åƒ-1ä¸‡æ–‡æ¡£
            base_percentage = 0.004   # 0.4%
            min_size = 15
        else:
            # å°è§„æ¨¡ï¼š5åƒä»¥ä¸‹æ–‡æ¡£
            base_percentage = 0.008   # 0.8%
            min_size = 8
        
        base_size = max(min_size, int(total_docs * base_percentage))
        
        # è¯­è¨€ç‰¹å®šä¼˜åŒ–
        if dominant_lang == 'ru':
            # ä¿„æ–‡è¯æ±‡å˜åŒ–ä¸°å¯Œï¼Œéœ€è¦æ›´å°çš„ä¸»é¢˜æ•è·è¯­ä¹‰å·®å¼‚
            base_size = max(min_size // 2, int(base_size * 0.7))
        elif dominant_lang == 'zh':
            # ä¸­æ–‡è¯­ä¹‰å¯†åº¦é«˜ï¼Œå¯ä»¥ç”¨ç¨å¤§çš„ä¸»é¢˜
            base_size = int(base_size * 1.1)
        
        # å¤æ‚åº¦è°ƒæ•´
        if complexity == 'high':
            base_size = max(min_size // 2, int(base_size * 0.8))
        elif complexity == 'low' and duplicate_ratio > 0.2:
            base_size = int(base_size * 1.3)
        
        # ä¸¥æ ¼è¾¹ç•Œé™åˆ¶
        return max(min_size // 2, min(min_size * 3, base_size))
    
    def _optimize_umap_neighbors(self, features: Dict[str, Any]) -> int:
        """ä¼˜åŒ–UMAPé‚»å±…æ•° - æµ·é‡æ•°æ®ä¸“ç”¨ç®—æ³•"""
        total_docs = features['total_docs']
        complexity = features['estimated_complexity']
        vocab_diversity = features['vocabulary_diversity']
        dominant_lang = features.get('dominant_language', 'unknown')
        
        # æµ·é‡æ•°æ®åˆ†çº§å¤„ç† - ä½¿ç”¨å¯¹æ•°ç¼©æ”¾
        if total_docs >= 100000:
            base_neighbors = 50  # è¶…å¤§è§„æ¨¡éœ€è¦æ›´å¤šé‚»å±…ä¿æŒå…¨å±€ç»“æ„
        elif total_docs >= 50000:
            base_neighbors = 40
        elif total_docs >= 10000:
            base_neighbors = 30
        elif total_docs >= 5000:
            base_neighbors = 25
        elif total_docs >= 1000:
            base_neighbors = 20
        else:
            base_neighbors = 15
        
        # è¯­è¨€ç‰¹å®šä¼˜åŒ–
        if dominant_lang == 'ru':
            # ä¿„æ–‡å½¢æ€å˜åŒ–ä¸°å¯Œï¼Œéœ€è¦æ›´å¤šé‚»å±…æ•è·è¯­ä¹‰ç›¸ä¼¼æ€§
            base_neighbors = int(base_neighbors * 1.2)
        elif dominant_lang == 'zh':
            # ä¸­æ–‡è¯æ±‡å¯†åº¦é«˜ï¼Œå¯ä»¥ç”¨ç¨å°‘é‚»å±…
            base_neighbors = int(base_neighbors * 0.9)
        
        # å¤æ‚åº¦å’Œå¤šæ ·æ€§è°ƒæ•´
        if complexity == 'high' and vocab_diversity > 0.6:
            # é«˜å¤æ‚åº¦é«˜å¤šæ ·æ€§ï¼šéœ€è¦æ›´å¤šé‚»å±…ä¿æŒç»“æ„
            base_neighbors = int(base_neighbors * 1.3)
        elif complexity == 'low' and vocab_diversity < 0.3:
            # ä½å¤æ‚åº¦ä½å¤šæ ·æ€§ï¼šå‡å°‘é‚»å±…é¿å…è¿‡åº¦å¹³æ»‘
            base_neighbors = int(base_neighbors * 0.8)
        
        # æµ·é‡æ•°æ®çš„ä¸¥æ ¼è¾¹ç•Œé™åˆ¶
        min_neighbors = max(10, int(np.log10(total_docs)) * 3)
        max_neighbors = min(100, int(np.sqrt(total_docs / 100)))
        
        return max(min_neighbors, min(max_neighbors, base_neighbors))
    
    def _optimize_umap_components(self, features: Dict[str, Any]) -> int:
        """ä¼˜åŒ–UMAPé™ç»´ç»´åº¦"""
        total_docs = features['total_docs']
        complexity = features['estimated_complexity']
        vocab_diversity = features['vocabulary_diversity']
        
        # åŸºç¡€ç»´åº¦é€‰æ‹©
        if complexity == 'high' and vocab_diversity > 0.5:
            # é«˜å¤æ‚åº¦é«˜å¤šæ ·æ€§ï¼šä¿ç•™æ›´å¤šç»´åº¦
            base_components = 7
        elif complexity == 'low' or total_docs > 10000:
            # ä½å¤æ‚åº¦æˆ–å¤§æ•°æ®é›†ï¼šé™ä½ç»´åº¦æé«˜é€Ÿåº¦
            base_components = 3
        else:
            # ä¸­ç­‰æƒ…å†µ
            base_components = 5
        
        return max(2, min(10, base_components))
    
    def _optimize_hdbscan_min_cluster(self, features: Dict[str, Any]) -> int:
        """ä¼˜åŒ–HDBSCANæœ€å°èšç±»å¤§å°"""
        # é€šå¸¸ä¸min_topic_sizeä¿æŒä¸€è‡´
        return self._optimize_min_topic_size(features)
    
    def _optimize_hdbscan_min_samples(self, features: Dict[str, Any]) -> int:
        """ä¼˜åŒ–HDBSCANæœ€å°æ ·æœ¬æ•° - æµ·é‡æ•°æ®ä¸“ç”¨ç®—æ³•"""
        min_cluster_size = self._optimize_hdbscan_min_cluster(features)
        duplicate_ratio = features['duplicate_ratio']
        total_docs = features['total_docs']
        dominant_lang = features.get('dominant_language', 'unknown')
        
        # æµ·é‡æ•°æ®çš„åŸºç¡€å€¼è®¡ç®—
        if total_docs >= 100000:
            # è¶…å¤§è§„æ¨¡ï¼šæ›´ä¸¥æ ¼çš„æ ·æœ¬è¦æ±‚
            base_samples = max(3, min_cluster_size // 4)
        elif total_docs >= 10000:
            # å¤§è§„æ¨¡ï¼šå¹³è¡¡ç²¾åº¦å’Œå¬å›
            base_samples = max(2, min_cluster_size // 5)
        else:
            # ä¸­å°è§„æ¨¡ï¼šä¼ ç»Ÿæ–¹æ³•
            base_samples = max(1, min_cluster_size // 3)
        
        # æ ¹æ®é‡å¤åº¦è°ƒæ•´
        if duplicate_ratio > 0.4:
            # é«˜é‡å¤åº¦ï¼šå¢åŠ æœ€å°æ ·æœ¬æ•°æé«˜ç¨³å®šæ€§
            base_samples = max(base_samples, min_cluster_size // 2)
        
        return max(1, min(20, base_samples))
    
    def _optimize_ngram_range(self, features: Dict[str, Any]) -> List[int]:
        """ä¼˜åŒ–N-gramèŒƒå›´"""
        avg_length = features['avg_text_length']
        dominant_lang = features['dominant_language']
        lexical_density = features['lexical_density']
        
        # åŸºäºè¯­è¨€ç‰¹å¾
        if dominant_lang == 'zh':
            # ä¸­æ–‡ï¼šè¾ƒçŸ­çš„n-gram
            if avg_length > 200:
                return [1, 3]
            else:
                return [1, 2]
        else:
            # è‹±æ–‡/ä¿„æ–‡ï¼šæ ¹æ®æ–‡æœ¬é•¿åº¦å’Œè¯æ±‡å¯†åº¦
            if avg_length > 500 and lexical_density > 0.6:
                return [1, 3]
            elif avg_length > 100:
                return [1, 2]
            else:
                return [1, 2]
    
    def _optimize_nr_topics(self, features: Dict[str, Any]) -> str:
        """ä¼˜åŒ–ä¸»é¢˜æ•°é‡"""
        total_docs = features['total_docs']
        complexity = features['estimated_complexity']
        
        # å¤§å¤šæ•°æƒ…å†µä¸‹ä½¿ç”¨è‡ªåŠ¨ç¡®å®š
        if complexity == 'low' and total_docs < 1000:
            # å°è§„æ¨¡ä½å¤æ‚åº¦æ•°æ®ï¼šé™åˆ¶ä¸»é¢˜æ•°é¿å…è¿‡æ‹Ÿåˆ
            return min(15, max(5, total_docs // 50))
        else:
            # å…¶ä»–æƒ…å†µï¼šè‡ªåŠ¨ç¡®å®š
            return 'auto'
    
    def _ensure_parameter_consistency(self, params: Dict[str, Any], features: Dict[str, Any]) -> Dict[str, Any]:
        """ç¡®ä¿å‚æ•°ä¹‹é—´çš„ä¸€è‡´æ€§"""
        # ç¡®ä¿min_cluster_size = min_topic_size
        if 'min_topic_size' in params and 'hdbscan_min_cluster_size' in params:
            params['hdbscan_min_cluster_size'] = params['min_topic_size']
        
        # ç¡®ä¿min_samples <= min_cluster_size
        if 'hdbscan_min_samples' in params and 'hdbscan_min_cluster_size' in params:
            params['hdbscan_min_samples'] = min(
                params['hdbscan_min_samples'], 
                params['hdbscan_min_cluster_size']
            )
        
        # ç¡®ä¿components <= neighbors
        if 'umap_n_components' in params and 'umap_n_neighbors' in params:
            if params['umap_n_components'] >= params['umap_n_neighbors']:
                params['umap_n_components'] = max(2, params['umap_n_neighbors'] - 1)
        
        return params
    
    def _generate_reasoning(self, features: Dict[str, Any], params: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆå‚æ•°é€‰æ‹©çš„æ¨ç†è¯´æ˜"""
        reasoning = []
        
        # æ•°æ®è§„æ¨¡åˆ†æ
        total_docs = features['total_docs']
        if total_docs > 5000:
            reasoning.append(f"ğŸ“Š å¤§è§„æ¨¡æ•°æ®é›†({total_docs}æ–‡æ¡£)ï¼Œé‡‡ç”¨å¹³è¡¡ç­–ç•¥ä¿è¯æ•ˆç‡")
        elif total_docs < 500:
            reasoning.append(f"ğŸ“Š å°è§„æ¨¡æ•°æ®é›†({total_docs}æ–‡æ¡£)ï¼Œé‡‡ç”¨ç²¾ç»†åŒ–åˆ†æç­–ç•¥")
        else:
            reasoning.append(f"ğŸ“Š ä¸­ç­‰è§„æ¨¡æ•°æ®é›†({total_docs}æ–‡æ¡£)ï¼Œé‡‡ç”¨æ ‡å‡†åˆ†æç­–ç•¥")
        
        # å¤æ‚åº¦åˆ†æ
        complexity = features['estimated_complexity']
        if complexity == 'high':
            reasoning.append("ğŸ” æ£€æµ‹åˆ°é«˜å¤æ‚åº¦æ–‡æœ¬ï¼Œä½¿ç”¨ç»†ç²’åº¦ä¸»é¢˜å‘ç°")
        elif complexity == 'low':
            reasoning.append("ğŸ“ æ£€æµ‹åˆ°ä½å¤æ‚åº¦æ–‡æœ¬ï¼Œä½¿ç”¨æ¦‚æ‹¬æ€§ä¸»é¢˜èšåˆ")
        else:
            reasoning.append("âš–ï¸ ä¸­ç­‰å¤æ‚åº¦æ–‡æœ¬ï¼Œå¹³è¡¡ç»†èŠ‚ä¸æ¦‚æ‹¬")
        
        # è¯­è¨€ç‰¹å¾
        dominant_lang = features['dominant_language']
        lang_names = {'zh': 'ä¸­æ–‡', 'en': 'è‹±æ–‡', 'ru': 'ä¿„æ–‡'}
        reasoning.append(f"ğŸŒ ä¸»è¦è¯­è¨€: {lang_names.get(dominant_lang, dominant_lang)}ï¼Œä¼˜åŒ–åˆ†è¯ç­–ç•¥")
        
        # å‚æ•°è§£é‡Š
        if 'min_topic_size' in params:
            reasoning.append(f"ğŸ¯ ä¸»é¢˜å¤§å°è®¾å®šä¸º{params['min_topic_size']}ï¼Œå¹³è¡¡ä¸»é¢˜è´¨é‡ä¸æ•°é‡")
        
        return reasoning


class IntelligentTuner:
    """æ™ºèƒ½å‚æ•°è°ƒä¼˜å™¨ä¸»ç±»"""
    
    def __init__(self):
        self.data_analyzer = DataAnalyzer()
        self.parameter_optimizer = ParameterOptimizer()
        self.logger = logging.getLogger(__name__)
    
    def auto_tune(self, documents: List[str]) -> Dict[str, Any]:
        """
        è‡ªåŠ¨è°ƒä¼˜ä¸»å‡½æ•°
        
        Args:
            documents: è¾“å…¥æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            ä¼˜åŒ–çš„å‚æ•°é…ç½®å’Œåˆ†ææŠ¥å‘Š
        """
        self.logger.info("å¼€å§‹æ™ºèƒ½å‚æ•°è°ƒä¼˜")
        
        # Step 1: åˆ†ææ•°æ®ç‰¹å¾
        self.logger.info("ç¬¬1æ­¥ï¼šåˆ†ææ•°æ®ç‰¹å¾...")
        data_features = self.data_analyzer.analyze_text_data(documents)
        
        # Step 2: ä¼˜åŒ–å‚æ•°
        self.logger.info("ç¬¬2æ­¥ï¼šæ™ºèƒ½å‚æ•°ä¼˜åŒ–...")
        optimized_params = self.parameter_optimizer.optimize_parameters(data_features)
        
        # Step 3: ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
        tuning_report = {
            'data_features': data_features,
            'optimized_parameters': optimized_params,
            'tuning_summary': self._generate_tuning_summary(data_features, optimized_params)
        }
        
        self.logger.info("æ™ºèƒ½å‚æ•°è°ƒä¼˜å®Œæˆ")
        return tuning_report
    
    def _generate_tuning_summary(self, features: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, str]:
        """ç”Ÿæˆè°ƒä¼˜æ‘˜è¦"""
        return {
            'data_scale': f"{features['total_docs']} ä¸ªæ–‡æ¡£",
            'complexity_level': features['estimated_complexity'],
            'primary_language': features['dominant_language'],
            'optimization_strategy': f"æœ€å°ä¸»é¢˜å¤§å°: {params.get('min_topic_size', 'N/A')}",
            'expected_performance': self._predict_performance(features, params)
        }
    
    def _predict_performance(self, features: Dict[str, Any], params: Dict[str, Any]) -> str:
        """é¢„æµ‹æ€§èƒ½è¡¨ç°"""
        total_docs = features['total_docs']
        complexity = features['estimated_complexity']
        
        if total_docs > 10000 and complexity == 'high':
            return "é¢„è®¡è¿è¡Œæ—¶é—´è¾ƒé•¿ï¼Œä½†ç»“æœè¯¦ç»†å‡†ç¡®"
        elif total_docs < 1000:
            return "å¿«é€Ÿåˆ†æï¼Œé€‚åˆæ¢ç´¢æ€§ç ”ç©¶"
        else:
            return "å¹³è¡¡çš„æ€§èƒ½å’Œè´¨é‡è¡¨ç°"
