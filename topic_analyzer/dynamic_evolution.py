"""
åŠ¨æ€ä¸»é¢˜æ¼”åŒ–åˆ†ææ¨¡å—
==================
å®ç°BERTopicçš„å®˜æ–¹åŠ¨æ€ä¸»é¢˜å»ºæ¨¡åŠŸèƒ½ï¼Œåˆ†æä¸»é¢˜éšæ—¶é—´çš„æ¼”åŒ–
"""

import pandas as pd
import numpy as np
import logging
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)


class DynamicTopicEvolution:
    """åŠ¨æ€ä¸»é¢˜æ¼”åŒ–åˆ†æå™¨ - SOTA & KISSå®ç°"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–åŠ¨æ€ä¸»é¢˜æ¼”åŒ–åˆ†æå™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.analysis_config = config['analysis']['time_analysis']
        self.results_paths = config['results_paths']
        
    def prepare_temporal_data(self, 
                            documents: List[str],
                            metadata_df: pd.DataFrame) -> Tuple[List[str], List[datetime]]:
        """
        å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ®
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            metadata_df: å…ƒæ•°æ®DataFrame
            
        Returns:
            (æ–‡æ¡£åˆ—è¡¨, æ—¶é—´æˆ³åˆ—è¡¨)
        """
        time_column = self.analysis_config['time_column']
        
        if time_column not in metadata_df.columns:
            logger.warning(f"  âš  æ—¶é—´åˆ— '{time_column}' ä¸å­˜åœ¨")
            return documents, []
        
        # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeæ ¼å¼
        try:
            timestamps = pd.to_datetime(metadata_df[time_column])
            # è¿‡æ»¤æ‰æ— æ•ˆæ—¶é—´æˆ³
            valid_mask = timestamps.notna()
            valid_documents = [doc for i, doc in enumerate(documents) if valid_mask.iloc[i]]
            valid_timestamps = timestamps[valid_mask].tolist()
            
            logger.info(f"  âœ“ å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ®: {len(valid_documents)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")
            logger.info(f"  âœ“ æ—¶é—´èŒƒå›´: {min(valid_timestamps)} ~ {max(valid_timestamps)}")
            
            return valid_documents, valid_timestamps
            
        except Exception as e:
            logger.error(f"  âœ— æ—¶é—´æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            return documents, []
    
    def analyze_dynamic_topics(self,
                             topic_model,
                             documents: List[str],
                             timestamps: List[datetime]) -> pd.DataFrame:
        """
        æ‰§è¡ŒåŠ¨æ€ä¸»é¢˜åˆ†æ
        
        Args:
            topic_model: è®­ç»ƒå¥½çš„BERTopicæ¨¡å‹
            documents: æ–‡æ¡£åˆ—è¡¨
            timestamps: æ—¶é—´æˆ³åˆ—è¡¨
            
        Returns:
            ä¸»é¢˜æ—¶é—´æ¼”åŒ–DataFrame
        """
        if not timestamps:
            logger.warning("  âš  æ²¡æœ‰æœ‰æ•ˆæ—¶é—´æˆ³ï¼Œè·³è¿‡åŠ¨æ€åˆ†æ")
            return pd.DataFrame()
        
        logger.info("ğŸ• æ‰§è¡ŒåŠ¨æ€ä¸»é¢˜æ¼”åŒ–åˆ†æ...")
        
        try:
            # ä½¿ç”¨BERTopicçš„topics_over_timeæ–¹æ³•
            bins = self.analysis_config.get('bins', 10)
            topics_over_time = topic_model.topics_over_time(
                documents=documents,
                timestamps=timestamps,
                nr_bins=bins,
                datetime_format=None,  # å·²ç»æ˜¯datetimeå¯¹è±¡
                evolution_tuning=True,  # å¯ç”¨æ¼”åŒ–è°ƒä¼˜
                global_tuning=True     # å¯ç”¨å…¨å±€è°ƒä¼˜
            )
            
            logger.info(f"  âœ“ åŠ¨æ€åˆ†æå®Œæˆ: {len(topics_over_time)} ä¸ªæ—¶é—´ç‚¹")
            return topics_over_time
            
        except Exception as e:
            logger.error(f"  âœ— åŠ¨æ€ä¸»é¢˜åˆ†æå¤±è´¥: {e}")
            return pd.DataFrame()
    
    def analyze_topic_birth_death(self,
                                 topics_over_time: pd.DataFrame) -> Dict[str, Any]:
        """
        åˆ†æä¸»é¢˜çš„è¯ç”Ÿå’Œæ¶ˆäº¡
        
        Args:
            topics_over_time: ä¸»é¢˜æ—¶é—´æ¼”åŒ–æ•°æ®
            
        Returns:
            ä¸»é¢˜è¯ç”Ÿæ¶ˆäº¡åˆ†æç»“æœ
        """
        if topics_over_time.empty:
            return {}
        
        logger.info("ğŸ“Š åˆ†æä¸»é¢˜è¯ç”Ÿå’Œæ¶ˆäº¡...")
        
        results = {
            'topic_births': {},
            'topic_deaths': {},
            'persistent_topics': [],
            'ephemeral_topics': []
        }
        
        try:
            # è·å–æ‰€æœ‰ä¸»é¢˜
            topics = topics_over_time['Topic'].unique()
            topics = [t for t in topics if t != -1]  # æ’é™¤ç¦»ç¾¤ç‚¹
            
            for topic in topics:
                topic_data = topics_over_time[topics_over_time['Topic'] == topic]
                topic_data = topic_data.sort_values('Timestamp')
                
                # æ‰¾åˆ°ä¸»é¢˜é¦–æ¬¡å‡ºç°å’Œæœ€åå‡ºç°çš„æ—¶é—´
                first_appearance = topic_data.iloc[0]['Timestamp']
                last_appearance = topic_data.iloc[-1]['Timestamp']
                
                # è®¡ç®—ä¸»é¢˜æ´»è·ƒåº¦ï¼ˆéé›¶é¢‘ç‡çš„æ—¶é—´ç‚¹æ¯”ä¾‹ï¼‰
                activity_ratio = len(topic_data[topic_data['Frequency'] > 0]) / len(topic_data)
                
                # è®°å½•è¯ç”Ÿæ—¶é—´
                results['topic_births'][topic] = first_appearance
                
                # åˆ¤æ–­æ˜¯å¦"æ­»äº¡"ï¼ˆæœ€è¿‘å‡ ä¸ªæ—¶é—´ç‚¹é¢‘ç‡ä¸º0ï¼‰
                recent_data = topic_data.tail(3)
                if all(recent_data['Frequency'] == 0):
                    results['topic_deaths'][topic] = last_appearance
                
                # åˆ†ç±»æŒç»­æ€§è¯é¢˜å’ŒçŸ­æš‚è¯é¢˜
                if activity_ratio > 0.5:
                    results['persistent_topics'].append(topic)
                else:
                    results['ephemeral_topics'].append(topic)
            
            logger.info(f"  âœ“ å‘ç° {len(results['topic_births'])} ä¸ªä¸»é¢˜è¯ç”Ÿ")
            logger.info(f"  âœ“ å‘ç° {len(results['topic_deaths'])} ä¸ªä¸»é¢˜æ¶ˆäº¡")
            logger.info(f"  âœ“ æŒç»­æ€§ä¸»é¢˜: {len(results['persistent_topics'])} ä¸ª")
            logger.info(f"  âœ“ çŸ­æš‚æ€§ä¸»é¢˜: {len(results['ephemeral_topics'])} ä¸ª")
            
        except Exception as e:
            logger.error(f"  âœ— ä¸»é¢˜è¯ç”Ÿæ¶ˆäº¡åˆ†æå¤±è´¥: {e}")
        
        return results
    
    def detect_topic_evolution_patterns(self,
                                      topics_over_time: pd.DataFrame) -> Dict[str, Any]:
        """
        æ£€æµ‹ä¸»é¢˜æ¼”åŒ–æ¨¡å¼
        
        Args:
            topics_over_time: ä¸»é¢˜æ—¶é—´æ¼”åŒ–æ•°æ®
            
        Returns:
            æ¼”åŒ–æ¨¡å¼åˆ†æç»“æœ
        """
        if topics_over_time.empty:
            return {}
        
        logger.info("ğŸ” æ£€æµ‹ä¸»é¢˜æ¼”åŒ–æ¨¡å¼...")
        
        patterns = {
            'rising_topics': [],      # ä¸Šå‡è¶‹åŠ¿ä¸»é¢˜
            'declining_topics': [],   # ä¸‹é™è¶‹åŠ¿ä¸»é¢˜
            'stable_topics': [],      # ç¨³å®šä¸»é¢˜
            'volatile_topics': [],    # æ³¢åŠ¨ä¸»é¢˜
            'seasonal_topics': []     # å­£èŠ‚æ€§ä¸»é¢˜
        }
        
        try:
            topics = topics_over_time['Topic'].unique()
            topics = [t for t in topics if t != -1]
            
            for topic in topics:
                topic_data = topics_over_time[topics_over_time['Topic'] == topic]
                topic_data = topic_data.sort_values('Timestamp')
                frequencies = topic_data['Frequency'].values
                
                if len(frequencies) < 3:
                    continue
                
                # è®¡ç®—è¶‹åŠ¿
                x = np.arange(len(frequencies))
                slope = np.polyfit(x, frequencies, 1)[0]
                
                # è®¡ç®—æ³¢åŠ¨æ€§ï¼ˆæ ‡å‡†å·®ï¼‰
                volatility = np.std(frequencies)
                mean_freq = np.mean(frequencies)
                cv = volatility / (mean_freq + 1e-6)  # å˜å¼‚ç³»æ•°
                
                # åˆ†ç±»æ¼”åŒ–æ¨¡å¼
                if slope > 0.01:
                    patterns['rising_topics'].append((topic, slope))
                elif slope < -0.01:
                    patterns['declining_topics'].append((topic, slope))
                elif cv < 0.3:
                    patterns['stable_topics'].append((topic, cv))
                elif cv > 0.8:
                    patterns['volatile_topics'].append((topic, cv))
                
                # æ£€æµ‹å­£èŠ‚æ€§ï¼ˆç®€å•å®ç°ï¼šæ£€æŸ¥å‘¨æœŸæ€§å³°å€¼ï¼‰
                if len(frequencies) >= 6:
                    autocorr = np.corrcoef(frequencies[:-3], frequencies[3:])[0, 1]
                    if autocorr > 0.6:
                        patterns['seasonal_topics'].append((topic, autocorr))
            
            # æ’åºç»“æœ
            patterns['rising_topics'].sort(key=lambda x: x[1], reverse=True)
            patterns['declining_topics'].sort(key=lambda x: x[1])
            patterns['stable_topics'].sort(key=lambda x: x[1])
            patterns['volatile_topics'].sort(key=lambda x: x[1], reverse=True)
            patterns['seasonal_topics'].sort(key=lambda x: x[1], reverse=True)
            
            logger.info(f"  âœ“ ä¸Šå‡è¶‹åŠ¿ä¸»é¢˜: {len(patterns['rising_topics'])} ä¸ª")
            logger.info(f"  âœ“ ä¸‹é™è¶‹åŠ¿ä¸»é¢˜: {len(patterns['declining_topics'])} ä¸ª")
            logger.info(f"  âœ“ ç¨³å®šä¸»é¢˜: {len(patterns['stable_topics'])} ä¸ª")
            logger.info(f"  âœ“ æ³¢åŠ¨ä¸»é¢˜: {len(patterns['volatile_topics'])} ä¸ª")
            logger.info(f"  âœ“ å­£èŠ‚æ€§ä¸»é¢˜: {len(patterns['seasonal_topics'])} ä¸ª")
            
        except Exception as e:
            logger.error(f"  âœ— æ¼”åŒ–æ¨¡å¼æ£€æµ‹å¤±è´¥: {e}")
        
        return patterns
    
    def save_evolution_analysis(self,
                               topics_over_time: pd.DataFrame,
                               birth_death_analysis: Dict[str, Any],
                               evolution_patterns: Dict[str, Any]) -> str:
        """
        ä¿å­˜æ¼”åŒ–åˆ†æç»“æœ
        
        Args:
            topics_over_time: ä¸»é¢˜æ—¶é—´æ¼”åŒ–æ•°æ®
            birth_death_analysis: è¯ç”Ÿæ¶ˆäº¡åˆ†æ
            evolution_patterns: æ¼”åŒ–æ¨¡å¼åˆ†æ
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        output_path = Path(self.results_paths['output_dir']) / 'dynamic_evolution_analysis.csv'
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        try:
            # åˆ›å»ºç»¼åˆåˆ†æç»“æœ
            analysis_results = []
            
            if not topics_over_time.empty:
                topics = topics_over_time['Topic'].unique()
                topics = [t for t in topics if t != -1]
                
                for topic in topics:
                    topic_data = topics_over_time[topics_over_time['Topic'] == topic]
                    
                    result = {
                        'Topic': topic,
                        'First_Appearance': birth_death_analysis.get('topic_births', {}).get(topic, ''),
                        'Last_Appearance': topic_data['Timestamp'].max(),
                        'Total_Frequency': topic_data['Frequency'].sum(),
                        'Average_Frequency': topic_data['Frequency'].mean(),
                        'Peak_Frequency': topic_data['Frequency'].max(),
                        'Activity_Duration': len(topic_data[topic_data['Frequency'] > 0]),
                        'Is_Persistent': topic in birth_death_analysis.get('persistent_topics', []),
                        'Evolution_Pattern': self._get_topic_pattern(topic, evolution_patterns)
                    }
                    analysis_results.append(result)
            
            # ä¿å­˜ä¸ºCSV
            if analysis_results:
                df = pd.DataFrame(analysis_results)
                df.to_csv(output_path, index=False, encoding='utf-8-sig')
                logger.info(f"  âœ“ æ¼”åŒ–åˆ†æç»“æœå·²ä¿å­˜: {output_path}")
            
            return str(output_path)
            
        except Exception as e:
            logger.error(f"  âœ— ä¿å­˜æ¼”åŒ–åˆ†æå¤±è´¥: {e}")
            return ""
    
    def _get_topic_pattern(self, topic: int, patterns: Dict[str, Any]) -> str:
        """è·å–ä¸»é¢˜çš„æ¼”åŒ–æ¨¡å¼æ ‡ç­¾"""
        for pattern_name, topic_list in patterns.items():
            if any(t[0] == topic for t in topic_list if isinstance(t, tuple)):
                return pattern_name.replace('_topics', '')
            elif topic in topic_list:
                return pattern_name.replace('_topics', '')
        return 'unknown'
    
    def run_full_evolution_analysis(self,
                                   topic_model,
                                   documents: List[str],
                                   metadata_df: pd.DataFrame) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„åŠ¨æ€æ¼”åŒ–åˆ†æ
        
        Args:
            topic_model: è®­ç»ƒå¥½çš„BERTopicæ¨¡å‹
            documents: æ–‡æ¡£åˆ—è¡¨
            metadata_df: å…ƒæ•°æ®
            
        Returns:
            å®Œæ•´çš„æ¼”åŒ–åˆ†æç»“æœ
        """
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´åŠ¨æ€ä¸»é¢˜æ¼”åŒ–åˆ†æ...")
        
        # 1. å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ®
        temporal_docs, timestamps = self.prepare_temporal_data(documents, metadata_df)
        
        if not timestamps:
            logger.warning("  âš  æ— æ³•è¿›è¡ŒåŠ¨æ€åˆ†æï¼šç¼ºå°‘æœ‰æ•ˆæ—¶é—´æ•°æ®")
            return {}
        
        # 2. æ‰§è¡ŒåŠ¨æ€ä¸»é¢˜åˆ†æ
        topics_over_time = self.analyze_dynamic_topics(topic_model, temporal_docs, timestamps)
        
        if topics_over_time.empty:
            logger.warning("  âš  åŠ¨æ€åˆ†æç»“æœä¸ºç©º")
            return {}
        
        # 3. åˆ†æä¸»é¢˜è¯ç”Ÿå’Œæ¶ˆäº¡
        birth_death_analysis = self.analyze_topic_birth_death(topics_over_time)
        
        # 4. æ£€æµ‹æ¼”åŒ–æ¨¡å¼
        evolution_patterns = self.detect_topic_evolution_patterns(topics_over_time)
        
        # 5. ä¿å­˜åˆ†æç»“æœ
        saved_path = self.save_evolution_analysis(
            topics_over_time, birth_death_analysis, evolution_patterns
        )
        
        # 6. ç»„ç»‡è¿”å›ç»“æœ
        results = {
            'topics_over_time': topics_over_time,
            'birth_death_analysis': birth_death_analysis,
            'evolution_patterns': evolution_patterns,
            'saved_path': saved_path,
            'summary': {
                'total_topics': len(topics_over_time['Topic'].unique()) - 1,  # æ’é™¤-1
                'time_points': len(topics_over_time['Timestamp'].unique()),
                'analysis_period': f"{min(timestamps)} to {max(timestamps)}"
            }
        }
        
        logger.info("âœ… åŠ¨æ€ä¸»é¢˜æ¼”åŒ–åˆ†æå®Œæˆ")
        return results
