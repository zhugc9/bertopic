"""
BERTopicæ¨¡å‹è®­ç»ƒä¸åˆ†ææ¨¡å—
==========================
SOTAå®ç°ï¼šä½¿ç”¨æœ€æ–°çš„BERTopicç‰¹æ€§å’Œä¼˜åŒ–æŠ€æœ¯
"""

import pandas as pd
import numpy as np
import logging
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# BERTopicæ ¸å¿ƒ
from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance

# ä¸“å®¶çº§å…³é”®è¯æå–
from .expert_keywords import ExpertKeywordExtractor

# å­¦æœ¯å›¾è¡¨ç”Ÿæˆ
from .academic_charts import AcademicChartGenerator

# åŠ¨æ€æ¼”åŒ–åˆ†æ
from .dynamic_evolution import DynamicTopicEvolution

# è·¨è¯­è¨€åˆ†æ
from .cross_lingual import CrossLingualAnalyzer

# é™ç»´å’Œèšç±»
from umap import UMAP
from hdbscan import HDBSCAN

# åµŒå…¥æ¨¡å‹
from sentence_transformers import SentenceTransformer

# å‘é‡åŒ–
from sklearn.feature_extraction.text import CountVectorizer

# å¯è§†åŒ–
import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
import matplotlib.pyplot as plt
import seaborn as sns

logger = logging.getLogger(__name__)


class TopicAnalyzer:
    """ä¸»é¢˜åˆ†æå™¨ç±» - SOTAå®ç°"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–ä¸»é¢˜åˆ†æå™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.model_params = config['bertopic_params']
        self.viz_params = config['visualization']
        self.results_paths = config['results_paths']
        
        # åˆå§‹åŒ–ä¸“å®¶çº§å…³é”®è¯æå–å™¨
        self.expert_extractor = ExpertKeywordExtractor(config)
        
        # åˆå§‹åŒ–å­¦æœ¯å›¾è¡¨ç”Ÿæˆå™¨
        self.chart_generator = AcademicChartGenerator(config)
        
        # åˆå§‹åŒ–åŠ¨æ€æ¼”åŒ–åˆ†æå™¨
        self.evolution_analyzer = DynamicTopicEvolution(config)
        
        # åˆå§‹åŒ–è·¨è¯­è¨€åˆ†æå™¨
        self.cross_lingual_analyzer = CrossLingualAnalyzer(config)
        
        # è®¾ç½®matplotlibä¸­æ–‡æ”¯æŒ
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
    def train_bertopic_model(self, 
                           documents: List[str]) -> Tuple[BERTopic, List[int]]:
        """
        è®­ç»ƒBERTopicæ¨¡å‹
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            topic_model: è®­ç»ƒå¥½çš„æ¨¡å‹
            topics: æ¯ä¸ªæ–‡æ¡£çš„ä¸»é¢˜ç¼–å·
        """
        logger.info("åˆå§‹åŒ–BERTopicç»„ä»¶...")
        
        # 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆSOTAï¼šå¤šè¯­è¨€æ”¯æŒï¼‰
        embedding_model = self._create_embedding_model()
        
        # 2. åˆå§‹åŒ–UMAPé™ç»´ï¼ˆSOTAä¼˜åŒ–å‚æ•°ï¼‰
        umap_model = self._create_umap_model()
        
        # 3. åˆå§‹åŒ–HDBSCANèšç±»ï¼ˆSOTAä¼˜åŒ–å‚æ•°ï¼‰
        hdbscan_model = self._create_hdbscan_model()
        
        # 4. åˆå§‹åŒ–å‘é‡åŒ–å™¨ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰
        vectorizer_model = self._create_vectorizer()
        
        # 5. åˆå§‹åŒ–è¡¨ç¤ºæ¨¡å‹ï¼ˆSOTAï¼šç»„åˆå¤šç§è¡¨ç¤ºæ–¹æ³•ï¼‰
        representation_model = self._create_representation_model()
        
        # 6. åˆ›å»ºBERTopicæ¨¡å‹
        topic_model = BERTopic(
            embedding_model=embedding_model,
            umap_model=umap_model,
            hdbscan_model=hdbscan_model,
            vectorizer_model=vectorizer_model,
            representation_model=representation_model,
            nr_topics=self._get_nr_topics(),
            top_n_words=10,
            verbose=self.config['system']['verbose'],
            calculate_probabilities=True  # SOTAï¼šè®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        )
        
        # 7. è®­ç»ƒæ¨¡å‹
        logger.info(f"å¼€å§‹è®­ç»ƒæ¨¡å‹ (æ–‡æ¡£æ•°: {len(documents)})...")
        topics, probs = topic_model.fit_transform(documents)
        
        # 8. ä¿å­˜æ¨¡å‹
        self._save_model(topic_model)
        
        # ç»Ÿè®¡ä¿¡æ¯
        topic_info = topic_model.get_topic_info()
        n_topics = len(topic_info) - 1  # å‡å»ç¦»ç¾¤ç‚¹
        logger.info(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ: å‘ç° {n_topics} ä¸ªä¸»é¢˜")
        
        return topic_model, topics
    
    def generate_results(self, 
                        topic_model: BERTopic,
                        documents: List[str],
                        topics: List[int],
                        metadata_df: pd.DataFrame):
        """
        ç”Ÿæˆæ‰€æœ‰åˆ†æç»“æœ
        
        Args:
            topic_model: è®­ç»ƒå¥½çš„æ¨¡å‹
            documents: æ–‡æ¡£åˆ—è¡¨
            topics: ä¸»é¢˜åˆ—è¡¨
            metadata_df: å…ƒæ•°æ®
        """
        logger.info("ç”Ÿæˆåˆ†æç»“æœ...")
        
        # 1. å°†ä¸»é¢˜æ·»åŠ åˆ°å…ƒæ•°æ®
        metadata_df['topic'] = topics
        metadata_df['topic_label'] = [
            f"Topic {t}" if t != -1 else "Outlier" 
            for t in topics
        ]
        
        # 2. ç”Ÿæˆå¹¶ä¿å­˜ä¸»é¢˜æ‘˜è¦
        self._save_topic_summary(topic_model)
        
        # 3. ç”Ÿæˆä¸»é¢˜å¯è§†åŒ–
        self._generate_topic_visualization(topic_model, documents)
        
        # 4. ç”Ÿæˆæ¥æºåˆ†æï¼ˆå¦‚æœæœ‰Sourceåˆ—ï¼‰
        if 'Source' in metadata_df.columns:
            self._generate_source_analysis(metadata_df, topic_model)
        
        # 5. ç”Ÿæˆæ—¶é—´æ¼”åŒ–åˆ†æï¼ˆå¦‚æœæœ‰æ—¥æœŸåˆ—ï¼‰
        if self.config['analysis']['time_analysis']['enable']:
            if 'æ—¥æœŸ' in metadata_df.columns:
                self._generate_temporal_analysis(
                    topic_model, documents, metadata_df
                )
        
        # 6. ç”Ÿæˆæ¡†æ¶çƒ­åŠ›å›¾ï¼ˆå¦‚æœæœ‰æ¡†æ¶åˆ—ï¼‰
        if self.config['analysis']['frame_analysis']['enable']:
            self._generate_frame_heatmap(metadata_df, topic_model)
        
        logger.info("âœ… æ‰€æœ‰åˆ†æç»“æœå·²ç”Ÿæˆ")
    
    def generate_enhanced_results(self, 
                                topic_model: BERTopic,
                                documents: List[str],
                                topics: List[int],
                                metadata_df: pd.DataFrame,
                                enhanced_topics: Optional[Dict] = None):
        """
        ç”Ÿæˆå¢å¼ºçš„åˆ†æç»“æœï¼Œé›†æˆæ‰€æœ‰æ–°æ¨¡å—
        
        Args:
            topic_model: è®­ç»ƒå¥½çš„æ¨¡å‹
            documents: æ–‡æ¡£åˆ—è¡¨
            topics: ä¸»é¢˜åˆ—è¡¨
            metadata_df: å…ƒæ•°æ®
            enhanced_topics: å¢å¼ºçš„ä¸»é¢˜è¡¨ç¤º
        """
        logger.info("ğŸš€ ç”Ÿæˆå¢å¼ºåˆ†æç»“æœ...")
        
        # 1. ç”ŸæˆåŸºç¡€ç»“æœ
        self.generate_results(topic_model, documents, topics, metadata_df)
        
        # 2. è·¨è¯­è¨€åˆ†æ
        logger.info("ğŸŒ æ‰§è¡Œè·¨è¯­è¨€ä¸»é¢˜æˆåˆ†åˆ†æ...")
        cross_lingual_results = self.cross_lingual_analyzer.run_full_cross_lingual_analysis(
            documents, topics
        )
        
        # 3. å­¦æœ¯çº§å›¾è¡¨ç”Ÿæˆ
        logger.info("ğŸ¨ ç”Ÿæˆå­¦æœ¯çº§å›¾è¡¨...")
        academic_charts = self.chart_generator.generate_all_academic_charts(
            topic_model, documents, topics, metadata_df, enhanced_topics
        )
        
        # 4. åŠ¨æ€æ¼”åŒ–åˆ†æï¼ˆå¦‚æœæœ‰æ—¶é—´æ•°æ®ï¼‰
        evolution_results = {}
        if 'æ—¥æœŸ' in metadata_df.columns and self.config['analysis']['time_analysis']['enable']:
            logger.info("ğŸ• æ‰§è¡ŒåŠ¨æ€ä¸»é¢˜æ¼”åŒ–åˆ†æ...")
            evolution_results = self.evolution_analyzer.run_full_evolution_analysis(
                topic_model, documents, metadata_df
            )
            
            # ç”Ÿæˆæ¼”åŒ–å›¾è¡¨
            if evolution_results.get('topics_over_time') is not None:
                topics_over_time = evolution_results['topics_over_time']
                if not topics_over_time.empty:
                    evolution_chart_path = self.chart_generator.generate_topic_evolution_chart(
                        topics_over_time
                    )
                    academic_charts['topic_evolution'] = evolution_chart_path
        
        # 5. æ›´æ–°ä¸»é¢˜æ‘˜è¦æ–‡ä»¶ï¼Œé›†æˆå¢å¼ºä¿¡æ¯
        self._save_enhanced_topic_summary(
            topic_model, enhanced_topics, cross_lingual_results.get('composition_df')
        )
        
        # 6. ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
        self._generate_comprehensive_report(
            topic_model, cross_lingual_results, evolution_results, academic_charts
        )
        
        logger.info("âœ¨ å¢å¼ºåˆ†æç»“æœç”Ÿæˆå®Œæˆï¼")
    
    def _save_enhanced_topic_summary(self,
                                   topic_model: BERTopic,
                                   enhanced_topics: Optional[Dict] = None,
                                   composition_df: Optional[pd.DataFrame] = None):
        """ä¿å­˜å¢å¼ºçš„ä¸»é¢˜æ‘˜è¦"""
        topic_info = topic_model.get_topic_info()
        
        # æ·»åŠ å¢å¼ºçš„å…³é”®è¯
        if enhanced_topics:
            topic_info['Enhanced_Keywords'] = topic_info['Topic'].apply(
                lambda x: ', '.join([word for word, _ in enhanced_topics.get(x, [])[:5]])
                if x in enhanced_topics else ''
            )
        
        # æ·»åŠ è¯­è¨€æ„æˆä¿¡æ¯
        if composition_df is not None and not composition_df.empty:
            # åˆ›å»ºè¯­è¨€ä¿¡æ¯å­—å…¸
            lang_info = {}
            for _, row in composition_df.iterrows():
                topic_id = row['Topic']
                lang_info[topic_id] = {
                    'Dominant_Language': row.get('Dominant_Language', ''),
                    'Topic_Type': row.get('Topic_Type', ''),
                    'Chinese_Percentage': row.get('Chinese_Percentage', 0),
                    'Russian_Percentage': row.get('Russian_Percentage', 0),
                    'English_Percentage': row.get('English_Percentage', 0)
                }
            
            # æ·»åŠ åˆ°ä¸»é¢˜ä¿¡æ¯ä¸­
            topic_info['Dominant_Language'] = topic_info['Topic'].apply(
                lambda x: lang_info.get(x, {}).get('Dominant_Language', '')
            )
            topic_info['Topic_Type'] = topic_info['Topic'].apply(
                lambda x: lang_info.get(x, {}).get('Topic_Type', '')
            )
            topic_info['Chinese_Pct'] = topic_info['Topic'].apply(
                lambda x: lang_info.get(x, {}).get('Chinese_Percentage', 0)
            )
            topic_info['Russian_Pct'] = topic_info['Topic'].apply(
                lambda x: lang_info.get(x, {}).get('Russian_Percentage', 0)
            )
            topic_info['English_Pct'] = topic_info['Topic'].apply(
                lambda x: lang_info.get(x, {}).get('English_Percentage', 0)
            )
        
        # æ·»åŠ åŸå§‹å…³é”®è¯ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        topic_info['Original_Keywords'] = topic_info['Topic'].apply(
            lambda x: ', '.join([word for word, _ in topic_model.get_topic(x)[:5]])
            if x != -1 else 'Outlier'
        )
        
        # ä¿å­˜å¢å¼ºçš„æ‘˜è¦æ–‡ä»¶
        enhanced_summary_path = self.results_paths['summary_file'].replace('.csv', '_enhanced.csv')
        topic_info.to_csv(enhanced_summary_path, index=False, encoding='utf-8-sig')
        logger.info(f"  âœ“ å¢å¼ºä¸»é¢˜æ‘˜è¦å·²ä¿å­˜: {enhanced_summary_path}")
    
    def _generate_comprehensive_report(self,
                                     topic_model: BERTopic,
                                     cross_lingual_results: Dict,
                                     evolution_results: Dict,
                                     academic_charts: Dict):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        report_path = Path(self.results_paths['output_dir']) / 'comprehensive_analysis_report.txt'
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write("BERTopic å¢å¼ºåˆ†æç»¼åˆæŠ¥å‘Š\n")
                f.write("=" * 80 + "\n\n")
                
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                # åŸºç¡€ç»Ÿè®¡
                topic_info = topic_model.get_topic_info()
                n_topics = len(topic_info) - 1
                f.write(f"ğŸ“Š åŸºç¡€ç»Ÿè®¡ä¿¡æ¯:\n")
                f.write(f"  è¯†åˆ«ä¸»é¢˜æ•°é‡: {n_topics} ä¸ª\n")
                f.write(f"  æ€»æ–‡æ¡£æ•°é‡: {topic_info['Count'].sum()} ä¸ª\n\n")
                
                # è·¨è¯­è¨€åˆ†æç»“æœ
                if cross_lingual_results.get('summary'):
                    summary = cross_lingual_results['summary']
                    f.write(f"ğŸŒ è·¨è¯­è¨€åˆ†æç»“æœ:\n")
                    f.write(f"  æ€»æ–‡æ¡£æ•°: {summary['total_documents']}\n")
                    f.write(f"  åˆ†æä¸»é¢˜æ•°: {summary['total_topics']}\n")
                    
                    lang_dist = summary['language_distribution']
                    f.write(f"  è¯­è¨€åˆ†å¸ƒ:\n")
                    f.write(f"    ä¸­æ–‡æ–‡æ¡£: {lang_dist['chinese']} ({lang_dist['chinese']/summary['total_documents']*100:.1f}%)\n")
                    f.write(f"    ä¿„æ–‡æ–‡æ¡£: {lang_dist['russian']} ({lang_dist['russian']/summary['total_documents']*100:.1f}%)\n")
                    f.write(f"    è‹±æ–‡æ–‡æ¡£: {lang_dist['english']} ({lang_dist['english']/summary['total_documents']*100:.1f}%)\n")
                    
                    f.write(f"  å…³é”®æ´å¯Ÿ:\n")
                    for insight in summary['key_insights']:
                        f.write(f"    â€¢ {insight}\n")
                    f.write("\n")
                
                # åŠ¨æ€æ¼”åŒ–åˆ†æç»“æœ
                if evolution_results.get('summary'):
                    evo_summary = evolution_results['summary']
                    f.write(f"ğŸ• åŠ¨æ€æ¼”åŒ–åˆ†æç»“æœ:\n")
                    f.write(f"  åˆ†ææ—¶é—´æ®µ: {evo_summary['analysis_period']}\n")
                    f.write(f"  æ—¶é—´ç‚¹æ•°é‡: {evo_summary['time_points']}\n")
                    f.write(f"  æ¼”åŒ–ä¸»é¢˜æ•°: {evo_summary['total_topics']}\n")
                    
                    if evolution_results.get('evolution_patterns'):
                        patterns = evolution_results['evolution_patterns']
                        f.write(f"  æ¼”åŒ–æ¨¡å¼:\n")
                        f.write(f"    ä¸Šå‡è¶‹åŠ¿ä¸»é¢˜: {len(patterns.get('rising_topics', []))} ä¸ª\n")
                        f.write(f"    ä¸‹é™è¶‹åŠ¿ä¸»é¢˜: {len(patterns.get('declining_topics', []))} ä¸ª\n")
                        f.write(f"    ç¨³å®šä¸»é¢˜: {len(patterns.get('stable_topics', []))} ä¸ª\n")
                        f.write(f"    æ³¢åŠ¨ä¸»é¢˜: {len(patterns.get('volatile_topics', []))} ä¸ª\n")
                    f.write("\n")
                
                # ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶
                if academic_charts:
                    f.write(f"ğŸ¨ ç”Ÿæˆçš„å­¦æœ¯çº§å›¾è¡¨:\n")
                    for chart_name, chart_path in academic_charts.items():
                        if chart_path:
                            f.write(f"  {chart_name}: {chart_path}\n")
                    f.write("\n")
                
                f.write("=" * 80 + "\n")
                f.write("æŠ¥å‘Šç»“æŸ\n")
                f.write("=" * 80 + "\n")
            
            logger.info(f"  âœ“ ç»¼åˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            
        except Exception as e:
            logger.error(f"  âœ— ç”Ÿæˆç»¼åˆæŠ¥å‘Šå¤±è´¥: {e}")
    
    def _create_embedding_model(self) -> SentenceTransformer:
        """åˆ›å»ºåµŒå…¥æ¨¡å‹"""
        model_name = self.model_params['embedding_model']
        logger.info(f"  â€¢ åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
        return SentenceTransformer(model_name)
    
    def _create_umap_model(self) -> UMAP:
        """åˆ›å»ºUMAPé™ç»´æ¨¡å‹"""
        params = self.model_params['umap_params']
        return UMAP(
            n_neighbors=params['n_neighbors'],
            n_components=params['n_components'],
            min_dist=params['min_dist'],
            metric=params['metric'],
            random_state=params['random_state']
        )
    
    def _create_hdbscan_model(self) -> HDBSCAN:
        """åˆ›å»ºHDBSCANèšç±»æ¨¡å‹"""
        params = self.model_params['hdbscan_params']
        return HDBSCAN(
            min_cluster_size=params['min_cluster_size'],
            min_samples=params['min_samples'],
            metric=params['metric'],
            cluster_selection_method=params['cluster_selection_method'],
            prediction_data=params['prediction_data']
        )
    
    def _create_vectorizer(self) -> CountVectorizer:
        """åˆ›å»ºå¢å¼ºçš„å‘é‡åŒ–å™¨ï¼Œé›†æˆä¸“å®¶çº§å…³é”®è¯æå–"""
        # ä½¿ç”¨ä¸“å®¶çº§å…³é”®è¯æå–å™¨åˆ›å»ºå¢å¼ºå‘é‡åŒ–å™¨
        enhanced_vectorizer = self.expert_extractor.create_enhanced_vectorizer()
        logger.info("  âœ“ åˆ›å»ºä¸“å®¶çº§å¢å¼ºå‘é‡åŒ–å™¨")
        return enhanced_vectorizer
    
    def _create_representation_model(self):
        """åˆ›å»ºè¡¨ç¤ºæ¨¡å‹ï¼ˆSOTAï¼šç»„åˆå¤šç§æ–¹æ³•ï¼‰"""
        # KeyBERTé£æ ¼çš„å…³é”®è¯æå–
        keybert = KeyBERTInspired()
        
        # æœ€å¤§è¾¹é™…ç›¸å…³æ€§
        mmr = MaximalMarginalRelevance(diversity=0.3)
        
        # ç»„åˆå¤šç§è¡¨ç¤ºæ–¹æ³•
        return [keybert, mmr]
    
    def _get_nr_topics(self) -> Optional[int]:
        """è·å–ä¸»é¢˜æ•°é‡è®¾ç½®"""
        nr_topics = self.model_params['nr_topics']
        if nr_topics == "auto" or nr_topics is None:
            return None
        return int(nr_topics)
    
    def _save_model(self, topic_model: BERTopic):
        """ä¿å­˜æ¨¡å‹"""
        model_path = self.results_paths['model_dir']
        Path(model_path).mkdir(parents=True, exist_ok=True)
        
        save_path = f"{model_path}/bertopic_model"
        topic_model.save(save_path, serialization="safetensors", save_ctfidf=True)
        logger.info(f"  âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    def _save_topic_summary(self, topic_model: BERTopic):
        """ä¿å­˜ä¸»é¢˜æ‘˜è¦"""
        topic_info = topic_model.get_topic_info()
        
        # æ·»åŠ æ›´å¤šæœ‰ç”¨ä¿¡æ¯
        topic_info['Top_Words'] = topic_info['Topic'].apply(
            lambda x: ', '.join([word for word, _ in topic_model.get_topic(x)[:5]])
            if x != -1 else 'Outlier'
        )
        
        # ä¿å­˜ä¸ºCSV
        summary_path = self.results_paths['summary_file']
        topic_info.to_csv(summary_path, index=False, encoding='utf-8-sig')
        logger.info(f"  âœ“ ä¸»é¢˜æ‘˜è¦å·²ä¿å­˜: {summary_path}")
    
    def _generate_topic_visualization(self, 
                                     topic_model: BERTopic, 
                                     documents: List[str]):
        """ç”Ÿæˆä¸»é¢˜å¯è§†åŒ–"""
        try:
            # ç”Ÿæˆä¸»é¢˜è·ç¦»å›¾
            fig = topic_model.visualize_topics()
            
            # ä¿å­˜ä¸ºHTML
            viz_path = self.results_paths['viz_file']
            fig.write_html(viz_path)
            logger.info(f"  âœ“ ä¸»é¢˜å¯è§†åŒ–å·²ä¿å­˜: {viz_path}")
            
        except Exception as e:
            logger.warning(f"  âš  ä¸»é¢˜å¯è§†åŒ–å¤±è´¥: {e}")
    
    def _generate_source_analysis(self, 
                                 metadata_df: pd.DataFrame,
                                 topic_model: BERTopic):
        """ç”Ÿæˆæ¥æºåˆ†æå›¾è¡¨"""
        try:
            # å‡†å¤‡æ•°æ®
            source_topic_df = metadata_df.groupby(['Source', 'topic_label']).size().reset_index(name='count')
            
            # è¿‡æ»¤æ‰ç¦»ç¾¤ç‚¹
            source_topic_df = source_topic_df[source_topic_df['topic_label'] != 'Outlier']
            
            # åˆ›å»ºå †å æŸ±çŠ¶å›¾
            fig = px.bar(
                source_topic_df, 
                x='Source', 
                y='count',
                color='topic_label',
                title='è®®é¢˜åœ¨ä¸åŒæ¥æºä¸­çš„åˆ†å¸ƒ',
                labels={'count': 'æ–‡æ¡£æ•°é‡', 'Source': 'æ¥æº', 'topic_label': 'ä¸»é¢˜'},
                color_discrete_sequence=px.colors.qualitative.Set3
            )
            
            fig.update_layout(
                xaxis_tickangle=-45,
                height=600,
                showlegend=True,
                legend=dict(
                    yanchor="top",
                    y=0.99,
                    xanchor="left",
                    x=0.01
                )
            )
            
            # ä¿å­˜
            source_path = self.results_paths['source_analysis']
            fig.write_html(source_path)
            logger.info(f"  âœ“ æ¥æºåˆ†æå·²ä¿å­˜: {source_path}")
            
        except Exception as e:
            logger.warning(f"  âš  æ¥æºåˆ†æå¤±è´¥: {e}")
    
    def _generate_temporal_analysis(self,
                                   topic_model: BERTopic,
                                   documents: List[str],
                                   metadata_df: pd.DataFrame):
        """ç”Ÿæˆæ—¶é—´æ¼”åŒ–åˆ†æ"""
        try:
            time_column = self.config['analysis']['time_analysis']['time_column']
            
            # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
            timestamps = pd.to_datetime(metadata_df[time_column])
            
            # ç”Ÿæˆä¸»é¢˜éšæ—¶é—´å˜åŒ–
            topics_over_time = topic_model.topics_over_time(
                documents,
                timestamps=timestamps,
                nr_bins=self.config['analysis']['time_analysis']['bins']
            )
            
            # åˆ›å»ºå¯è§†åŒ–
            fig = topic_model.visualize_topics_over_time(topics_over_time)
            
            # ä¿å­˜
            timeline_path = self.results_paths['timeline_analysis']
            fig.write_html(timeline_path)
            logger.info(f"  âœ“ æ—¶é—´æ¼”åŒ–åˆ†æå·²ä¿å­˜: {timeline_path}")
            
        except Exception as e:
            logger.warning(f"  âš  æ—¶é—´æ¼”åŒ–åˆ†æå¤±è´¥: {e}")
    
    def _generate_frame_heatmap(self, 
                               metadata_df: pd.DataFrame,
                               topic_model: BERTopic):
        """ç”Ÿæˆæ¡†æ¶çƒ­åŠ›å›¾"""
        try:
            # æŸ¥æ‰¾æ‰€æœ‰æ¡†æ¶åˆ—
            frame_columns = [col for col in metadata_df.columns 
                           if col.startswith('Frame_') and col.endswith('_Present')]
            
            if not frame_columns:
                logger.warning("  âš  æœªæ‰¾åˆ°æ¡†æ¶åˆ—ï¼Œè·³è¿‡æ¡†æ¶åˆ†æ")
                return
            
            # è®¡ç®—æ¯ä¸ªä¸»é¢˜çš„æ¡†æ¶ä½¿ç”¨é¢‘ç‡
            topic_frame_matrix = []
            topic_labels = []
            
            for topic in sorted(metadata_df['topic'].unique()):
                if topic == -1:  # è·³è¿‡ç¦»ç¾¤ç‚¹
                    continue
                    
                topic_data = metadata_df[metadata_df['topic'] == topic]
                frame_freq = topic_data[frame_columns].mean()
                topic_frame_matrix.append(frame_freq.values)
                topic_labels.append(f"Topic {topic}")
            
            # åˆ›å»ºçƒ­åŠ›å›¾æ•°æ®
            frame_names = [col.replace('Frame_', '').replace('_Present', '') 
                          for col in frame_columns]
            
            # ä½¿ç”¨plotlyåˆ›å»ºçƒ­åŠ›å›¾
            fig = go.Figure(data=go.Heatmap(
                z=topic_frame_matrix,
                x=frame_names,
                y=topic_labels,
                colorscale='RdBu_r',
                text=np.round(topic_frame_matrix, 2),
                texttemplate='%{text}',
                textfont={"size": 10},
                colorbar=dict(title="é¢‘ç‡")
            ))
            
            fig.update_layout(
                title='ä¸»é¢˜-æ¡†æ¶å…³è”çƒ­åŠ›å›¾',
                xaxis_title='å™äº‹æ¡†æ¶',
                yaxis_title='ä¸»é¢˜',
                height=max(400, len(topic_labels) * 30),
                xaxis={'tickangle': -45}
            )
            
            # ä¿å­˜
            heatmap_path = self.results_paths['frame_heatmap']
            fig.write_html(heatmap_path)
            logger.info(f"  âœ“ æ¡†æ¶çƒ­åŠ›å›¾å·²ä¿å­˜: {heatmap_path}")
            
        except Exception as e:
            logger.warning(f"  âš  æ¡†æ¶çƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {e}")