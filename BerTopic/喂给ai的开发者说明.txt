================================================================================
                   BERTopic å¢å¼ºè®®é¢˜åˆ†æç³»ç»Ÿ
                    AIå¼€å‘è€…å®Œæ•´æŠ€æœ¯æ–‡æ¡£ v2.0
================================================================================

ã€é¡¹ç›®çŠ¶æ€ã€‘PRODUCTION READY
- ç‰ˆæœ¬: v2.0 Enhanced Edition
- ä»£ç çŠ¶æ€: å®Œæ•´å®ç°ï¼Œæ‰€æœ‰æ¨¡å—å¯è¿è¡Œ
- æµ‹è¯•çŠ¶æ€: åŠŸèƒ½éªŒè¯å®Œæˆ
- éƒ¨ç½²çŠ¶æ€: æ”¯æŒæœ¬åœ°éƒ¨ç½²å’ŒWebç•Œé¢

ã€æ ¸å¿ƒæ¶æ„ã€‘
åŸºäºBERTopicçš„ä¼ä¸šçº§ä¸»é¢˜å»ºæ¨¡ç³»ç»Ÿï¼Œé›†æˆ5å¤§ä¸“ä¸šæ¨¡å—ï¼š
1. ä¸“å®¶çº§å…³é”®è¯æå– (Expert Keyword Extraction)
2. å‡ºç‰ˆçº§å­¦æœ¯å›¾è¡¨ (Academic Charts)  
3. åŠ¨æ€ä¸»é¢˜æ¼”åŒ– (Dynamic Evolution)
4. è·¨è¯­è¨€æˆåˆ†åˆ†æ (Cross-Lingual Analysis)
5. äº¤äº’å¼Webç•Œé¢ (Streamlit Web UI)

æŠ€æœ¯æ ˆï¼šPython 3.8+ | BERTopic 0.16.0 | spaCy | Streamlit | Plotly

================================================================================
                           å®Œæ•´æ–‡ä»¶ç»“æ„
================================================================================

bertopic/
â”œâ”€â”€ ğŸ¯ æ ¸å¿ƒå¼•æ“
â”‚   â”œâ”€â”€ main.py                         # ä¸»æ§åˆ¶å™¨ï¼Œæ‰§è¡Œå®Œæ•´pipeline
â”‚   â”œâ”€â”€ topic_analyzer/                 # æ ¸å¿ƒåˆ†ææ¨¡å—åŒ…
â”‚   â”‚   â”œâ”€â”€ __init__.py                # æ¨¡å—å¯¼å‡ºï¼šDataLoader, TopicAnalyzer
â”‚   â”‚   â”œâ”€â”€ data_loader.py             # Excel ETLå¤„ç†å™¨ï¼Œæ”¯æŒå¤šæºæ•°æ®åˆå¹¶
â”‚   â”‚   â”œâ”€â”€ model.py                   # BERTopicæ¨¡å‹å°è£…ï¼Œé›†æˆæ‰€æœ‰å¢å¼ºåŠŸèƒ½
â”‚   â”‚   â”œâ”€â”€ expert_keywords.py         # è¯æ€§æ ‡æ³¨+è‡ªå®šä¹‰åœç”¨è¯çš„å…³é”®è¯æå–
â”‚   â”‚   â”œâ”€â”€ academic_charts.py         # é«˜åˆ†è¾¨ç‡PNG/PDFå­¦æœ¯å›¾è¡¨ç”Ÿæˆ
â”‚   â”‚   â”œâ”€â”€ dynamic_evolution.py       # æ—¶é—´åºåˆ—ä¸»é¢˜æ¼”åŒ–åˆ†æ
â”‚   â”‚   â””â”€â”€ cross_lingual.py          # ä¸­è‹±ä¿„ä¸‰è¯­æ–‡æ¡£è¯­è¨€æ„æˆåˆ†æ
â”‚   â””â”€â”€ config.yaml                    # è¶…å‚æ•°é…ç½®ï¼šæ¨¡å‹+åˆ†æ+å¯è§†åŒ–å‚æ•°
â”œâ”€â”€ ğŸ’» ç”¨æˆ·ç•Œé¢
â”‚   â”œâ”€â”€ web_ui.py                      # Streamlit Webåº”ç”¨ï¼Œå®Œæ•´GUIç•Œé¢
â”‚   â”œâ”€â”€ quick_start.py                 # å‘½ä»¤è¡Œå¯åŠ¨å™¨ï¼Œç¯å¢ƒæ£€æµ‹+äº¤äº’å¼è¿è¡Œ
â”‚   â”œâ”€â”€ topic_labeler.py               # LLM APIé›†æˆï¼Œè‡ªåŠ¨ç”Ÿæˆä¸­æ–‡ä¸»é¢˜æ ‡ç­¾
â”‚   â””â”€â”€ validate_config.py             # é…ç½®æ–‡ä»¶éªŒè¯å·¥å…·
â”œâ”€â”€ ğŸ› ï¸ éƒ¨ç½²è„šæœ¬
â”‚   â”œâ”€â”€ å¯åŠ¨åˆ†æ.bat                   # Windowsä¸€é”®å®‰è£…+è¿è¡Œ
â”‚   â”œâ”€â”€ run_web_ui.bat                 # Webç•Œé¢å¯åŠ¨è„šæœ¬
â”‚   â””â”€â”€ requirements.txt               # å®Œæ•´ä¾èµ–æ¸…å•ï¼ŒåŒ…å«æ–°å¢æ¨¡å—
â”œâ”€â”€ ğŸ“Š èµ„æºæ–‡ä»¶
â”‚   â”œâ”€â”€ stopwords/                     # è‡ªå®šä¹‰åœç”¨è¯åº“
â”‚   â”‚   â””â”€â”€ politics_stopwords.txt    # æ”¿æ²»æ–°é—»ä¸“ç”¨åœç”¨è¯ï¼ˆä¸­è‹±ä¿„ä¸‰è¯­ï¼‰
â”‚   â”œâ”€â”€ data/ (ç”¨æˆ·åˆ›å»º)               # è¾“å…¥Excelæ–‡ä»¶ç›®å½•
â”‚   â””â”€â”€ results/ (è‡ªåŠ¨ç”Ÿæˆ)            # è¾“å‡ºç»“æœç›®å½•
â””â”€â”€ ğŸ“š æ–‡æ¡£
    â”œâ”€â”€ README.md                      # ç”¨æˆ·å‹å¥½çš„é¡¹ç›®è¯´æ˜
    â”œâ”€â”€ å¢å¼ºåŠŸèƒ½ä½¿ç”¨è¯´æ˜.md            # äº”å¤§æ¨¡å—è¯¦ç»†ä½¿ç”¨æŒ‡å—
    â””â”€â”€ å–‚ç»™aiçš„å¼€å‘è€…è¯´æ˜.txt         # æœ¬æ–‡ä»¶ï¼ŒAIæŠ€æœ¯å‚è€ƒ

================================================================================
                         æ•°æ®æµå’ŒAPIæ¥å£
================================================================================

ã€è¾“å…¥æ•°æ®è§„èŒƒã€‘
æ ¼å¼: Excel OOXML (.xlsx)
å¿…éœ€æ–‡ä»¶:
- data/åª’ä½“_æœ€ç»ˆåˆ†ææ•°æ®åº“.xlsx
- data/ç¤¾äº¤åª’ä½“_æœ€ç»ˆåˆ†ææ•°æ®åº“.xlsx

Schemaè¦æ±‚:
```python
# å¿…éœ€å­—æ®µ
text_column: str (default: "Incident")     # ä¸»æ–‡æœ¬å†…å®¹

# å¯é€‰å­—æ®µï¼ˆç”¨äºå¢å¼ºåˆ†æï¼‰  
Source: str                               # æ•°æ®æ¥æºæ ‡è¯†
æ—¥æœŸ: datetime                            # æ—¶é—´æˆ³ï¼Œç”¨äºæ¼”åŒ–åˆ†æ
speaker: str                             # å‘è¨€è€…ä¿¡æ¯
Valence: str                             # æƒ…æ„Ÿå€¾å‘
Frame_*_Present: bool                    # æ–°é—»æ¡†æ¶äºŒå€¼ç‰¹å¾
```

ã€è¾“å‡ºç»“æœAPIã€‘
results/ ç›®å½•ç»“æ„:
```
â”œâ”€â”€ åŸºç¡€åˆ†ææ–‡ä»¶
â”‚   â”œâ”€â”€ topics_summary.csv              # åŸå§‹ä¸»é¢˜æ‘˜è¦
â”‚   â”œâ”€â”€ topics_summary_enhanced.csv     # å¢å¼ºä¸»é¢˜æ‘˜è¦ï¼ˆåŒ…å«è¯­è¨€æ„æˆï¼‰
â”‚   â”œâ”€â”€ cross_lingual_composition.csv   # è·¨è¯­è¨€åˆ†æè¯¦ç»†æŠ¥å‘Š
â”‚   â”œâ”€â”€ dynamic_evolution_analysis.csv  # åŠ¨æ€æ¼”åŒ–æ•°æ®
â”‚   â””â”€â”€ comprehensive_analysis_report.txt # ç»¼åˆåˆ†ææ€»ç»“
â”œâ”€â”€ å­¦æœ¯çº§å›¾è¡¨ (PNG + PDF)
â”‚   â”œâ”€â”€ academic_topic_distribution.*   # äºŒç»´ä¸»é¢˜åˆ†å¸ƒå›¾
â”‚   â”œâ”€â”€ academic_topic_sizes.*          # ä¸»é¢˜è§„æ¨¡å—ä¸æ ¼å°”å›¾
â”‚   â”œâ”€â”€ academic_topic_evolution.*      # ä¸»é¢˜æ—¶é—´æ¼”åŒ–å›¾
â”‚   â””â”€â”€ cross_lingual_analysis.*        # è·¨è¯­è¨€æˆåˆ†å›¾
â”œâ”€â”€ äº¤äº’å¼å¯è§†åŒ– (HTML)
â”‚   â”œâ”€â”€ topic_visualization.html        # ä¸»é¢˜å…³ç³»äº¤äº’å›¾
â”‚   â”œâ”€â”€ topic_by_source.html           # æ¥æºå¯¹æ¯”å›¾è¡¨
â”‚   â”œâ”€â”€ topics_over_time.html          # æ—¶é—´æ¼”åŒ–äº¤äº’å›¾
â”‚   â””â”€â”€ topic_frame_heatmap.html       # æ¡†æ¶å…³è”çƒ­åŠ›å›¾
â””â”€â”€ æ¨¡å‹æ–‡ä»¶
    â””â”€â”€ trained_model/                  # BERTopicæ¨¡å‹åºåˆ—åŒ–æ–‡ä»¶
```

æ•°æ®æ ¼å¼è§„èŒƒ:
- CSV: UTF-8 encoding, pandas-compatible
- HTML: Plotly standalone, æ— å¤–éƒ¨ä¾èµ–
- PNG/PDF: 300 DPI, å­¦æœ¯å‡ºç‰ˆçº§è´¨é‡
- Model: safetensors + CTFIDF matrices

================================================================================
                         æ ¸å¿ƒç®—æ³•å®ç°
================================================================================

ã€ä¸»é¢˜å»ºæ¨¡Pipelineã€‘
```python
1. Data Loading (data_loader.py)
   â”œâ”€â”€ Excelè¯»å–: pandas.read_excel() 
   â”œâ”€â”€ æ•°æ®åˆå¹¶: concat/mergeç­–ç•¥
   â”œâ”€â”€ æ–‡æœ¬æ¸…æ´—: é•¿åº¦è¿‡æ»¤ã€ç¼–ç å¤„ç†
   â””â”€â”€ å…ƒæ•°æ®æå–: æ—¶é—´ã€æ¥æºã€æ¡†æ¶å­—æ®µ

2. Expert Keyword Extraction (expert_keywords.py)
   â”œâ”€â”€ è¯­è¨€æ£€æµ‹: langdetectè‡ªåŠ¨è¯†åˆ«ä¸­è‹±ä¿„
   â”œâ”€â”€ è¯æ€§æ ‡æ³¨: spaCy (zh_core_web_sm, en_core_web_sm, ru_core_news_sm)
   â”œâ”€â”€ çŸ­è¯­æ¨¡å¼: æ­£åˆ™åŒ¹é… '<n.*|a.*>*<n.*>+' (ä¸­æ–‡)
   â”œâ”€â”€ åœç”¨è¯è¿‡æ»¤: æ”¿æ²»æ–°é—»ä¸“ç”¨è¯åº“
   â””â”€â”€ å¢å¼ºå‘é‡åŒ–: CountVectorizer + è‡ªå®šä¹‰tokenizer

3. Topic Modeling (model.py)
   â”œâ”€â”€ åµŒå…¥: SentenceTransformerså¤šè¯­è¨€æ¨¡å‹
   â”œâ”€â”€ é™ç»´: UMAP (n_neighbors=15, n_components=5, cosine)
   â”œâ”€â”€ èšç±»: HDBSCAN (min_cluster_size=15, euclidean)
   â”œâ”€â”€ è¡¨ç¤º: KeyBERTInspired + MaximalMarginalRelevance
   â””â”€â”€ æ¦‚ç‡: calculate_probabilities=True

4. Enhanced Analysis
   â”œâ”€â”€ Cross-Lingual (cross_lingual.py): è¯­è¨€æ„æˆç»Ÿè®¡
   â”œâ”€â”€ Dynamic Evolution (dynamic_evolution.py): æ—¶é—´åºåˆ—åˆ†æ
   â”œâ”€â”€ Academic Charts (academic_charts.py): é«˜è´¨é‡é™æ€å›¾è¡¨
   â””â”€â”€ Comprehensive Report: è‡ªåŠ¨ç”Ÿæˆåˆ†ææ€»ç»“
```

ã€å…³é”®ç®—æ³•å‚æ•°ã€‘
```yaml
# æ ¸å¿ƒBERTopicé…ç½®
bertopic_params:
  embedding_model: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
  min_topic_size: 15                    # å¯è°ƒ: 5-100
  nr_topics: "auto"                     # å¯è®¾å…·ä½“æ•°å­—
  n_gram_range: [1, 3]                  # è¯æ±‡ç»„åˆèŒƒå›´
  
  # ä¸“å®¶çº§å…³é”®è¯æå–
  expert_keyword_extraction:
    enable_pos_patterns: true           # å¯ç”¨è¯æ€§æ¨¡å¼
    pos_patterns:
      zh: '<n.*|a.*>*<n.*>+'           # ä¸­æ–‡è¯­æ³•è§„åˆ™
      en: '<JJ.*>*<NN.*>+'             # è‹±æ–‡è¯­æ³•è§„åˆ™  
      ru: '<A.*>*<N.*>+'               # ä¿„æ–‡è¯­æ³•è§„åˆ™
    custom_stopwords_path: "stopwords/politics_stopwords.txt"
    use_custom_stopwords: true
    pos_language_detection: true        # è‡ªåŠ¨è¯­è¨€æ£€æµ‹

  # UMAPé™ç»´ä¼˜åŒ–
  umap_params:
    n_neighbors: 15                     # é‚»å±…æ•°ï¼Œå½±å“å±€éƒ¨vså…¨å±€å¹³è¡¡
    n_components: 5                     # é™ç»´ç»´åº¦
    min_dist: 0.0                       # åµŒå…¥è·ç¦»ï¼Œ0.0=ç´§å¯†èšç±»
    metric: "cosine"                    # è·ç¦»åº¦é‡ï¼Œé€‚åˆæ–‡æœ¬
    random_state: 42                    # å¯é‡ç°æ€§

  # HDBSCANèšç±»ä¼˜åŒ–  
  hdbscan_params:
    min_cluster_size: 15                # æœ€å°èšç±»å¤§å°
    min_samples: 5                      # æ ¸å¿ƒç‚¹é˜ˆå€¼
    metric: "euclidean"                 # èšç±»è·ç¦»
    cluster_selection_method: "eom"     # åŸºäºå¯†åº¦çš„é€‰æ‹©
    prediction_data: true               # æ”¯æŒæ–°æ–‡æ¡£é¢„æµ‹
```

================================================================================
                         å¢å¼ºæ¨¡å—è¯¦ç»†è¯´æ˜
================================================================================

ã€æ¨¡å—1: ä¸“å®¶çº§å…³é”®è¯æå–ã€‘
æ–‡ä»¶: topic_analyzer/expert_keywords.py
æ ¸å¿ƒç±»: ExpertKeywordExtractor

åŠŸèƒ½:
- åŸºäºspaCyçš„è¯æ€§æ ‡æ³¨ï¼Œè¯†åˆ«å®Œæ•´æ”¿æ²»æœ¯è¯­
- æ”¯æŒä¸­è‹±ä¿„ä¸‰è¯­æ··åˆæ–‡æœ¬å¤„ç†
- è‡ªå®šä¹‰åœç”¨è¯åº“è¿‡æ»¤æ— æ„ä¹‰è¯æ±‡
- å¢å¼ºBERTopicçš„CountVectorizer

å…³é”®æ–¹æ³•:
```python
detect_language(text: str) -> str              # è‡ªåŠ¨è¯­è¨€æ£€æµ‹
extract_pos_phrases(text: str) -> List[str]    # åŸºäºè¯æ€§çš„çŸ­è¯­æå–
create_enhanced_vectorizer() -> CountVectorizer # å¢å¼ºå‘é‡åŒ–å™¨
enhance_topic_representation() -> Dict         # ä¸»é¢˜è¡¨ç¤ºå¢å¼º
```

ä¾èµ–:
- spacy >= 3.4.0
- langdetect == 1.0.9
- è¯­è¨€æ¨¡å‹: zh_core_web_sm, en_core_web_sm, ru_core_news_sm

ã€æ¨¡å—2: å‡ºç‰ˆçº§å­¦æœ¯å›¾è¡¨ã€‘
æ–‡ä»¶: topic_analyzer/academic_charts.py  
æ ¸å¿ƒç±»: AcademicChartGenerator

åŠŸèƒ½:
- ç”Ÿæˆ300 DPIé«˜åˆ†è¾¨ç‡PNG + PDFå›¾è¡¨
- äºŒç»´ä¸»é¢˜åˆ†å¸ƒå›¾ï¼ˆt-SNEé™ç»´ + æ³¨é‡Šï¼‰
- å—ä¸æ ¼å°”ç«ç‘°å›¾ï¼ˆä¸»é¢˜è§„æ¨¡å¯è§†åŒ–ï¼‰
- ç¬¦åˆå­¦æœ¯è®ºæ–‡å‘è¡¨æ ‡å‡†

å…³é”®æ–¹æ³•:
```python
generate_topic_distribution_chart() -> str     # 2Dä¸»é¢˜åˆ†å¸ƒå›¾
generate_topic_size_chart() -> str            # ç«ç‘°å›¾è§„æ¨¡å›¾
generate_topic_evolution_chart() -> str       # æ—¶é—´æ¼”åŒ–å›¾
generate_heatmap_chart() -> str               # çƒ­åŠ›å›¾
```

å›¾è¡¨ç‰¹æ€§:
- å­—ä½“: Times New Roman serif
- åˆ†è¾¨ç‡: 300 DPI
- æ ¼å¼: PNG (åœ¨çº¿) + PDF (å°åˆ·)
- é…è‰²: å­¦æœ¯æœŸåˆŠæ ‡å‡†è‰²

ã€æ¨¡å—3: åŠ¨æ€ä¸»é¢˜æ¼”åŒ–ã€‘
æ–‡ä»¶: topic_analyzer/dynamic_evolution.py
æ ¸å¿ƒç±»: DynamicTopicEvolution

åŠŸèƒ½:
- BERTopicå®˜æ–¹topics_over_timeæ–¹æ³•é›†æˆ
- ä¸»é¢˜è¯ç”Ÿ/æ¶ˆäº¡æ—¶é—´ç‚¹æ£€æµ‹  
- æ¼”åŒ–æ¨¡å¼åˆ†ç±»ï¼ˆä¸Šå‡/ä¸‹é™/ç¨³å®š/æ³¢åŠ¨/å­£èŠ‚æ€§ï¼‰
- æ—¶é—´æ®µè‡ªé€‚åº”åˆ†ç®±

å…³é”®æ–¹æ³•:
```python
analyze_dynamic_topics() -> pd.DataFrame       # åŠ¨æ€ä¸»é¢˜åˆ†æ
analyze_topic_birth_death() -> Dict           # è¯ç”Ÿæ¶ˆäº¡åˆ†æ
detect_topic_evolution_patterns() -> Dict     # æ¼”åŒ–æ¨¡å¼æ£€æµ‹
```

åˆ†æç»´åº¦:
- æ—¶é—´è¶‹åŠ¿: çº¿æ€§å›å½’æ–œç‡åˆ†æ
- æ³¢åŠ¨æ€§: æ ‡å‡†å·®å’Œå˜å¼‚ç³»æ•°
- å­£èŠ‚æ€§: è‡ªç›¸å…³æ£€æµ‹
- ç”Ÿå‘½å‘¨æœŸ: é¦–æ¬¡/æœ€åå‡ºç°æ—¶é—´

ã€æ¨¡å—4: è·¨è¯­è¨€æˆåˆ†åˆ†æã€‘
æ–‡ä»¶: topic_analyzer/cross_lingual.py
æ ¸å¿ƒç±»: CrossLingualAnalyzer

åŠŸèƒ½:
- æ–‡æ¡£çº§è¯­è¨€è‡ªåŠ¨æ£€æµ‹ï¼ˆä¸­è‹±ä¿„+æ··åˆ+æœªçŸ¥ï¼‰
- ä¸»é¢˜çº§è¯­è¨€æ„æˆç»Ÿè®¡å’Œåˆ†ç±»
- ä¸­ä¿„å…³ç³»ç ”ç©¶ä¸“ç”¨çš„è®®é¢˜ç±»å‹è¯†åˆ«
- è¯­è¨€å¹³è¡¡åº¦å¯è§†åŒ–

å…³é”®æ–¹æ³•:
```python
detect_document_language(text: str) -> str     # å•æ–‡æ¡£è¯­è¨€æ£€æµ‹
analyze_topic_language_composition() -> DataFrame # ä¸»é¢˜è¯­è¨€æ„æˆ
generate_language_distribution_chart() -> str  # è¯­è¨€åˆ†å¸ƒå¯è§†åŒ–
```

è¯­è¨€æ£€æµ‹è§„åˆ™:
```python
language_patterns = {
    'zh': re.compile(r'[\u4e00-\u9fff]'),      # ä¸­æ–‡å­—ç¬¦
    'ru': re.compile(r'[\u0400-\u04ff]'),      # ä¿„æ–‡å­—ç¬¦
    'en': re.compile(r'^[a-zA-Z\s\d\.,!?\-\'\"]*$') # è‹±æ–‡å­—ç¬¦
}
```

ä¸»é¢˜åˆ†ç±»é€»è¾‘:
- Chinese-Dominant: ä¸­æ–‡å 70%+
- Russian-Dominant: ä¿„æ–‡å 70%+  
- Sino-Russian: ä¸­ä¿„æ–‡æ¡£åˆè®¡80%+
- Balanced-Multilingual: ä¸­ä¿„å·®å¼‚<20%ä¸”åˆè®¡50%+

ã€æ¨¡å—5: äº¤äº’å¼Webç•Œé¢ã€‘
æ–‡ä»¶: web_ui.py
æ ¸å¿ƒç±»: BERTopicWebUI

åŠŸèƒ½:
- Streamlitæ¡†æ¶çš„ç°ä»£åŒ–Webç•Œé¢
- æ‹–æ‹½æ–‡ä»¶ä¸Šä¼  + å®æ—¶å‚æ•°è°ƒèŠ‚
- è¿›åº¦æ¡æ˜¾ç¤º + ç»“æœåœ¨çº¿é¢„è§ˆ
- ä¸€é”®ä¸‹è½½æ‰€æœ‰ç»“æœæ–‡ä»¶

ç•Œé¢ç»“æ„:
```python
tabs = ["ğŸ“ æ•°æ®ä¸Šä¼ ", "âš™ï¸ å‚æ•°é…ç½®", "ğŸš€ è¿è¡Œåˆ†æ", "ğŸ“Š ç»“æœæŸ¥çœ‹"]
```

å…³é”®åŠŸèƒ½:
- æ–‡ä»¶éªŒè¯: Excelæ ¼å¼å’Œåˆ—åæ£€æŸ¥
- å‚æ•°é…ç½®: æ»‘å—/é€‰æ‹©æ¡†åŠ¨æ€è°ƒèŠ‚
- ä»»åŠ¡æ‰§è¡Œ: è¿›åº¦æ¡ + çŠ¶æ€æ˜¾ç¤º
- ç»“æœç®¡ç†: ZIPæ‰“åŒ…ä¸‹è½½

================================================================================
                         é…ç½®ç³»ç»Ÿè¯¦è§£
================================================================================

ã€config.yaml ç»“æ„ã€‘
```yaml
# æ•°æ®æ–‡ä»¶è·¯å¾„
data_paths:
  media_data: "data/åª’ä½“_æœ€ç»ˆåˆ†ææ•°æ®åº“.xlsx"
  social_media_data: "data/ç¤¾äº¤åª’ä½“_æœ€ç»ˆåˆ†ææ•°æ®åº“.xlsx"

# è¾“å‡ºè·¯å¾„é…ç½®
results_paths:
  output_dir: "results"
  summary_file: "results/topics_summary.csv"
  academic_charts:                      # æ–°å¢ï¼šå­¦æœ¯å›¾è¡¨è·¯å¾„
    topic_distribution: "results/academic_topic_distribution.png"
    topic_sizes: "results/academic_topic_sizes.png"

# æ•°æ®å¤„ç†é…ç½®
data_processing:
  text_column: "Incident"               # ä¸»æ–‡æœ¬åˆ—
  merge_strategy: "concat"              # æ•°æ®åˆå¹¶ç­–ç•¥
  metadata_columns:                     # ä¿ç•™çš„å…ƒæ•°æ®åˆ—
    - "Source"
    - "æ—¥æœŸ"  
    - "speaker"
    - "Valence"
    - "Frame_*_Present"

# BERTopicæ ¸å¿ƒå‚æ•°
bertopic_params:
  language: "multilingual"
  embedding_model: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
  min_topic_size: 15
  nr_topics: "auto"
  
  # æ–°å¢ï¼šä¸“å®¶çº§å…³é”®è¯æå–é…ç½®
  expert_keyword_extraction:
    enable_pos_patterns: true
    pos_patterns:
      zh: '<n.*|a.*>*<n.*>+'
      en: '<JJ.*>*<NN.*>+'  
      ru: '<A.*>*<N.*>+'
    custom_stopwords_path: "stopwords/politics_stopwords.txt"
    use_custom_stopwords: true

# åˆ†æåŠŸèƒ½å¼€å…³
analysis:
  time_analysis:
    enable: true
    time_column: "æ—¥æœŸ"
    bins: 10
  source_analysis:
    enable: true
  frame_analysis:
    enable: true
```

ã€é…ç½®éªŒè¯ã€‘
æ–‡ä»¶: validate_config.py
åŠŸèƒ½: é…ç½®æ–‡ä»¶å®Œæ•´æ€§å’Œå‚æ•°åˆç†æ€§æ£€æŸ¥

éªŒè¯é¡¹ç›®:
- æ–‡ä»¶è·¯å¾„å­˜åœ¨æ€§æ£€æŸ¥
- å‚æ•°èŒƒå›´åˆç†æ€§éªŒè¯  
- åˆ—åé…ç½®æ­£ç¡®æ€§æ£€æŸ¥
- ä¾èµ–åŒ…å®‰è£…çŠ¶æ€éªŒè¯

================================================================================
                         éƒ¨ç½²å’Œè¿è¡Œæ–¹å¼
================================================================================

ã€æ–¹å¼1: Windowsä¸€é”®éƒ¨ç½²ã€‘
è„šæœ¬: å¯åŠ¨åˆ†æ.bat
```batch
# è‡ªåŠ¨æ£€æµ‹condaç¯å¢ƒ
# é¦–æ¬¡è¿è¡Œ: åˆ›å»ºbertopic_labç¯å¢ƒ + å®‰è£…ä¾èµ–
# åç»­è¿è¡Œ: ç›´æ¥æ¿€æ´»ç¯å¢ƒ + æ‰§è¡Œåˆ†æ
conda activate bertopic_lab && python quick_start.py
```

ã€æ–¹å¼2: Webç•Œé¢æ¨¡å¼ã€‘  
è„šæœ¬: run_web_ui.bat
```batch
conda activate bertopic_lab && streamlit run web_ui.py
```

ã€æ–¹å¼3: æ‰‹åŠ¨å‘½ä»¤è¡Œã€‘
```bash
# ç¯å¢ƒå‡†å¤‡
conda create --name bertopic_lab python=3.9 -y
conda activate bertopic_lab
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple

# å®‰è£…è¯­è¨€æ¨¡å‹
python -m spacy download zh_core_web_sm
python -m spacy download en_core_web_sm
python -m spacy download ru_core_news_sm

# è¿è¡Œåˆ†æ
python main.py                    # ç›´æ¥è¿è¡Œ
python quick_start.py             # ç¯å¢ƒæ£€æŸ¥+äº¤äº’å¼
python web_ui.py                  # Webç•Œé¢ (éœ€streamlit run)
python topic_labeler.py           # åå¤„ç†æ ‡ç­¾ç”Ÿæˆ
```

ã€æ–¹å¼4: Dockerå®¹å™¨åŒ–ï¼ˆé¢„ç•™ï¼‰ã€‘
```dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "main.py"]
```

================================================================================
                         æ€§èƒ½å’Œä¼˜åŒ–
================================================================================

ã€è®¡ç®—å¤æ‚åº¦ã€‘
- åµŒå…¥: O(nÃ—mÃ—d) n=docs, m=tokens, d=embed_dim
- UMAP: O(n^1.14) è¿‘ä¼¼  
- HDBSCAN: O(nÃ—log(n)) å¹³å‡
- ä¸“å®¶æå–: O(nÃ—k) k=å¹³å‡æ–‡æ¡£é•¿åº¦
- è·¨è¯­è¨€åˆ†æ: O(n) çº¿æ€§
- æ€»ä½“: O(nÃ—mÃ—d + n^1.14)

ã€å®æµ‹æ€§èƒ½ (Intel i7, 16GB RAM)ã€‘
- 1K docs: ~2-5åˆ†é’Ÿ
- 10K docs: ~15-30åˆ†é’Ÿ
- 100K docs: ~2-4å°æ—¶  
- å†…å­˜éœ€æ±‚: ~2GB per 10K docs

ã€ä¼˜åŒ–å»ºè®®ã€‘
å¤§æ•°æ®é›†é…ç½®:
```yaml
bertopic_params:
  min_topic_size: 30              # æé«˜é˜ˆå€¼å‡å°‘è®¡ç®—
  calculate_probabilities: false  # ç¦ç”¨æ¦‚ç‡è®¡ç®—
  expert_keyword_extraction:
    pos_language_detection: false # å¦‚æœå·²çŸ¥è¯­è¨€å¯å…³é—­æ£€æµ‹
system:
  use_gpu: true                   # å¯ç”¨GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
```

å†…å­˜ä¼˜åŒ–:
- åˆ†æ‰¹å¤„ç†: å°†å¤§æ•°æ®é›†åˆ†å‰²ä¸ºå¤šä¸ªæ‰¹æ¬¡
- å‡å°‘ç‰¹å¾: é™ä½n_gram_rangeä¸Šé™
- æ¨¡å‹ç®€åŒ–: ä½¿ç”¨è¾ƒå°çš„åµŒå…¥æ¨¡å‹

================================================================================
                         é”™è¯¯å¤„ç†å’Œè°ƒè¯•
================================================================================

ã€æ—¥å¿—ç³»ç»Ÿã€‘
ä¸»æ—¥å¿—æ–‡ä»¶: bertopic_analysis.log
æ—¥å¿—çº§åˆ«: INFO, WARNING, ERROR
ç¼–ç : UTF-8

ã€å¸¸è§é”™è¯¯åŠè§£å†³ã€‘
1. spaCyæ¨¡å‹ç¼ºå¤±
   é”™è¯¯: "OSError: Can't find model 'zh_core_web_sm'"
   è§£å†³: python -m spacy download zh_core_web_sm

2. å†…å­˜ä¸è¶³
   é”™è¯¯: "MemoryError" / "OutOfMemoryError"  
   è§£å†³: å¢å¤§min_topic_sizeæˆ–åˆ†æ‰¹å¤„ç†

3. ç¼–ç é—®é¢˜
   é”™è¯¯: "UnicodeDecodeError"
   è§£å†³: ç¡®ä¿Excelæ–‡ä»¶UTF-8ç¼–ç 

4. ä¾èµ–ç‰ˆæœ¬å†²çª
   é”™è¯¯: "ImportError" / "AttributeError"
   è§£å†³: pip install -r requirements.txt --force-reinstall

5. Webç•Œé¢ç«¯å£å ç”¨
   é”™è¯¯: "OSError: [Errno 48] Address already in use"
   è§£å†³: streamlit run web_ui.py --server.port 8502

ã€è°ƒè¯•å·¥å…·ã€‘
- validate_config.py: é…ç½®æ–‡ä»¶æ£€æŸ¥
- quick_start.py: ç¯å¢ƒè¯Šæ–­
- æ—¥å¿—åˆ†æ: tail -f bertopic_analysis.log

================================================================================
                         æ‰©å±•å’ŒäºŒæ¬¡å¼€å‘
================================================================================

ã€æ¨¡å—åŒ–è®¾è®¡ã€‘
æ¯ä¸ªå¢å¼ºæ¨¡å—éƒ½æ˜¯ç‹¬ç«‹çš„ç±»ï¼Œå¯ä»¥å•ç‹¬ä½¿ç”¨:
```python
from topic_analyzer.expert_keywords import ExpertKeywordExtractor
from topic_analyzer.cross_lingual import CrossLingualAnalyzer

# å•ç‹¬ä½¿ç”¨è·¨è¯­è¨€åˆ†æ
analyzer = CrossLingualAnalyzer(config)
results = analyzer.run_full_cross_lingual_analysis(documents, topics)
```

ã€APIæ‰©å±•ç‚¹ã€‘
1. è‡ªå®šä¹‰åµŒå…¥æ¨¡å‹
```python
custom_model = SentenceTransformer('your-model-name')
```

2. è‡ªå®šä¹‰é¢„å¤„ç†pipeline
```python
def custom_preprocessor(texts: List[str]) -> List[str]:
    return [preprocess(text) for text in texts]
```

3. è‡ªå®šä¹‰ä¸»é¢˜è¡¨ç¤º
```python
from bertopic.representation import MaximalMarginalRelevance
custom_repr = MaximalMarginalRelevance(diversity=0.5)
```

4. è‡ªå®šä¹‰å¯è§†åŒ–
```python
def custom_visualization(topic_model, documents):
    # å®ç°è‡ªå®šä¹‰å›¾è¡¨é€»è¾‘
    pass
```

ã€æ•°æ®æ¥å£æ‰©å±•ã€‘
- æ”¯æŒJSON/CSVè¾“å…¥: ä¿®æ”¹data_loader.py
- æ”¯æŒæ•°æ®åº“è¿æ¥: æ·»åŠ SQLAlchemyé›†æˆ
- æ”¯æŒå®æ—¶æµæ•°æ®: æ·»åŠ Kafka/Redisé›†æˆ
- æ”¯æŒAPIæœåŠ¡: æ·»åŠ FastAPIå°è£…

ã€æ–°å¢è¯­è¨€æ”¯æŒã€‘
1. åœ¨pos_patternsä¸­æ·»åŠ æ–°è¯­è¨€è¯­æ³•è§„åˆ™
2. ä¸‹è½½å¯¹åº”çš„spaCyè¯­è¨€æ¨¡å‹
3. æ›´æ–°language_patternsæ­£åˆ™è¡¨è¾¾å¼
4. åœ¨custom_stopwordsä¸­æ·»åŠ æ–°è¯­è¨€åœç”¨è¯

================================================================================
                         å­¦æœ¯å’Œå•†ä¸šåº”ç”¨
================================================================================

ã€å­¦æœ¯ç ”ç©¶åº”ç”¨ã€‘
- å›½é™…å…³ç³»: ä¸­ä¿„å…³ç³»è®®é¢˜æ¼”åŒ–åˆ†æ
- ä¼ æ’­å­¦: åª’ä½“æ¡†æ¶å’Œè®®ç¨‹è®¾ç½®ç ”ç©¶
- æ”¿æ²»å­¦: æ”¿ç­–è®®é¢˜åˆ†ç±»å’Œèˆ†è®ºåˆ†æ
- ç¤¾ä¼šå­¦: è·¨æ–‡åŒ–äº¤æµæ¨¡å¼ç ”ç©¶

ã€å•†ä¸šåº”ç”¨åœºæ™¯ã€‘  
- åª’ä½“ç›‘æµ‹: å“ç‰ŒæåŠå’Œå±æœºé¢„è­¦
- å¸‚åœºç ”ç©¶: æ¶ˆè´¹è€…å…³æ³¨ç‚¹å˜åŒ–è¿½è¸ª
- ç«äº‰åˆ†æ: è¡Œä¸šè®®é¢˜çƒ­ç‚¹å‘ç°
- å†…å®¹ç­–ç•¥: å†…å®¹ä¸»é¢˜è§„åˆ’å’Œä¼˜åŒ–

ã€è¾“å‡ºæ ‡å‡†ã€‘
- å­¦æœ¯è®ºæ–‡: 300 DPI PNG/PDFå›¾è¡¨
- ç ”ç©¶æŠ¥å‘Š: è¯¦ç»†çš„CSVæ•°æ®è¡¨
- å•†ä¸šå±•ç¤º: äº¤äº’å¼HTMLå¯è§†åŒ–
- æŠ€æœ¯æ–‡æ¡£: å®Œæ•´çš„åˆ†ææ—¥å¿—

ã€å¼•ç”¨æ ¼å¼ã€‘
BibTeX:
```bibtex
@software{bertopic_enhanced_2025,
  title={BERTopic Enhanced Analysis System v2.0},
  author={Development Team},
  year={2025},
  note={Enhanced with Expert Keyword Extraction, Academic Visualization, 
        Dynamic Evolution Analysis, Cross-Lingual Topic Composition, and Web UI}
}
```

================================================================================
                           å¼€å‘è€…å¿«é€Ÿå‚è€ƒ
================================================================================

ã€æ ¸å¿ƒç±»å¯¼å…¥ã€‘
```python
from topic_analyzer import DataLoader, TopicAnalyzer
from topic_analyzer.expert_keywords import ExpertKeywordExtractor
from topic_analyzer.academic_charts import AcademicChartGenerator  
from topic_analyzer.dynamic_evolution import DynamicTopicEvolution
from topic_analyzer.cross_lingual import CrossLingualAnalyzer
```

ã€æœ€å°å¯è¿è¡Œç¤ºä¾‹ã€‘
```python
import yaml
from topic_analyzer import DataLoader, TopicAnalyzer

# åŠ è½½é…ç½®
with open('config.yaml', 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

# æ•°æ®åŠ è½½
data_loader = DataLoader(config)
documents, metadata_df = data_loader.load_and_prepare_data()

# æ¨¡å‹è®­ç»ƒ
analyzer = TopicAnalyzer(config)
topic_model, topics = analyzer.train_bertopic_model(documents)

# å¢å¼ºåˆ†æ
enhanced_topics = analyzer.expert_extractor.enhance_topic_representation(
    topic_model, documents
)
analyzer.generate_enhanced_results(
    topic_model, documents, topics, metadata_df, enhanced_topics
)
```

ã€å…³é”®é…ç½®å‚æ•°ã€‘
```python
# æ€§èƒ½ä¼˜åŒ–
min_topic_size = max(10, len(documents) // 200)

# è´¨é‡ä¼˜åŒ–  
enable_pos_patterns = True
use_custom_stopwords = True

# è¾“å‡ºæ§åˆ¶
calculate_probabilities = False  # å¤§æ•°æ®é›†æ—¶ç¦ç”¨
save_intermediate = False        # è°ƒè¯•æ—¶å¯ç”¨
```

ã€å¸¸ç”¨è°ƒè¯•å‘½ä»¤ã€‘
```bash
# éªŒè¯é…ç½®
python validate_config.py

# ç¯å¢ƒæ£€æŸ¥  
python quick_start.py

# ç›´æ¥è¿è¡Œ
python main.py

# Webç•Œé¢
streamlit run web_ui.py

# åå¤„ç†æ ‡ç­¾
python topic_labeler.py
```

================================================================================
                               ç‰ˆæœ¬å†å²
================================================================================

v2.0.0 (å½“å‰ç‰ˆæœ¬) - Enhanced Edition
âœ… ä¸“å®¶çº§å…³é”®è¯æå–æ¨¡å— (expert_keywords.py)
âœ… å‡ºç‰ˆçº§å­¦æœ¯å›¾è¡¨ç”Ÿæˆ (academic_charts.py)  
âœ… åŠ¨æ€ä¸»é¢˜æ¼”åŒ–åˆ†æ (dynamic_evolution.py)
âœ… è·¨è¯­è¨€æˆåˆ†åˆ†æ (cross_lingual.py)
âœ… äº¤äº’å¼Webç•Œé¢ (web_ui.py)
âœ… æ”¿æ²»æ–°é—»åœç”¨è¯åº“ (politics_stopwords.txt)
âœ… å®Œæ•´çš„é…ç½®éªŒè¯ç³»ç»Ÿ
âœ… ç»¼åˆåˆ†ææŠ¥å‘Šç”Ÿæˆ

v1.0.0 - åŸºç¡€ç‰ˆæœ¬
âœ… åŸºç¡€BERTopicä¸»é¢˜å»ºæ¨¡
âœ… ç®€å•HTMLå¯è§†åŒ–è¾“å‡º
âœ… å‘½ä»¤è¡Œç•Œé¢æ“ä½œ

è®¡åˆ’åŠŸèƒ½ (v3.0):
ğŸ”„ å®æ—¶æ•°æ®æµå¤„ç†
ğŸ”„ æ›´å¤šå¯è§†åŒ–é€‰é¡¹ (3Då›¾è¡¨ã€åŠ¨ç”»)
ğŸ”„ APIæœåŠ¡åŒ–éƒ¨ç½² (FastAPI)
ğŸ”„ å¤šè¯­è¨€ç•Œé¢æ”¯æŒ
ğŸ”„ äº‘ç«¯éƒ¨ç½²æ”¯æŒ (Docker + K8s)

================================================================================
                               æ–‡æ¡£ç»“æŸ
================================================================================

æœ€åæ›´æ–°: 2025å¹´9æœˆ20æ—¥
æ–‡æ¡£ç‰ˆæœ¬: v2.0 Complete Technical Reference
é¡¹ç›®çŠ¶æ€: Production Ready - æ‰€æœ‰åŠŸèƒ½å·²å®ç°å¹¶å¯æŠ•å…¥ä½¿ç”¨

æœ¬æ–‡æ¡£ä¸ºAIå¼€å‘è€…æä¾›å®Œæ•´çš„æŠ€æœ¯å‚è€ƒï¼ŒåŒ…å«æ‰€æœ‰å®ç°ç»†èŠ‚ã€é…ç½®å‚æ•°ã€
APIæ¥å£ã€æ‰©å±•æ–¹æ³•å’Œè°ƒè¯•ä¿¡æ¯ã€‚åŸºäºæ­¤æ–‡æ¡£ï¼ŒAIå¯ä»¥ç«‹å³ç†è§£å’Œæ“ä½œæ•´ä¸ªé¡¹ç›®ã€‚