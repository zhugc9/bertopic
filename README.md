# 🚀 BERTopic 专家级主题分析系统

**SOTA技术 · KISS设计 · 专业级分析**

=============================================

## ✨ 系统特色

🎯 **专家级关键词提取**：基于词性标注(PoS)的智能短语识别 + 自定义领域停用词  
📊 **出版级学术图表**：高分辨率PNG/PDF图表，直接用于论文发表  
🔄 **动态主题演化**：时间序列分析，追踪主题变化趋势  
🌍 **跨语言成分分析**：中英俄多语言混合文本的语言分布统计  
🎨 **完整可视化界面**：4类交互式图表 + 实时参数调优建议  

## 🚀 一键启动
```bash
双击 run_web_ui.bat → 访问 http://localhost:8502
```
**4步完成分析**：📁上传Excel → ⚙️选择模式 → 🚀开始分析 → 📊查看结果

## 💡 核心原理
**主题分析**：自动发现文本中的主要话题和讨论主题。**技术流程**：文本嵌入 → 降维聚类 → 关键词提取 → 主题命名。**应用场景**：新闻分析、舆情监测、内容分类、趋势发现。

## 📊 数据要求  
**格式**：.xlsx文件，包含文本列(Incident/Text/Content/文本)，建议500-5000条，支持中英俄文。**示例**：`文本|日期|来源` → `政策新闻|2024-01-01|人民网`

=============================================

## 🤖 智能调参系统

**智能推荐模式**：自动分析数据规模、语言分布、文本复杂度、词汇多样性，智能推荐最优参数组合。**使用**：上传文件 → 选择智能推荐 → 点击智能分析 → 查看推荐参数 → 自动应用。**适用**：新手用户、复杂数据、快速分析。

**手动配置模式**：完全自定义所有参数，包括主题大小、UMAP降维、HDBSCAN聚类、N-gram范围等。**适用**：高级用户、特定需求、精细控制。

=============================================

## ⚙️ 核心参数详解

**最小主题大小(min_topic_size)**：决定主题包含的最少文档数。小值→细粒度多主题，大值→宏观少主题。**公式**：max(5, 文档数×0.01~0.05)。**案例**：500条→10-15，2000条→20-30，10000条→50-100。**策略**：初始值=文档数÷100，主题太多则增大，太少则减小。

**目标主题数(nr_topics)**：auto模式(推荐)基于数据结构自动确定最优数量。手动设置公式：√(文档数量)。**规模建议**：500-1000条→5-15个，1000-3000条→10-25个，3000条以上→15-40个。**场景**：新闻8-12个，学术15-20个，社交20-25个。**手动设置**：已知领域分类、特定粒度需求、历史对比。

**N-gram范围(n_gram_range)**：决定关键词组合长度。**[1,1]**单词模式快速但语义模糊，**[1,2]**标准模式(推荐)平衡速度精度，**[1,3]**精细模式详细但成本高，**[2,3]**短语模式专注复合概念。**策略**：新闻/社交[1,2]，学术[1,3]，法律[2,3]。**语言**：中文[1,2]，英文[1,3]，俄文[1,2]。

**UMAP降维参数**：将高维嵌入降维为聚类准备。**邻居数(n_neighbors)**控制局部vs全局平衡：小值(5-10)细节结构但碎片化，中值(15-25)平衡标准，大值(30-50)全局稳定。**公式**：min(50,max(5,√文档数))。**维度(n_components)**：2-3维快速但信息损失，4-6维(推荐)平衡性能，7-10维高精度高成本。**策略**：简单数据3-4维，标准5维，复杂6-8维。

**HDBSCAN聚类参数**：基于密度的层次聚类识别任意形状主题簇。**最小聚类大小**：与min_topic_size保持一致。**最小样本数(min_samples)**决定聚类严格程度：小值(1-3)宽松包容但可能噪音，中值(4-7)平衡稳定，大值(8-15)严格高质量。**公式**：基础值=聚类大小÷3，高重复率÷2(严格)，低重复率÷4(宽松)。**案例**：新闻20→7，社交15→8，学术25→6。

**高级文本处理**：**停用词系统**自动过滤中英俄功能词，支持领域自定义(新闻、学术、社交专用词表)。**词性标注**基于NLP提取有意义词汇组合：中文(名词|形容词)*名词+，英文形容词*名词+，俄文形容词*名词+。**效果**：从"经济、的、快速、发展"提升为"经济发展、快速发展、经济增长"。

=============================================

## 📋 场景化参数配置

**新闻媒体**：主题20-30|auto|[1,2]|邻居15|维度5 → 标准政经社会分类  
**社交媒体**：主题15-25|auto|[1,2]|邻居10|维度3 → 快速话题识别  
**学术论文**：主题25-40|auto|[1,3]|邻居20|维度8 → 专业术语提取  
**客服对话**：主题10-20|auto|[1,2]|邻居12|维度4 → 问题分类优化

=============================================

## 🔧 问题诊断与解决

**主题过多过细**(>50个相似主题)：增大主题大小×2，增大邻居数+5~10，删除重复文档。**主题过少过粗**(<5个异质主题)：减小主题大小÷2，减小邻居数-5，增加维度+2~3。**关键词质量差**(功能词/乱码)：启用停用词过滤和词性标注，调整N-gram为[1,2]，清理数据源。

**噪音主题过多**(主题-1>30%)：优化数据质量(删重复/过滤短文本/统一编码)，减小min_samples，启用智能分词。**性能内存问题**：数据采样<8000条，文本截断<1000字符，降低维度到3，关闭其他程序。**时间过长**：<1000条1-3分钟标准参数，>10000条数据采样，使用快速配置(邻居10/维度3/样本3)。

=============================================

## 📊 结果解读

### 📁 **完整输出文件列表**
```
results/
├── topics_summary.csv                      # 基础主题摘要
├── document_topic_mapping.csv              # 文档-主题完整映射
├── cross_lingual_composition.csv           # 跨语言成分分析
├── dynamic_evolution_analysis.csv          # 动态演化分析
├── analysis_metadata.json                  # 分析元数据
├── academic_topic_distribution.png         # 学术级主题分布图
├── academic_topic_sizes.png                # 学术级主题规模图
├── academic_topic_evolution.png            # 学术级演化图
├── academic_cross_lingual.png              # 学术级跨语言图
└── trained_model/bertopic_model.pkl        # 完整训练模型
```

### 📈 **可视化解读**
**主题分布图**：圆圈=主题，大小=文档数，距离=相似度，颜色=语言分布  
**Outlier分析**：<5%优秀，5-10%良好，10-15%需优化，>15%严重问题  
**跨语言成分**：每个主题的中英俄文档比例，识别语言特色议题  
**动态演化**：主题随时间的兴衰变化，突发事件检测  

### ✅ **质量评估标准**
**优秀分析**：Outlier<10%，主题数=文档数/80±20%，关键词语义连贯  
**需要优化**：Outlier>15%，主题过多/过少，关键词截断/无意义

=============================================

## 💡 最佳实践

**数据准备检查单**：UTF-8编码统一，删除重复文档(相似度>90%)，过滤短文本(<10字符)，处理缺失值，语言一致性检查。**数据规模**：探索500-1500条，标准1500-5000条，详细5000-10000条，大规模>10000条需采样。

**调参工作流**：数据特征分析→智能推荐参数→质量评估→人工微调→效果确认。**调参原则**：智能推荐优先，单参数调整，小步快跑，记录结果。**业务价值**：主题映射业务分类，识别趋势热点，监测舆情风险，指导内容策略。

