"""
BERTopicæ¨¡å‹è®­ç»ƒä¸åˆ†ææ¨¡å—
==========================
SOTAå®ç°ï¼šä½¿ç”¨æœ€æ–°çš„BERTopicç‰¹æ€§å’Œä¼˜åŒ–æŠ€æœ¯
"""

import pandas as pd
import numpy as np
import logging
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# BERTopicæ ¸å¿ƒ
from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance

# ä¸“å®¶çº§å…³é”®è¯æå–
from .expert_keywords import ExpertKeywordExtractor

# å­æ¨¡å—å¯¼å…¥
from .visualizations import VisualizationGenerator
from .dynamic_evolution import DynamicTopicEvolution
from .cross_lingual import CrossLingualAnalyzer
from .hyperparameter_optimizer import OptunaBERTopicOptimizer
from .multilingual_preprocessor import EnhancedMultilingualVectorizer
from .results_generator import ResultsGenerator

# é™ç»´å’Œèšç±»
from umap import UMAP
from hdbscan import HDBSCAN

# åµŒå…¥æ¨¡å‹
from sentence_transformers import SentenceTransformer

# å‘é‡åŒ–
from sklearn.feature_extraction.text import CountVectorizer

# å¯è§†åŒ–
import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
import matplotlib.pyplot as plt
import seaborn as sns

logger = logging.getLogger(__name__)


class TopicAnalyzer:
    """ä¸»é¢˜åˆ†æå™¨ç±» - SOTAå®ç°"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–ä¸»é¢˜åˆ†æå™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.model_params = config['bertopic_params']
        self.viz_params = config['visualization']
        self.results_paths = config['results_paths']
        
        # åˆå§‹åŒ–ä¸“å®¶çº§å…³é”®è¯æå–å™¨
        self.expert_extractor = ExpertKeywordExtractor(config)
        
        # åˆå§‹åŒ–å¯è§†åŒ–ç”Ÿæˆå™¨
        self.visualizer = VisualizationGenerator(config)
        
        # åˆå§‹åŒ–åŠ¨æ€æ¼”åŒ–åˆ†æå™¨
        self.evolution_analyzer = DynamicTopicEvolution(config)
        
        # åˆå§‹åŒ–è·¨è¯­è¨€åˆ†æå™¨
        self.cross_lingual_analyzer = CrossLingualAnalyzer(config)
        
        # åˆå§‹åŒ–è¶…å‚æ•°ä¼˜åŒ–å™¨
        self.hyperparameter_optimizer = OptunaBERTopicOptimizer(config)
        
        # åˆå§‹åŒ–å¤šè¯­è¨€é¢„å¤„ç†å™¨
        self.multilingual_vectorizer = EnhancedMultilingualVectorizer(config)
        
        # åˆå§‹åŒ–ç»“æœç”Ÿæˆå™¨
        self.results_generator = ResultsGenerator(config)

        self.random_seed = self.config.get('system', {}).get('random_seed', 42)
        
        # è®¾ç½®matplotlibä¸­æ–‡æ”¯æŒ
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
    def train_bertopic_model(self, 
                           documents: List[str]) -> Tuple[BERTopic, List[int]]:
        """
        è®­ç»ƒBERTopicæ¨¡å‹
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            topic_model: è®­ç»ƒå¥½çš„æ¨¡å‹
            topics: æ¯ä¸ªæ–‡æ¡£çš„ä¸»é¢˜ç¼–å·
        """
        logger.info("åˆå§‹åŒ–BERTopicç»„ä»¶...")
        
        # 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆSOTAï¼šå¤šè¯­è¨€æ”¯æŒï¼‰
        embedding_model = self._create_embedding_model()
        
        # 2. åˆå§‹åŒ–UMAPé™ç»´ï¼ˆSOTAä¼˜åŒ–å‚æ•°ï¼‰
        umap_model = self._create_umap_model()
        
        # 3. åˆå§‹åŒ–HDBSCANèšç±»ï¼ˆSOTAä¼˜åŒ–å‚æ•°ï¼‰
        hdbscan_model = self._create_hdbscan_model()
        
        # 4. åˆå§‹åŒ–å‘é‡åŒ–å™¨ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰
        vectorizer_model = self._create_vectorizer()
        
        # 5. åˆå§‹åŒ–è¡¨ç¤ºæ¨¡å‹ï¼ˆSOTAï¼šç»„åˆå¤šç§è¡¨ç¤ºæ–¹æ³•ï¼‰
        representation_model = self._create_representation_model()
        
        # 6. åˆ›å»ºBERTopicæ¨¡å‹
        topic_model = BERTopic(
            embedding_model=embedding_model,
            umap_model=umap_model,
            hdbscan_model=hdbscan_model,
            vectorizer_model=vectorizer_model,
            representation_model=representation_model,
            nr_topics=self._get_nr_topics(),
            top_n_words=10,
            verbose=self.config['system']['verbose'],
            calculate_probabilities=True  # SOTAï¼šè®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        )
        
        # 7. è®­ç»ƒæ¨¡å‹
        logger.info(f"å¼€å§‹è®­ç»ƒæ¨¡å‹ (æ–‡æ¡£æ•°: {len(documents)})...")
        topics, probs = topic_model.fit_transform(documents)
        
        # 8. ä¿å­˜æ¨¡å‹
        self._save_model(topic_model)
        
        # ç»Ÿè®¡ä¿¡æ¯
        topic_info = topic_model.get_topic_info()
        n_topics = len(topic_info) - 1  # å‡å»ç¦»ç¾¤ç‚¹
        logger.info(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ: å‘ç° {n_topics} ä¸ªä¸»é¢˜")
        
        return topic_model, topics
    
    def analyze_document_languages(self, documents: List[str]) -> Dict[str, Any]:
        """
        åˆ†ææ–‡æ¡£çš„è¯­è¨€åˆ†å¸ƒ
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            è¯­è¨€åˆ†æç»“æœ
        """
        logger.info("ğŸŒ åˆ†ææ–‡æ¡£è¯­è¨€åˆ†å¸ƒ...")
        
        language_stats = self.multilingual_vectorizer.get_language_statistics(documents)
        
        # æ˜¾ç¤ºåˆ†æç»“æœ
        logger.info("ğŸ“Š è¯­è¨€åˆ†å¸ƒç»Ÿè®¡:")
        for lang, stats in language_stats['language_distribution'].items():
            lang_names = {'zh': 'ä¸­æ–‡', 'en': 'è‹±æ–‡', 'ru': 'ä¿„æ–‡', 'unknown': 'æœªçŸ¥'}
            lang_name = lang_names.get(lang, lang)
            logger.info(f"  {lang_name}: {stats['count']} ä¸ªæ–‡æ¡£ ({stats['percentage']:.1f}%)")
        
        return language_stats
    
    def generate_sota_visualizations(self, 
                                    topic_model: BERTopic,
                                    documents: List[str],
                                    topics: List[int],
                                    metadata_df: Optional[pd.DataFrame] = None) -> Dict[str, str]:
        """
        ç”ŸæˆSOTAçº§å­¦æœ¯å¯è§†åŒ–å›¾è¡¨
        
        Args:
            topic_model: è®­ç»ƒå¥½çš„æ¨¡å‹
            documents: æ–‡æ¡£åˆ—è¡¨
            topics: ä¸»é¢˜åˆ†é…
            metadata_df: å…ƒæ•°æ®DataFrame
            
        Returns:
            ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        logger.info("ğŸ¨ ç”ŸæˆSOTAçº§å­¦æœ¯å¯è§†åŒ–å›¾è¡¨...")
        
        # å‡†å¤‡æ—¶é—´æ•°æ®
        timestamps = None
        if metadata_df is not None:
            # å°è¯•ä»å…ƒæ•°æ®ä¸­æå–æ—¶é—´ä¿¡æ¯
            date_columns = ['æ—¥æœŸ', 'Date', 'date', 'timestamp', 'time']
            for col in date_columns:
                if col in metadata_df.columns:
                    try:
                        timestamps = pd.to_datetime(metadata_df[col], errors='coerce').tolist()
                        break
                    except:
                        continue
        
        # ç”Ÿæˆæ‰€æœ‰SOTAå›¾è¡¨
        charts = self.visualizer.generate_all_visualizations(
            topic_model=topic_model,
            documents=documents,
            topics=topics,
            metadata_df=metadata_df,
            timestamps=timestamps
        )
        
        # ä¿å­˜å›¾è¡¨è·¯å¾„ä¿¡æ¯
        results_dir = Path(self.results_paths['output_dir'])
        results_dir.mkdir(exist_ok=True)
        
        chart_summary_file = Path(self.results_paths['charts_summary'])
        chart_summary_file.parent.mkdir(parents=True, exist_ok=True)
        with open(chart_summary_file, 'w', encoding='utf-8') as f:
            f.write("å­¦æœ¯çº§å¯è§†åŒ–å›¾è¡¨ç”ŸæˆæŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ç”Ÿæˆå›¾è¡¨æ•°é‡: {len(charts)}\n\n")
            
            for chart_name, chart_path in charts.items():
                f.write(f"â€¢ {chart_name}: {chart_path}\n")
        
        logger.info(f"ğŸ‰ å­¦æœ¯çº§å›¾è¡¨ç”Ÿæˆå®Œæˆï¼Œå…± {len(charts)} ä¸ªå›¾è¡¨")
        
        return charts
    
    def optimize_hyperparameters(self, documents: List[str]) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            ä¼˜åŒ–ç»“æœå­—å…¸
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨è¶…å‚æ•°ä¼˜åŒ–
        opt_config = self.config.get('hyperparameter_optimization', {})
        if not opt_config.get('enable', False):
            logger.info("è¶…å‚æ•°ä¼˜åŒ–æœªå¯ç”¨ï¼Œè·³è¿‡ä¼˜åŒ–æ­¥éª¤")
            return None
        
        logger.info("ğŸ” å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–...")
        
        # è·å–ä¼˜åŒ–é…ç½®
        n_trials = opt_config.get('n_trials', 50)
        study_name = opt_config.get('study_name', 'auto')
        if study_name == 'auto':
            study_name = f"bertopic_opt_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # æ‰§è¡Œä¼˜åŒ–
        optimization_results = self.hyperparameter_optimizer.optimize_hyperparameters(
            documents=documents,
            n_trials=n_trials,
            study_name=study_name,
            save_results=opt_config.get('save_intermediate', True)
        )
        
        # æ˜¾ç¤ºä¼˜åŒ–ç»“æœ
        self._display_optimization_results(optimization_results)
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦ä½¿ç”¨æœ€ä½³å‚æ•°
        if opt_config.get('auto_apply_best', False):
            logger.info("ğŸš€ è‡ªåŠ¨åº”ç”¨æœ€ä½³å‚æ•°è¿›è¡Œåˆ†æ")
            self._apply_optimized_parameters(optimization_results['best_params'])
        else:
            logger.info("ğŸ’¡ è¶…å‚æ•°ä¼˜åŒ–å®Œæˆï¼Œå¯é€šè¿‡é…ç½®å¯ç”¨æœ€ä½³å‚æ•°")
        
        return optimization_results
    
    def train_with_optimized_parameters(self, 
                                      documents: List[str],
                                      optimized_params: Optional[Dict[str, Any]] = None) -> Tuple[BERTopic, List[int]]:
        """
        ä½¿ç”¨ä¼˜åŒ–çš„å‚æ•°è®­ç»ƒæ¨¡å‹
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            optimized_params: ä¼˜åŒ–çš„å‚æ•°å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™å°è¯•åŠ è½½å·²ä¿å­˜çš„æœ€ä½³å‚æ•°
            
        Returns:
            topic_model: è®­ç»ƒå¥½çš„æ¨¡å‹
            topics: æ¯ä¸ªæ–‡æ¡£çš„ä¸»é¢˜ç¼–å·
        """
        if optimized_params is None:
            # å°è¯•åŠ è½½å·²ä¿å­˜çš„æœ€ä½³å‚æ•°
            optimized_params = self.hyperparameter_optimizer.load_best_parameters()
            if optimized_params is None:
                logger.warning("æœªæ‰¾åˆ°ä¼˜åŒ–å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return self.train_bertopic_model(documents)
        
        logger.info("ğŸ¯ ä½¿ç”¨ä¼˜åŒ–å‚æ•°è®­ç»ƒæ¨¡å‹...")
        
        # ä¸´æ—¶åº”ç”¨ä¼˜åŒ–å‚æ•°
        # ä¸´æ—¶åº”ç”¨ä¼˜åŒ–å‚æ•°
        backup_config = self._backup_current_config()
        self._apply_optimized_parameters(optimized_params)

        try:
            topic_model, topics = self.train_bertopic_model(documents)
            return topic_model, topics
        finally:
            self._restore_config(backup_config)
    
    def _display_optimization_results(self, results: Dict[str, Any]) -> None:
        """æ˜¾ç¤ºä¼˜åŒ–ç»“æœ"""
        logger.info("ğŸ“Š è¶…å‚æ•°ä¼˜åŒ–ç»“æœ:")
        logger.info(f"   ğŸ† æœ€ä½³åˆ†æ•°: {results['best_score']:.4f}")
        logger.info(f"   â±ï¸  ä¼˜åŒ–æ—¶é—´: {results['optimization_time']:.1f} ç§’")
        logger.info(f"   ğŸ”¬ è¯•éªŒæ¬¡æ•°: {results['n_trials']}")
        
        logger.info("ğŸ” å‰5åå‚æ•°ç»„åˆ:")
        for result in results['top_5_results']:
            logger.info(f"   #{result['rank']}: åˆ†æ•°={result['score']:.4f}")
            logger.info(f"        min_cluster_size={result['params']['min_cluster_size']}, "
                       f"n_neighbors={result['params']['n_neighbors']}")
    
    def _apply_optimized_parameters(self, optimized_params: Dict[str, Any]) -> None:
        """åº”ç”¨ä¼˜åŒ–çš„å‚æ•°åˆ°é…ç½®ä¸­"""
        # æ›´æ–°UMAPå‚æ•°
        if 'n_neighbors' in optimized_params:
            self.config['bertopic_params']['umap_params']['n_neighbors'] = optimized_params['n_neighbors']
        if 'n_components' in optimized_params:
            self.config['bertopic_params']['umap_params']['n_components'] = optimized_params['n_components']
        if 'min_dist' in optimized_params:
            self.config['bertopic_params']['umap_params']['min_dist'] = optimized_params['min_dist']
        
        # æ›´æ–°HDBSCANå‚æ•°
        if 'min_cluster_size' in optimized_params:
            self.config['bertopic_params']['hdbscan_params']['min_cluster_size'] = optimized_params['min_cluster_size']
            self.config['bertopic_params']['min_topic_size'] = optimized_params['min_cluster_size']
        if 'min_samples' in optimized_params:
            self.config['bertopic_params']['hdbscan_params']['min_samples'] = optimized_params['min_samples']
        
        # æ›´æ–°N-gramå‚æ•°
        if 'ngram_range_start' in optimized_params and 'ngram_range_end' in optimized_params:
            self.config['bertopic_params']['n_gram_range'] = [
                optimized_params['ngram_range_start'], 
                optimized_params['ngram_range_end']
            ]
        
        logger.info("âœ… ä¼˜åŒ–å‚æ•°å·²åº”ç”¨åˆ°é…ç½®ä¸­")
    
    def _backup_current_config(self) -> Dict[str, Any]:
        """å¤‡ä»½å½“å‰é…ç½®"""
        import copy
        return copy.deepcopy(self.config)
    
    def _restore_config(self, backup_config: Dict[str, Any]) -> None:
        """æ¢å¤é…ç½®"""
        self.config = backup_config
        
    def generate_results(self, 
                        topic_model: BERTopic,
                        documents: List[str],
                        topics: List[int],
                        metadata_df: pd.DataFrame):
        """
        ç”Ÿæˆæ‰€æœ‰åˆ†æç»“æœ
        
        Args:
            topic_model: è®­ç»ƒå¥½çš„æ¨¡å‹
            documents: æ–‡æ¡£åˆ—è¡¨
            topics: ä¸»é¢˜åˆ—è¡¨
            metadata_df: å…ƒæ•°æ®
        """
        logger.info("ç”Ÿæˆåˆ†æç»“æœ...")
        
        # 1. å°†ä¸»é¢˜æ·»åŠ åˆ°å…ƒæ•°æ®
        metadata_df['topic'] = topics
        metadata_df['topic_label'] = [
            f"Topic {t}" if t != -1 else "Outlier" 
            for t in topics
        ]
        
        # 2. ç”Ÿæˆå¹¶ä¿å­˜ä¸»é¢˜æ‘˜è¦
        self.results_generator.save_topic_summary(topic_model)
        
        # 3. ç”Ÿæˆä¸»é¢˜å¯è§†åŒ–
        self.results_generator.generate_topic_visualization(topic_model, documents)
        
        # 4. ç”Ÿæˆæ¥æºåˆ†æï¼ˆå¦‚æœæœ‰Sourceåˆ—ï¼‰
        if 'Source' in metadata_df.columns:
            self.results_generator.generate_source_analysis(metadata_df, topic_model)
        
        # 5. ç”Ÿæˆæ—¶é—´æ¼”åŒ–åˆ†æï¼ˆå¦‚æœæœ‰æ—¥æœŸåˆ—ï¼‰
        if self.config['analysis']['time_analysis']['enable']:
            if 'æ—¥æœŸ' in metadata_df.columns:
                self.results_generator.generate_timeline_analysis(metadata_df, topic_model, documents)
        
        # 6. ç”Ÿæˆæ¡†æ¶çƒ­åŠ›å›¾ï¼ˆå¦‚æœæœ‰æ¡†æ¶åˆ—ï¼‰
        if self.config['analysis']['frame_analysis']['enable']:
            self.results_generator.generate_frame_heatmap(metadata_df)
        
        logger.info("âœ… æ‰€æœ‰åˆ†æç»“æœå·²ç”Ÿæˆ")
    
    def generate_enhanced_results(self, 
                                topic_model: BERTopic,
                                documents: List[str],
                                topics: List[int],
                                metadata_df: pd.DataFrame,
                                enhanced_topics: Optional[Dict] = None):
        """
        ç”Ÿæˆå¢å¼ºçš„åˆ†æç»“æœï¼Œé›†æˆæ‰€æœ‰æ–°æ¨¡å—
        
        Args:
            topic_model: è®­ç»ƒå¥½çš„æ¨¡å‹
            documents: æ–‡æ¡£åˆ—è¡¨
            topics: ä¸»é¢˜åˆ—è¡¨
            metadata_df: å…ƒæ•°æ®
            enhanced_topics: å¢å¼ºçš„ä¸»é¢˜è¡¨ç¤º
        """
        logger.info("ğŸš€ ç”Ÿæˆå¢å¼ºåˆ†æç»“æœ...")
        
        # 1. ç”ŸæˆåŸºç¡€ç»“æœ
        self.generate_results(topic_model, documents, topics, metadata_df)
        
        # 2. è·¨è¯­è¨€åˆ†æ
        logger.info("ğŸŒ æ‰§è¡Œè·¨è¯­è¨€ä¸»é¢˜æˆåˆ†åˆ†æ...")
        cross_lingual_results = self.cross_lingual_analyzer.run_full_cross_lingual_analysis(
            documents, topics
        )
        
        # 3. å­¦æœ¯çº§å›¾è¡¨ç”Ÿæˆ
        logger.info("ğŸ¨ ç”Ÿæˆå­¦æœ¯çº§å›¾è¡¨...")
        academic_charts = self.visualizer.generate_all_visualizations(
            topic_model=topic_model,
            documents=documents,
            topics=topics,
            metadata_df=metadata_df
        )
        
        # 4. åŠ¨æ€æ¼”åŒ–åˆ†æï¼ˆå¦‚æœæœ‰æ—¶é—´æ•°æ®ï¼‰
        evolution_results = {}
        timestamps = None
        if 'æ—¥æœŸ' in metadata_df.columns and self.config['analysis']['time_analysis']['enable']:
            logger.info("ğŸ• æ‰§è¡ŒåŠ¨æ€ä¸»é¢˜æ¼”åŒ–åˆ†æ...")
            
            # å‡†å¤‡æ—¶é—´æˆ³æ•°æ®
            try:
                import pandas as pd
                timestamps = pd.to_datetime(metadata_df['æ—¥æœŸ'], errors='coerce').dropna().tolist()
                logger.info(f"  âœ“ å‡†å¤‡æ—¶é—´æˆ³æ•°æ®: {len(timestamps)} ä¸ªæœ‰æ•ˆæ—¶é—´ç‚¹")
            except Exception as e:
                logger.warning(f"  âš  æ—¶é—´æˆ³æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
                timestamps = None
            
            evolution_results = self.evolution_analyzer.run_full_evolution_analysis(
                topic_model, documents, metadata_df
            )
            
            # ç”Ÿæˆæ¼”åŒ–å›¾è¡¨
            if evolution_results.get('topics_over_time') is not None:
                topics_over_time = evolution_results['topics_over_time']
                if not topics_over_time.empty:
                    # ä½¿ç”¨å¯è§†åŒ–ç”Ÿæˆå™¨çš„æ¼”åŒ–å›¾è¡¨åŠŸèƒ½
                    evolution_charts = self.visualizer.generate_all_visualizations(
                        topic_model=topic_model,
                        documents=documents,
                        topics=topics,
                        metadata_df=metadata_df,
                        timestamps=timestamps
                    )
                    academic_charts.update(evolution_charts)
        
        # 5. æ›´æ–°ä¸»é¢˜æ‘˜è¦æ–‡ä»¶ï¼Œé›†æˆå¢å¼ºä¿¡æ¯
        self._save_enhanced_topic_summary(
            topic_model, enhanced_topics, cross_lingual_results.get('composition_df')
        )
        
        # 6. ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
        self._generate_comprehensive_report(
            topic_model, cross_lingual_results, evolution_results, academic_charts
        )
        
        logger.info("âœ¨ å¢å¼ºåˆ†æç»“æœç”Ÿæˆå®Œæˆï¼")
    
    def _save_enhanced_topic_summary(self,
                                   topic_model: BERTopic,
                                   enhanced_topics: Optional[Dict] = None,
                                   composition_df: Optional[pd.DataFrame] = None):
        """ä¿å­˜å¢å¼ºçš„ä¸»é¢˜æ‘˜è¦"""
        topic_info = topic_model.get_topic_info()
        
        # æ·»åŠ å¢å¼ºçš„å…³é”®è¯
        if enhanced_topics:
            topic_info['Enhanced_Keywords'] = topic_info['Topic'].apply(
                lambda x: ', '.join([word for word, _ in enhanced_topics.get(x, [])[:5]])
                if x in enhanced_topics else ''
            )
        
        # æ·»åŠ è¯­è¨€æ„æˆä¿¡æ¯
        if composition_df is not None and not composition_df.empty:
            # åˆ›å»ºè¯­è¨€ä¿¡æ¯å­—å…¸
            lang_info = {}
            for _, row in composition_df.iterrows():
                topic_id = row['Topic']
                lang_info[topic_id] = {
                    'Dominant_Language': row.get('Dominant_Language', ''),
                    'Topic_Type': row.get('Topic_Type', ''),
                    'Chinese_Percentage': row.get('Chinese_Percentage', 0),
                    'Russian_Percentage': row.get('Russian_Percentage', 0),
                    'English_Percentage': row.get('English_Percentage', 0)
                }
            
            # æ·»åŠ åˆ°ä¸»é¢˜ä¿¡æ¯ä¸­
            topic_info['Dominant_Language'] = topic_info['Topic'].apply(
                lambda x: lang_info.get(x, {}).get('Dominant_Language', '')
            )
            topic_info['Topic_Type'] = topic_info['Topic'].apply(
                lambda x: lang_info.get(x, {}).get('Topic_Type', '')
            )
            topic_info['Chinese_Pct'] = topic_info['Topic'].apply(
                lambda x: lang_info.get(x, {}).get('Chinese_Percentage', 0)
            )
            topic_info['Russian_Pct'] = topic_info['Topic'].apply(
                lambda x: lang_info.get(x, {}).get('Russian_Percentage', 0)
            )
            topic_info['English_Pct'] = topic_info['Topic'].apply(
                lambda x: lang_info.get(x, {}).get('English_Percentage', 0)
            )
        
        # æ·»åŠ åŸå§‹å…³é”®è¯ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        topic_info['Original_Keywords'] = topic_info['Topic'].apply(
            lambda x: ', '.join([word for word, _ in topic_model.get_topic(x)[:5]])
            if x != -1 else 'Outlier'
        )
        
        # ä¿å­˜å¢å¼ºçš„æ‘˜è¦æ–‡ä»¶
        enhanced_summary_path = str(Path(self.results_paths['summary_file']).with_name(Path(self.results_paths['summary_file']).stem + '_enhanced.csv'))
        topic_info.to_csv(enhanced_summary_path, index=False, encoding='utf-8-sig')
        logger.info(f"  âœ“ å¢å¼ºä¸»é¢˜æ‘˜è¦å·²ä¿å­˜: {enhanced_summary_path}")
    
    def _generate_comprehensive_report(self,
                                     topic_model: BERTopic,
                                     cross_lingual_results: Dict,
                                     evolution_results: Dict,
                                     academic_charts: Dict):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        report_path = Path(self.results_paths.get('analysis_report', str(Path(self.results_paths['output_dir']) / 'ä¸»é¢˜åˆ†ææŠ¥å‘Š.txt')))
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write("BERTopic å¢å¼ºåˆ†æç»¼åˆæŠ¥å‘Š\n")
                f.write("=" * 80 + "\n\n")
                
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                # åŸºç¡€ç»Ÿè®¡
                topic_info = topic_model.get_topic_info()
                n_topics = len(topic_info) - 1
                f.write(f"ğŸ“Š åŸºç¡€ç»Ÿè®¡ä¿¡æ¯:\n")
                f.write(f"  è¯†åˆ«ä¸»é¢˜æ•°é‡: {n_topics} ä¸ª\n")
                f.write(f"  æ€»æ–‡æ¡£æ•°é‡: {topic_info['Count'].sum()} ä¸ª\n\n")
                
                # è·¨è¯­è¨€åˆ†æç»“æœ
                if cross_lingual_results.get('summary'):
                    summary = cross_lingual_results['summary']
                    f.write(f"ğŸŒ è·¨è¯­è¨€åˆ†æç»“æœ:\n")
                    f.write(f"  æ€»æ–‡æ¡£æ•°: {summary['total_documents']}\n")
                    f.write(f"  åˆ†æä¸»é¢˜æ•°: {summary['total_topics']}\n")
                    
                    lang_dist = summary['language_distribution']
                    f.write(f"  è¯­è¨€åˆ†å¸ƒ:\n")
                    f.write(f"    ä¸­æ–‡æ–‡æ¡£: {lang_dist['chinese']} ({lang_dist['chinese']/summary['total_documents']*100:.1f}%)\n")
                    f.write(f"    ä¿„æ–‡æ–‡æ¡£: {lang_dist['russian']} ({lang_dist['russian']/summary['total_documents']*100:.1f}%)\n")
                    f.write(f"    è‹±æ–‡æ–‡æ¡£: {lang_dist['english']} ({lang_dist['english']/summary['total_documents']*100:.1f}%)\n")
                    
                    f.write(f"  å…³é”®æ´å¯Ÿ:\n")
                    for insight in summary['key_insights']:
                        f.write(f"    â€¢ {insight}\n")
                    f.write("\n")
                
                # åŠ¨æ€æ¼”åŒ–åˆ†æç»“æœ
                if evolution_results.get('summary'):
                    evo_summary = evolution_results['summary']
                    f.write(f"ğŸ• åŠ¨æ€æ¼”åŒ–åˆ†æç»“æœ:\n")
                    f.write(f"  åˆ†ææ—¶é—´æ®µ: {evo_summary['analysis_period']}\n")
                    f.write(f"  æ—¶é—´ç‚¹æ•°é‡: {evo_summary['time_points']}\n")
                    f.write(f"  æ¼”åŒ–ä¸»é¢˜æ•°: {evo_summary['total_topics']}\n")
                    
                    if evolution_results.get('evolution_patterns'):
                        patterns = evolution_results['evolution_patterns']
                        f.write(f"  æ¼”åŒ–æ¨¡å¼:\n")
                        f.write(f"    ä¸Šå‡è¶‹åŠ¿ä¸»é¢˜: {len(patterns.get('rising_topics', []))} ä¸ª\n")
                        f.write(f"    ä¸‹é™è¶‹åŠ¿ä¸»é¢˜: {len(patterns.get('declining_topics', []))} ä¸ª\n")
                        f.write(f"    ç¨³å®šä¸»é¢˜: {len(patterns.get('stable_topics', []))} ä¸ª\n")
                        f.write(f"    æ³¢åŠ¨ä¸»é¢˜: {len(patterns.get('volatile_topics', []))} ä¸ª\n")
                    f.write("\n")
                
                # ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶
                if academic_charts:
                    f.write(f"ğŸ¨ ç”Ÿæˆçš„å­¦æœ¯çº§å›¾è¡¨:\n")
                    for chart_name, chart_path in academic_charts.items():
                        if chart_path:
                            f.write(f"  {chart_name}: {chart_path}\n")
                    f.write("\n")
                
                f.write("=" * 80 + "\n")
                f.write("æŠ¥å‘Šç»“æŸ\n")
                f.write("=" * 80 + "\n")
            
            logger.info(f"  âœ“ ç»¼åˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            
        except Exception as e:
            logger.error(f"  âœ— ç”Ÿæˆç»¼åˆæŠ¥å‘Šå¤±è´¥: {e}")
    
    def _create_embedding_model(self) -> SentenceTransformer:
        """åˆ›å»ºåµŒå…¥æ¨¡å‹"""
        model_name = self.model_params['embedding_model']
        logger.info(f"  â€¢ åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
        return SentenceTransformer(model_name)
    
    def _create_umap_model(self) -> UMAP:
        """åˆ›å»ºUMAPé™ç»´æ¨¡å‹"""
        params = self.model_params['umap_params']
        return UMAP(
            n_neighbors=params['n_neighbors'],
            n_components=params['n_components'],
            min_dist=params['min_dist'],
            metric=params['metric'],
            random_state=params.get('random_state', self.random_seed)
        )
    
    def _create_hdbscan_model(self) -> HDBSCAN:
        """åˆ›å»ºHDBSCANèšç±»æ¨¡å‹"""
        params = self.model_params['hdbscan_params']
        return HDBSCAN(
            min_cluster_size=params['min_cluster_size'],
            min_samples=params['min_samples'],
            metric=params['metric'],
            cluster_selection_method=params['cluster_selection_method'],
            prediction_data=params['prediction_data']
        )
    
    def _create_vectorizer(self) -> CountVectorizer:
        """åˆ›å»ºå¢å¼ºçš„å‘é‡åŒ–å™¨ï¼Œé›†æˆå¤šè¯­è¨€å¤„ç†å’Œä¸“å®¶çº§å…³é”®è¯æå–"""
        # æ£€æŸ¥é…ç½®ï¼Œå†³å®šä½¿ç”¨å“ªç§å‘é‡åŒ–å™¨
        advanced_preprocessing = self.config.get('bertopic_params', {}).get('expert_keyword_extraction', {})
        use_multilingual = advanced_preprocessing.get('enable_multilingual_preprocessing', True)
        
        if use_multilingual:
            # ä½¿ç”¨æ–°çš„å¤šè¯­è¨€å‘é‡åŒ–å™¨
            enhanced_vectorizer = self.multilingual_vectorizer.create_vectorizer()
            logger.info("  âœ“ åˆ›å»ºå¤šè¯­è¨€å¢å¼ºå‘é‡åŒ–å™¨")
        else:
            # ä½¿ç”¨åŸæœ‰çš„ä¸“å®¶çº§å…³é”®è¯æå–å™¨
            enhanced_vectorizer = self.expert_extractor.create_enhanced_vectorizer()
            logger.info("  âœ“ åˆ›å»ºä¸“å®¶çº§å¢å¼ºå‘é‡åŒ–å™¨")
        
        return enhanced_vectorizer
    
    def _create_representation_model(self):
        """åˆ›å»ºè¡¨ç¤ºæ¨¡å‹ï¼ˆSOTAï¼šç»„åˆå¤šç§æ–¹æ³•ï¼‰"""
        # KeyBERTé£æ ¼çš„å…³é”®è¯æå–
        keybert = KeyBERTInspired()
        
        # æœ€å¤§è¾¹é™…ç›¸å…³æ€§
        mmr = MaximalMarginalRelevance(diversity=0.3)
        
        # ç»„åˆå¤šç§è¡¨ç¤ºæ–¹æ³•
        return [keybert, mmr]
    
    def _save_model(self, topic_model: BERTopic) -> None:
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        model_dir = Path(self.results_paths['model_dir'])
        model_dir.mkdir(exist_ok=True, parents=True)
        model_path = model_dir / 'bertopic_model'
        topic_model.save(str(model_path), serialization="safetensors", save_ctfidf=True)
        logger.info(f"  âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    def _get_nr_topics(self) -> Optional[int]:
        """è·å–ä¸»é¢˜æ•°é‡è®¾ç½®"""
        nr_topics = self.model_params['nr_topics']
        if nr_topics == "auto" or nr_topics is None:
            return None
        return int(nr_topics)
    
    
    def _generate_source_analysis(self, 
                                 metadata_df: pd.DataFrame,
                                 topic_model: BERTopic):
        """ç”Ÿæˆæ¥æºåˆ†æå›¾è¡¨"""
        try:
            # å‡†å¤‡æ•°æ®
            source_topic_df = metadata_df.groupby(['Source', 'topic_label']).size().reset_index(name='count')
            
            # è¿‡æ»¤æ‰ç¦»ç¾¤ç‚¹
            source_topic_df = source_topic_df[source_topic_df['topic_label'] != 'Outlier']
            
            # åˆ›å»ºå †å æŸ±çŠ¶å›¾
            fig = px.bar(
                source_topic_df, 
                x='Source', 
                y='count',
                color='topic_label',
                title='è®®é¢˜åœ¨ä¸åŒæ¥æºä¸­çš„åˆ†å¸ƒ',
                labels={'count': 'æ–‡æ¡£æ•°é‡', 'Source': 'æ¥æº', 'topic_label': 'ä¸»é¢˜'},
                color_discrete_sequence=px.colors.qualitative.Set3
            )
            
            fig.update_layout(
                xaxis_tickangle=-45,
                height=600,
                showlegend=True,
                legend=dict(
                    yanchor="top",
                    y=0.99,
                    xanchor="left",
                    x=0.01
                )
            )
            
            # ä¿å­˜
            source_path = self.results_paths['source_analysis']
            fig.write_html(source_path)
            logger.info(f"  âœ“ æ¥æºåˆ†æå·²ä¿å­˜: {source_path}")
            
        except Exception as e:
            logger.warning(f"  âš  æ¥æºåˆ†æå¤±è´¥: {e}")
    
    def _generate_temporal_analysis(self,
                                   topic_model: BERTopic,
                                   documents: List[str],
                                   metadata_df: pd.DataFrame):
        """ç”Ÿæˆæ—¶é—´æ¼”åŒ–åˆ†æ"""
        try:
            time_column = self.config['analysis']['time_analysis']['time_column']
            
            # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
            timestamps = pd.to_datetime(metadata_df[time_column])
            
            # ç”Ÿæˆä¸»é¢˜éšæ—¶é—´å˜åŒ–
            topics_over_time = topic_model.topics_over_time(
                documents,
                timestamps=timestamps,
                nr_bins=self.config['analysis']['time_analysis']['bins']
            )
            
            # åˆ›å»ºå¯è§†åŒ–
            fig = topic_model.visualize_topics_over_time(topics_over_time)
            
            # ä¿å­˜
            timeline_path = self.results_paths['timeline_analysis']
            fig.write_html(timeline_path)
            logger.info(f"  âœ“ æ—¶é—´æ¼”åŒ–åˆ†æå·²ä¿å­˜: {timeline_path}")
            
        except Exception as e:
            logger.warning(f"  âš  æ—¶é—´æ¼”åŒ–åˆ†æå¤±è´¥: {e}")
    
    def _generate_frame_heatmap(self, 
                               metadata_df: pd.DataFrame,
                               topic_model: BERTopic):
        """ç”Ÿæˆæ¡†æ¶çƒ­åŠ›å›¾"""
        try:
            # æŸ¥æ‰¾æ‰€æœ‰æ¡†æ¶åˆ—
            frame_columns = [col for col in metadata_df.columns 
                           if col.startswith('Frame_') and col.endswith('_Present')]
            
            if not frame_columns:
                logger.warning("  âš  æœªæ‰¾åˆ°æ¡†æ¶åˆ—ï¼Œè·³è¿‡æ¡†æ¶åˆ†æ")
                return
            
            # è®¡ç®—æ¯ä¸ªä¸»é¢˜çš„æ¡†æ¶ä½¿ç”¨é¢‘ç‡
            topic_frame_matrix = []
            topic_labels = []
            
            for topic in sorted(metadata_df['topic'].unique()):
                if topic == -1:  # è·³è¿‡ç¦»ç¾¤ç‚¹
                    continue
                    
                topic_data = metadata_df[metadata_df['topic'] == topic]
                frame_freq = topic_data[frame_columns].mean()
                topic_frame_matrix.append(frame_freq.values)
                topic_labels.append(f"Topic {topic}")
            
            # åˆ›å»ºçƒ­åŠ›å›¾æ•°æ®
            frame_names = [col.replace('Frame_', '').replace('_Present', '') 
                          for col in frame_columns]
            
            # ä½¿ç”¨plotlyåˆ›å»ºçƒ­åŠ›å›¾
            fig = go.Figure(data=go.Heatmap(
                z=topic_frame_matrix,
                x=frame_names,
                y=topic_labels,
                colorscale='RdBu_r',
                text=np.round(topic_frame_matrix, 2),
                texttemplate='%{text}',
                textfont={"size": 10},
                colorbar=dict(title="é¢‘ç‡")
            ))
            
            fig.update_layout(
                title='ä¸»é¢˜-æ¡†æ¶å…³è”çƒ­åŠ›å›¾',
                xaxis_title='å™äº‹æ¡†æ¶',
                yaxis_title='ä¸»é¢˜',
                height=max(400, len(topic_labels) * 30),
                xaxis={'tickangle': -45}
            )
            
            # ä¿å­˜
            heatmap_path = self.results_paths['frame_heatmap']
            fig.write_html(heatmap_path)
            logger.info(f"  âœ“ æ¡†æ¶çƒ­åŠ›å›¾å·²ä¿å­˜: {heatmap_path}")
            
        except Exception as e:
            logger.warning(f"  âš  æ¡†æ¶çƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {e}")