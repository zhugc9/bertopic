"""
BERTopicæ¨¡å‹è®­ç»ƒä¸åˆ†ææ¨¡å—
==========================
SOTAå®ç°ï¼šä½¿ç”¨æœ€æ–°çš„BERTopicç‰¹æ€§å’Œä¼˜åŒ–æŠ€æœ¯
"""

import pandas as pd
import numpy as np
import logging
import os
import json
import time
import re
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# BERTopicæ ¸å¿ƒ
from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance

# ä¸“å®¶çº§å…³é”®è¯æå–
from .expert_keywords import ExpertKeywordExtractor

# å­æ¨¡å—å¯¼å…¥
from .visualizations import VisualizationGenerator
from .dynamic_evolution import DynamicTopicEvolution
from .cross_lingual import CrossLingualAnalyzer
from .hyperparameter_optimizer import OptunaBERTopicOptimizer
from .multilingual_preprocessor import EnhancedMultilingualVectorizer
from .results_generator import ResultsGenerator

# é™ç»´å’Œèšç±»
from umap import UMAP
from hdbscan import HDBSCAN

# åµŒå…¥æ¨¡å‹
from sentence_transformers import SentenceTransformer

# å‘é‡åŒ–
from sklearn.feature_extraction.text import CountVectorizer

# å¯è§†åŒ–
import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
import matplotlib.pyplot as plt
import seaborn as sns

logger = logging.getLogger(__name__)


class TopicAnalyzer:
    """ä¸»é¢˜åˆ†æå™¨ç±» - SOTAå®ç°"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–ä¸»é¢˜åˆ†æå™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.model_params = config['bertopic_params']
        self.viz_params = config['visualization']
        self.results_paths = config['results_paths']
        
        # åˆå§‹åŒ–ä¸“å®¶çº§å…³é”®è¯æå–å™¨
        self.expert_extractor = ExpertKeywordExtractor(config)
        
        # åˆå§‹åŒ–å¯è§†åŒ–ç”Ÿæˆå™¨
        self.visualizer = VisualizationGenerator(config)
        
        # åˆå§‹åŒ–åŠ¨æ€æ¼”åŒ–åˆ†æå™¨
        self.evolution_analyzer = DynamicTopicEvolution(config)
        
        # åˆå§‹åŒ–è·¨è¯­è¨€åˆ†æå™¨
        self.cross_lingual_analyzer = CrossLingualAnalyzer(config)

        self._console_prefix = "[TopicAnalyzer]"
        
        # åˆå§‹åŒ–è¶…å‚æ•°ä¼˜åŒ–å™¨
        self.hyperparameter_optimizer = OptunaBERTopicOptimizer(config)
        
        # åˆå§‹åŒ–å¤šè¯­è¨€é¢„å¤„ç†å™¨
        self.multilingual_vectorizer = EnhancedMultilingualVectorizer(config)
        
        # åˆå§‹åŒ–ç»“æœç”Ÿæˆå™¨
        self.results_generator = ResultsGenerator(config)

        self.random_seed = self.config.get('system', {}).get('random_seed', 42)
        
        # è®¾ç½®matplotlibå¤šè¯­è¨€æ”¯æŒï¼ˆä¸­æ–‡ã€ä¿„æ–‡ã€è‹±æ–‡ï¼‰
        plt.rcParams.update({
            'font.sans-serif': ['Microsoft YaHei', 'SimHei', 'Arial', 'DejaVu Sans', 'sans-serif'],
            'font.family': 'sans-serif',
            'axes.unicode_minus': False,
            'pdf.fonttype': 42,  # TrueTypeå­—ä½“åµŒå…¥PDF
            'ps.fonttype': 42    # PostScriptå­—ä½“åµŒå…¥PS
        })
        
    def train_bertopic_model(self, 
                           documents: List[str]) -> Tuple[BERTopic, List[int]]:
        """
        è®­ç»ƒBERTopicæ¨¡å‹
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            topic_model: è®­ç»ƒå¥½çš„æ¨¡å‹
            topics: æ¯ä¸ªæ–‡æ¡£çš„ä¸»é¢˜ç¼–å·
        """
        logger.info("åˆå§‹åŒ–BERTopicç»„ä»¶...")
        
        # 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆSOTAï¼šå¤šè¯­è¨€æ”¯æŒï¼‰
        embedding_model = self._create_embedding_model()
        
        # 2. åˆå§‹åŒ–UMAPé™ç»´ï¼ˆSOTAä¼˜åŒ–å‚æ•°ï¼‰
        umap_model = self._create_umap_model()
        
        # 3. åˆå§‹åŒ–HDBSCANèšç±»ï¼ˆSOTAä¼˜åŒ–å‚æ•°ï¼‰
        hdbscan_model = self._create_hdbscan_model()
        
        # 4. åˆå§‹åŒ–å‘é‡åŒ–å™¨ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰
        vectorizer_model = self._create_vectorizer()
        
        # 5. åˆå§‹åŒ–è¡¨ç¤ºæ¨¡å‹ï¼ˆSOTAï¼šç»„åˆå¤šç§è¡¨ç¤ºæ–¹æ³•ï¼‰
        representation_model = self._create_representation_model()
        
        # 6. åˆ›å»ºBERTopicæ¨¡å‹
        topic_model = BERTopic(
            embedding_model=embedding_model,
            umap_model=umap_model,
            hdbscan_model=hdbscan_model,
            vectorizer_model=vectorizer_model,
            representation_model=representation_model,
            nr_topics=self._get_nr_topics(),
            top_n_words=10,
            verbose=self.config['system']['verbose'],
            calculate_probabilities=True  # SOTAï¼šè®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        )
        
        # 7. è®­ç»ƒæ¨¡å‹
        self._announce("[2/3] å¼€å§‹æ ¸å¿ƒæ¨¡å‹è®­ç»ƒ")
        logger.info(f"å¼€å§‹è®­ç»ƒæ¨¡å‹ (æ–‡æ¡£æ•°: {len(documents)})...")
        topics, probs = topic_model.fit_transform(documents)
        self._announce("[2/3] æ ¸å¿ƒæ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # 8. ä¿å­˜æ¨¡å‹
        self._save_model(topic_model)
        
        # ç»Ÿè®¡ä¿¡æ¯
        topic_info = topic_model.get_topic_info()
        n_topics = len(topic_info) - 1  # å‡å»ç¦»ç¾¤ç‚¹
        logger.info(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ: å‘ç° {n_topics} ä¸ªä¸»é¢˜")
        
        return topic_model, topics

    def _announce(self, message: str, level: str = "info") -> None:
        console_message = f"{self._console_prefix} {message}"
        print(console_message, flush=True)
        if level == "info":
            logger.info(message)
        elif level == "warning":
            logger.warning(message)
        else:
            logger.error(message)
    
    def analyze_document_languages(self, documents: List[str]) -> Dict[str, Any]:
        """
        åˆ†ææ–‡æ¡£çš„è¯­è¨€åˆ†å¸ƒ
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            è¯­è¨€åˆ†æç»“æœ
        """
        logger.info("ğŸŒ åˆ†ææ–‡æ¡£è¯­è¨€åˆ†å¸ƒ...")
        
        language_stats = self.multilingual_vectorizer.get_language_statistics(documents)
        
        # æ˜¾ç¤ºåˆ†æç»“æœ
        logger.info("ğŸ“Š è¯­è¨€åˆ†å¸ƒç»Ÿè®¡:")
        for lang, stats in language_stats['language_distribution'].items():
            lang_names = {'zh': 'ä¸­æ–‡', 'en': 'è‹±æ–‡', 'ru': 'ä¿„æ–‡', 'unknown': 'æœªçŸ¥'}
            lang_name = lang_names.get(lang, lang)
            logger.info(f"  {lang_name}: {stats['count']} ä¸ªæ–‡æ¡£ ({stats['percentage']:.1f}%)")
        
        return language_stats
    
    def generate_sota_visualizations(self, 
                                    topic_model: BERTopic,
                                    documents: List[str],
                                    topics: List[int],
                                    metadata_df: Optional[pd.DataFrame] = None) -> Dict[str, str]:
        """
        ç”ŸæˆSOTAçº§å­¦æœ¯å¯è§†åŒ–å›¾è¡¨
        
        Args:
            topic_model: è®­ç»ƒå¥½çš„æ¨¡å‹
            documents: æ–‡æ¡£åˆ—è¡¨
            topics: ä¸»é¢˜åˆ†é…
            metadata_df: å…ƒæ•°æ®DataFrame
            
        Returns:
            ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        import traceback
        logger.info("ğŸ¨ ç”ŸæˆSOTAçº§å­¦æœ¯å¯è§†åŒ–å›¾è¡¨...")
        
        try:
            # å‡†å¤‡æ—¶é—´æ•°æ®
            timestamps = None
            if metadata_df is not None:
                # å°è¯•ä»å…ƒæ•°æ®ä¸­æå–æ—¶é—´ä¿¡æ¯
                date_columns = ['æ—¥æœŸ', 'Date', 'date', 'timestamp', 'time']
                for col in date_columns:
                    if col in metadata_df.columns:
                        try:
                            timestamps = pd.to_datetime(metadata_df[col], errors='coerce').tolist()
                            logger.info(f"âœ“ ä»'{col}'åˆ—æå–äº†æ—¶é—´æˆ³æ•°æ®")
                            break
                        except Exception as e:
                            logger.warning(f"ä»'{col}'åˆ—æå–æ—¶é—´æˆ³å¤±è´¥: {e}")
                            continue
            
            # ç”Ÿæˆæ‰€æœ‰SOTAå›¾è¡¨
            logger.info("â†’ è°ƒç”¨å¯è§†åŒ–ç”Ÿæˆå™¨...")
            charts = self.visualizer.generate_all_visualizations(
                topic_model=topic_model,
                documents=documents,
                topics=topics,
                metadata_df=metadata_df,
                timestamps=timestamps
            )
            
            logger.info(f"âœ… å­¦æœ¯çº§å›¾è¡¨ç”Ÿæˆå®Œæˆï¼Œå…± {len(charts)} ä¸ªå›¾è¡¨")
            return charts
            
        except Exception as e:
            logger.error(f"âŒ SOTAå¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {type(e).__name__}: {e}")
            logger.error(f"è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
            return {}
    
    def optimize_hyperparameters(self, documents: List[str]) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            ä¼˜åŒ–ç»“æœå­—å…¸
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨è¶…å‚æ•°ä¼˜åŒ–
        opt_config = self.config.get('hyperparameter_optimization', {})
        if not opt_config.get('enable', False):
            logger.info("è¶…å‚æ•°ä¼˜åŒ–æœªå¯ç”¨ï¼Œè·³è¿‡ä¼˜åŒ–æ­¥éª¤")
            return None
        
        logger.info("ğŸ” å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–...")
        
        # è·å–ä¼˜åŒ–é…ç½®
        n_trials = opt_config.get('n_trials', 50)
        study_name = opt_config.get('study_name', 'auto')
        if study_name == 'auto':
            study_name = f"bertopic_opt_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # æ‰§è¡Œä¼˜åŒ–
        optimization_results = self.hyperparameter_optimizer.optimize_hyperparameters(
            documents=documents,
            n_trials=n_trials,
            study_name=study_name,
            save_results=opt_config.get('save_intermediate', True)
        )
        
        # æ˜¾ç¤ºä¼˜åŒ–ç»“æœ
        self._display_optimization_results(optimization_results)
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦ä½¿ç”¨æœ€ä½³å‚æ•°
        if opt_config.get('auto_apply_best', False):
            logger.info("è‡ªåŠ¨åº”ç”¨æœ€ä½³å‚æ•°è¿›è¡Œåˆ†æ")
            self._apply_optimized_parameters(optimization_results['best_params'])
        else:
            logger.info("ğŸ’¡ è¶…å‚æ•°ä¼˜åŒ–å®Œæˆï¼Œå¯é€šè¿‡é…ç½®å¯ç”¨æœ€ä½³å‚æ•°")
        
        return optimization_results
    
    def train_with_optimized_parameters(self, 
                                      documents: List[str],
                                      optimized_params: Optional[Dict[str, Any]] = None) -> Tuple[BERTopic, List[int]]:
        """
        ä½¿ç”¨ä¼˜åŒ–çš„å‚æ•°è®­ç»ƒæ¨¡å‹
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            optimized_params: ä¼˜åŒ–çš„å‚æ•°å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™å°è¯•åŠ è½½å·²ä¿å­˜çš„æœ€ä½³å‚æ•°
            
        Returns:
            topic_model: è®­ç»ƒå¥½çš„æ¨¡å‹
            topics: æ¯ä¸ªæ–‡æ¡£çš„ä¸»é¢˜ç¼–å·
        """
        if optimized_params is None:
            # å°è¯•åŠ è½½å·²ä¿å­˜çš„æœ€ä½³å‚æ•°
            optimized_params = self.hyperparameter_optimizer.load_best_parameters()
            if optimized_params is None:
                logger.warning("æœªæ‰¾åˆ°ä¼˜åŒ–å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return self.train_bertopic_model(documents)
        
        logger.info("ğŸ¯ ä½¿ç”¨ä¼˜åŒ–å‚æ•°è®­ç»ƒæ¨¡å‹...")
        
        # ä¸´æ—¶åº”ç”¨ä¼˜åŒ–å‚æ•°
        # ä¸´æ—¶åº”ç”¨ä¼˜åŒ–å‚æ•°
        backup_config = self._backup_current_config()
        self._apply_optimized_parameters(optimized_params)

        try:
            topic_model, topics = self.train_bertopic_model(documents)
            return topic_model, topics
        finally:
            self._restore_config(backup_config)
    
    def _display_optimization_results(self, results: Dict[str, Any]) -> None:
        """æ˜¾ç¤ºä¼˜åŒ–ç»“æœ"""
        best_score = results.get('best_score')
        optimization_time = results.get('optimization_time')
        n_trials = results.get('n_trials')
        best_score_str = f"{best_score:.4f}" if isinstance(best_score, (int, float)) else str(best_score)
        optimization_time_str = f"{optimization_time:.1f}" if isinstance(optimization_time, (int, float)) else str(optimization_time)
        n_trials_str = str(n_trials) if n_trials is not None else "N/A"
        logger.info("ğŸ“Š è¶…å‚æ•°ä¼˜åŒ–ç»“æœ:")
        logger.info(f"   ğŸ† æœ€ä½³åˆ†æ•°: {best_score_str}")
        logger.info(f"   â±ï¸  ä¼˜åŒ–æ—¶é—´: {optimization_time_str}")
        logger.info(f"   ğŸ”¬ è¯•éªŒæ¬¡æ•°: {n_trials_str}")

        top_results = results.get('top_5_results', [])

        logger.info("ğŸ” å‰5åå‚æ•°ç»„åˆ:")
        for result in top_results:
            params = result.get('params', {})
            score = result.get('score')
            score_str = f"{score:.4f}" if isinstance(score, (int, float)) else str(score)
            logger.info(f"   #{result.get('rank', 'N/A')}: åˆ†æ•°={score_str}")
            logger.info(f"        min_cluster_size={params.get('min_cluster_size')}, "
                       f"n_neighbors={params.get('n_neighbors')}")
    
    def _apply_optimized_parameters(self, optimized_params: Dict[str, Any]) -> None:
        """åº”ç”¨ä¼˜åŒ–çš„å‚æ•°åˆ°é…ç½®ä¸­"""
        # æ›´æ–°UMAPå‚æ•°
        if 'n_neighbors' in optimized_params:
            self.config['bertopic_params']['umap_params']['n_neighbors'] = optimized_params['n_neighbors']
        if 'n_components' in optimized_params:
            self.config['bertopic_params']['umap_params']['n_components'] = optimized_params['n_components']
        if 'min_dist' in optimized_params:
            self.config['bertopic_params']['umap_params']['min_dist'] = optimized_params['min_dist']
        
        # æ›´æ–°HDBSCANå‚æ•°
        if 'min_cluster_size' in optimized_params:
            self.config['bertopic_params']['hdbscan_params']['min_cluster_size'] = optimized_params['min_cluster_size']
            self.config['bertopic_params']['min_topic_size'] = optimized_params['min_cluster_size']
        if 'min_samples' in optimized_params:
            self.config['bertopic_params']['hdbscan_params']['min_samples'] = optimized_params['min_samples']
        
        # æ›´æ–°N-gramå‚æ•°
        if 'ngram_range_start' in optimized_params and 'ngram_range_end' in optimized_params:
            self.config['bertopic_params']['n_gram_range'] = [
                optimized_params['ngram_range_start'], 
                optimized_params['ngram_range_end']
            ]
        
        logger.info("âœ… ä¼˜åŒ–å‚æ•°å·²åº”ç”¨åˆ°é…ç½®ä¸­")
    
    def _backup_current_config(self) -> Dict[str, Any]:
        """å¤‡ä»½å½“å‰é…ç½®"""
        import copy
        return copy.deepcopy(self.config)
    
    def _restore_config(self, backup_config: Dict[str, Any]) -> None:
        """æ¢å¤é…ç½®"""
        self.config = backup_config
        
    def generate_results(self, 
                        topic_model: BERTopic,
                        documents: List[str],
                        topics: List[int],
                        metadata_df: pd.DataFrame):
        """
        ç”Ÿæˆæ‰€æœ‰åˆ†æç»“æœ
        
        Args:
            topic_model: è®­ç»ƒå¥½çš„æ¨¡å‹
            documents: æ–‡æ¡£åˆ—è¡¨
            topics: ä¸»é¢˜åˆ—è¡¨
            metadata_df: å…ƒæ•°æ®
        """
        self._generate_results_enhanced(topic_model, documents, topics, metadata_df)

    def _generate_results_enhanced(self,
                                   topic_model: BERTopic,
                                   documents: List[str],
                                   topics: List[int],
                                   metadata_df: pd.DataFrame) -> None:
        import traceback
        logger.info("ç”Ÿæˆå¢å¼ºç‰ˆåˆ†æç»“æœ...")

        try:
            metadata_df['topic'] = topics
            metadata_df['topic_label'] = [
                f"ä¸»é¢˜ {t}" if t != -1 else "ç¦»ç¾¤ç‚¹"
                for t in topics
            ]
            logger.info("âœ“ å…ƒæ•°æ®å‡†å¤‡å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ å…ƒæ•°æ®å‡†å¤‡å¤±è´¥: {e}\n{traceback.format_exc()}")
            raise

        # åŸºç¡€ç»“æœ
        try:
            logger.info("â†’ ä¿å­˜ä¸»é¢˜æ‘˜è¦è¡¨...")
            self.results_generator.save_topic_summary(topic_model)
            logger.info("âœ“ ä¸»é¢˜æ‘˜è¦è¡¨å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ä¸»é¢˜æ‘˜è¦è¡¨å¤±è´¥: {e}\n{traceback.format_exc()}")
            raise

        try:
            logger.info("â†’ ä¿å­˜æ–‡æ¡£ä¸»é¢˜æ˜ å°„...")
            self.results_generator.save_document_topic_mapping(documents, topics, metadata_df)
            logger.info("âœ“ æ–‡æ¡£ä¸»é¢˜æ˜ å°„å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡æ¡£ä¸»é¢˜æ˜ å°„å¤±è´¥: {e}\n{traceback.format_exc()}")
            raise

        try:
            logger.info("â†’ ç”Ÿæˆä¸»é¢˜å¯è§†åŒ–...")
            self.results_generator.generate_topic_visualization(topic_model, documents)
            logger.info("âœ“ ä¸»é¢˜å¯è§†åŒ–å·²ç”Ÿæˆ")
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆä¸»é¢˜å¯è§†åŒ–å¤±è´¥: {e}\n{traceback.format_exc()}")
            # å¯è§†åŒ–å¤±è´¥ä¸åº”è¯¥ä¸­æ–­æ•´ä¸ªæµç¨‹
            logger.warning("âš  å¯è§†åŒ–å¤±è´¥ï¼Œç»§ç»­å…¶ä»–æ­¥éª¤...")

        try:
            if 'Source' in metadata_df.columns:
                logger.info("â†’ ç”Ÿæˆæ¥æºåˆ†æ...")
                self.results_generator.generate_source_analysis(metadata_df, topic_model)
                logger.info("âœ“ æ¥æºåˆ†æå·²ç”Ÿæˆ")
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ¥æºåˆ†æå¤±è´¥: {e}\n{traceback.format_exc()}")

        try:
            if self.config['analysis']['time_analysis']['enable'] and 'æ—¥æœŸ' in metadata_df.columns:
                logger.info("â†’ ç”Ÿæˆæ—¶é—´çº¿åˆ†æ...")
                self.results_generator.generate_timeline_analysis(metadata_df, topic_model, documents)
                logger.info("âœ“ æ—¶é—´çº¿åˆ†æå·²ç”Ÿæˆ")
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ—¶é—´çº¿åˆ†æå¤±è´¥: {e}\n{traceback.format_exc()}")

        try:
            if self.config['analysis']['frame_analysis']['enable']:
                logger.info("â†’ ç”Ÿæˆæ¡†æ¶çƒ­åŠ›å›¾...")
                self.results_generator.generate_frame_heatmap(metadata_df)
                logger.info("âœ“ æ¡†æ¶çƒ­åŠ›å›¾å·²ç”Ÿæˆ")
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ¡†æ¶çƒ­åŠ›å›¾å¤±è´¥: {e}\n{traceback.format_exc()}")

        # å¢å¼ºæ¨¡å—
        outputs_config = self.config.get('outputs', {})

        cross_lingual_results = {}
        try:
            if outputs_config.get('cross_lingual', True):
                logger.info("â†’ è¿è¡Œè·¨è¯­è¨€åˆ†æ...")
                cross_lingual_results = self.cross_lingual_analyzer.run_full_cross_lingual_analysis(documents, topics)
                logger.info("âœ“ è·¨è¯­è¨€åˆ†æå®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ è·¨è¯­è¨€åˆ†æå¤±è´¥: {e}\n{traceback.format_exc()}")

        evolution_results = {}
        try:
            if (outputs_config.get('comprehensive_report', True) or outputs_config.get('cross_lingual', True)) and \
                    self.config['analysis']['time_analysis']['enable'] and 'æ—¥æœŸ' in metadata_df.columns:
                logger.info("â†’ è¿è¡Œä¸»é¢˜æ¼”åŒ–åˆ†æ...")
                evolution_results = self.evolution_analyzer.run_full_evolution_analysis(topic_model, documents, metadata_df)
                logger.info("âœ“ ä¸»é¢˜æ¼”åŒ–åˆ†æå®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ ä¸»é¢˜æ¼”åŒ–åˆ†æå¤±è´¥: {e}\n{traceback.format_exc()}")

        try:
            if outputs_config.get('enhanced_keywords', True):
                logger.info("â†’ å¢å¼ºå…³é”®è¯æå–...")
                enhanced_topics = self.expert_extractor.enhance_topic_representation(topic_model, documents)
                logger.info("âœ“ å¢å¼ºå…³é”®è¯æå–å®Œæˆ")
            else:
                enhanced_topics = None
        except Exception as e:
            logger.error(f"âŒ å¢å¼ºå…³é”®è¯æå–å¤±è´¥: {e}\n{traceback.format_exc()}")
            enhanced_topics = None

        try:
            if outputs_config.get('enhanced_summary', True):
                logger.info("â†’ ç”Ÿæˆå¢å¼ºä¸»é¢˜æ‘˜è¦...")
                self._save_enhanced_topic_summary(
                    topic_model, enhanced_topics, cross_lingual_results.get('composition_df') if cross_lingual_results else None
                )
                logger.info("âœ“ å¢å¼ºä¸»é¢˜æ‘˜è¦å·²ç”Ÿæˆ")
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå¢å¼ºä¸»é¢˜æ‘˜è¦å¤±è´¥: {e}\n{traceback.format_exc()}")

        try:
            if outputs_config.get('comprehensive_report', True):
                logger.info("â†’ ç”Ÿæˆç»¼åˆæŠ¥å‘Š...")
                self._generate_comprehensive_report(
                    topic_model,
                    cross_lingual_results if cross_lingual_results else {},
                    evolution_results,
                    {}
                )
                logger.info("âœ“ ç»¼åˆæŠ¥å‘Šå·²ç”Ÿæˆ")
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç»¼åˆæŠ¥å‘Šå¤±è´¥: {e}\n{traceback.format_exc()}")

        try:
            logger.info("â†’ ç”ŸæˆAIä¸»é¢˜æ ‡ç­¾...")
            self._generate_ai_topic_labels(topic_model)
            logger.info("âœ“ AIä¸»é¢˜æ ‡ç­¾ç”Ÿæˆæµç¨‹å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ AIä¸»é¢˜æ ‡ç­¾ç”Ÿæˆå¤±è´¥: {type(e).__name__}: {e}")
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            # ç¡®ä¿é”™è¯¯ä¿¡æ¯ä¹Ÿè¾“å‡ºåˆ°æ§åˆ¶å°
            print(f"AI ä¸»é¢˜æ ‡ç­¾ç”Ÿæˆå¤±è´¥: {type(e).__name__}: {e}")
            print(f"è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
        
        logger.info("âœ¨ åˆ†æç»“æœå·²ç”Ÿæˆ")

    def _generate_ai_topic_labels(self, topic_model: BERTopic) -> None:
        """åœ¨åˆ†ææµç¨‹ä¸­å°è¯•ç”ŸæˆAIä¸»é¢˜æ ‡ç­¾"""
        ai_config = self.config.get('ai_labeling', {})
        if not ai_config.get('enable', False):
            return

        api_cfg = self.config.get('ai_labeling_advanced', {}).get('api_config', {})
        api_keys = api_cfg.get('API_KEYS', [])
        base_url = api_cfg.get('BASE_URL')
        model_name = api_cfg.get('MODEL')

        if not api_keys or not api_keys[0] or api_keys[0] == 'YOUR_API_KEY_HERE':
            logger.warning("AI ä¸»é¢˜æ ‡ç­¾æœªç”Ÿæˆï¼šAPI key æœªé…ç½®")
            return
        if not base_url or not model_name:
            logger.warning("AI ä¸»é¢˜æ ‡ç­¾æœªç”Ÿæˆï¼šAPI base æˆ– model æœªé…ç½®")
            return

        try:
            # ç›´æ¥è¯»å–ä¸»é¢˜æ‘˜è¦è¡¨
            summary_path = Path(self.results_paths['summary_file'])
            if not summary_path.exists():
                logger.warning("AI ä¸»é¢˜æ ‡ç­¾æœªç”Ÿæˆï¼šæœªæ‰¾åˆ°ä¸»é¢˜æ‘˜è¦è¡¨")
                return

            df = pd.read_csv(summary_path, encoding='utf-8')
            if 'Top_Words' not in df.columns:
                logger.warning("AI ä¸»é¢˜æ ‡ç­¾æœªç”Ÿæˆï¼šæ‘˜è¦è¡¨ä¸­ç¼ºå°‘Top_Wordsåˆ—")
                return
                
            labels = self._call_ai_labeler(df)
            if labels is None:
                logger.warning("AI ä¸»é¢˜æ ‡ç­¾ç”Ÿæˆå¤±è´¥ï¼šAPIè¿”å›ç©ºç»“æœ")
                return

            # åˆ†ç¦»æ ‡ç­¾å’Œè¯¦ç»†åˆ†æ
            label_list = []
            meaning_list = []
            discourse_list = []
            uniqueness_list = []
            
            for label_data in labels:
                if isinstance(label_data, dict):
                    label_list.append(label_data.get('topic_label', ''))
                    meaning_list.append(label_data.get('core_meaning', ''))
                    discourse = label_data.get('typical_discourse', [])
                    if isinstance(discourse, list):
                        discourse_list.append('; '.join(discourse))
                    else:
                        discourse_list.append(str(discourse) if discourse else '')
                    uniqueness_list.append(label_data.get('uniqueness', ''))
                else:
                    # å…¼å®¹æ—§æ ¼å¼ï¼ˆçº¯å­—ç¬¦ä¸²ï¼‰
                    label_list.append(str(label_data) if label_data else '')
                    meaning_list.append('')
                    discourse_list.append('')
                    uniqueness_list.append('')
            
            df['AI_Label'] = label_list
            df['AI_CoreMeaning'] = meaning_list
            df['AI_TypicalDiscourse'] = discourse_list
            df['AI_Uniqueness'] = uniqueness_list
            
            output_path = summary_path.with_name(summary_path.stem + '_å¸¦AIæ ‡ç­¾.csv')
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            logger.info(f"AI ä¸»é¢˜æ ‡ç­¾å·²ç”Ÿæˆ: {output_path}")
            logger.info(f"  åŒ…å«å­—æ®µï¼šæ ‡ç­¾ã€æ ¸å¿ƒå†…æ¶µã€è¯è¯­ç‰¹å¾ã€ç‹¬ç‰¹æ€§")
        except Exception as exc:
            import traceback
            logger.warning(f"AI ä¸»é¢˜æ ‡ç­¾ç”Ÿæˆå¤±è´¥: {type(exc).__name__}: {exc}")
            logger.debug(f"è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")

    def _call_ai_labeler(self, df: pd.DataFrame) -> Optional[List[str]]:
        """è°ƒç”¨è¯­è¨€æ¨¡å‹ä¸ºä¸»é¢˜ç”Ÿæˆæ ‡ç­¾"""
        try:
            import openai
        except ImportError:
            logger.warning("è¯·å…ˆå®‰è£… openai åº“ä»¥å¯ç”¨ä¸»é¢˜å‘½åï¼špip install openai")
            return None

        ai_config = self.config.get('ai_labeling', {})
        ai_advanced = self.config.get('ai_labeling_advanced', {})
        api_cfg = ai_advanced.get('api_config', {})
        label_settings = ai_advanced.get('label_settings', {})
        prompt_style = ai_advanced.get('prompt_style', 'academic')
        prompt_templates = ai_advanced.get('prompt_templates', {})

        prompt_template = prompt_templates.get(
            prompt_style,
            'åŸºäºå…³é”®è¯ï¼š{keywords}ï¼Œç”Ÿæˆ{length}çš„{style}ä¸­æ–‡æ ‡ç­¾ã€‚è¾“å‡ºï¼š{"topic_label": "æ ‡ç­¾"}'
        )

        length = label_settings.get('length', '8-12ä¸ªæ±‰å­—')
        style = label_settings.get('style', 'å­¦æœ¯åŒ–ç®€æ´')
        request_delay = label_settings.get('request_delay', 0.5)
        max_retries = label_settings.get('max_retries', 3)
        timeout = label_settings.get('timeout', 60)

        api_key_raw = api_cfg.get('API_KEYS', [''])[0]
        if api_key_raw.startswith('${') and api_key_raw.endswith('}'):  # ç¯å¢ƒå˜é‡
            env_var = api_key_raw[2:-1]
            api_key = os.getenv(env_var, '')
        else:
            api_key = api_key_raw

        if not api_key:
            logger.warning("AI ä¸»é¢˜æ ‡ç­¾æœªç”Ÿæˆï¼šæœªæ‰¾åˆ°æœ‰æ•ˆçš„ API key")
            return None

        base_url = api_cfg.get('BASE_URL')
        model_name = api_cfg.get('MODEL')
        
        # æ‰“å°é…ç½®ä¿¡æ¯ï¼ˆç”¨äºè¯Šæ–­ï¼‰
        logger.info("ğŸ¤– å¼€å§‹AIä¸»é¢˜æ ‡ç­¾ç”Ÿæˆ...")
        logger.info(f"  APIåœ°å€: {base_url}")
        logger.info(f"  æ¨¡å‹: {model_name}")
        logger.info(f"  ä¸»é¢˜æ•°é‡: {len(df)}")
        logger.info(f"  é‡è¯•æ¬¡æ•°: {max_retries}")
        logger.info(f"  è¶…æ—¶è®¾ç½®: {timeout}ç§’")
        
        # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
        print(f"\nğŸ¤– å¼€å§‹AIä¸»é¢˜æ ‡ç­¾ç”Ÿæˆ...")
        print(f"  API: {base_url}")
        print(f"  æ¨¡å‹: {model_name}")
        print(f"  ä¸»é¢˜æ•°é‡: {len(df)}")
        print(f"  å°†ä¸ºæ¯ä¸ªä¸»é¢˜ç”Ÿæˆä¸­æ–‡æ ‡ç­¾...\n")
        
        client = openai.OpenAI(api_key=api_key, base_url=base_url)

        labels: List[str] = []
        for idx, row in df.iterrows():
            try:
                # è¯»å–æ‰€æœ‰å¯ç”¨å­—æ®µ
                top_words = row.get('Top_Words', '')
                representation = row.get('Representation', '')
                enhanced_keywords = row.get('Enhanced_Keywords', '')
                representative_docs = row.get('Representative_Docs', '')
                
                # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
                if not isinstance(top_words, str):
                    top_words = str(top_words)
                if not isinstance(representation, str):
                    representation = str(representation)
                if not isinstance(enhanced_keywords, str):
                    enhanced_keywords = str(enhanced_keywords)
                if not isinstance(representative_docs, str):
                    representative_docs = str(representative_docs)
                
                # é™åˆ¶ representative_docs é•¿åº¦ï¼ˆå–å‰2000å­—ï¼‰
                if len(representative_docs) > 2000:
                    representative_docs = representative_docs[:2000] + "..."

                topic_id = row.get('Topic', idx)
                logger.info(f"  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
                logger.info(f"  ã€å¼€å§‹å¤„ç†ã€‘ä¸»é¢˜ {topic_id} ({idx + 1}/{len(df)})")
                logger.info(f"  ã€å…³é”®è¯ã€‘: {top_words[:100]}")
                
                # ä½¿ç”¨æ–°çš„å‚æ•°æ ¼å¼
                prompt = prompt_template.format(
                    top_words=top_words,
                    representation=representation,
                    enhanced_keywords=enhanced_keywords,
                    representative_docs=representative_docs,
                    length=length,
                    style=style
                )
                logger.info(f"  ã€æç¤ºè¯é•¿åº¦ã€‘: {len(prompt)} å­—ç¬¦")
                
                label = self._request_label(client, model_name, prompt, max_retries, timeout)
                
                if label:
                    # æ˜¾ç¤ºæ ‡ç­¾é¢„è§ˆ
                    if isinstance(label, dict):
                        preview = label.get('topic_label', str(label)[:50])
                    else:
                        preview = str(label)[:50]
                    logger.info(f"  âœ… ã€æˆåŠŸã€‘ä¸»é¢˜ {topic_id} æ ‡ç­¾: {preview}")
                else:
                    logger.warning(f"  âŒ ã€å¤±è´¥ã€‘ä¸»é¢˜ {topic_id} æ ‡ç­¾ç”Ÿæˆå¤±è´¥ï¼Œå°†ä½¿ç”¨ç©ºå­—å…¸")
                
                labels.append(label if label else {})
                time.sleep(request_delay)
                
            except Exception as e:
                logger.error(f"  âŒ ã€å¼‚å¸¸ã€‘å¤„ç†ä¸»é¢˜ {topic_id} æ—¶å‘ç”Ÿé”™è¯¯: {type(e).__name__}: {e}")
                import traceback
                logger.error(f"  ã€å †æ ˆã€‘:\n{traceback.format_exc()}")
                labels.append("")  # å¤±è´¥æ—¶æ·»åŠ ç©ºå­—ç¬¦ä¸²
                continue

        # ç»Ÿè®¡æˆåŠŸæ•°é‡
        success_count = sum(1 for l in labels if l and (isinstance(l, dict) or isinstance(l, str)))
        logger.info(f"ğŸ‰ AIæ ‡ç­¾ç”Ÿæˆå®Œæˆ: {success_count}/{len(labels)} ä¸ªæˆåŠŸ")
        
        # ç»Ÿè®¡å®Œæ•´åˆ†ææ•°é‡ï¼ˆåŒ…å«æ ¸å¿ƒå†…æ¶µç­‰å­—æ®µï¼‰
        full_analysis_count = sum(1 for l in labels if isinstance(l, dict) and 'core_meaning' in l)
        if full_analysis_count > 0:
            logger.info(f"  å…¶ä¸­ {full_analysis_count} ä¸ªåŒ…å«è¯¦ç»†åˆ†æï¼ˆæ ¸å¿ƒå†…æ¶µã€è¯è¯­ç‰¹å¾ç­‰ï¼‰")
        
        return labels

    def _request_label(self, client, model_name: str, prompt: str, max_retries: int, timeout: int) -> str:
        """è°ƒç”¨ OpenAI API è·å–å•ä¸ªä¸»é¢˜æ ‡ç­¾"""
        for attempt in range(max_retries):
            try:
                # ç›´æ¥è°ƒç”¨ï¼Œä¸å¼ºåˆ¶JSONæ ¼å¼ï¼ˆè®©APIè‡ªç„¶è¿”å›ï¼‰
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "user", "content": prompt}],
                    timeout=timeout
                )
                content = response.choices[0].message.content.strip()
                
                # è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºå®Œæ•´çš„APIè¿”å›å†…å®¹
                logger.info(f"  ã€APIåŸå§‹è¿”å›ã€‘: {content}")
                print(f"  ã€APIåŸå§‹è¿”å›ã€‘: {content}")  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
                
                try:
                    # å…ˆå°è¯•æ¸…ç†å¯èƒ½çš„æ ¼å¼é—®é¢˜
                    content_cleaned = content.strip()
                    
                    # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                    if content_cleaned.startswith('```json'):
                        content_cleaned = content_cleaned[7:]
                    elif content_cleaned.startswith('```'):
                        content_cleaned = content_cleaned[3:]
                    if content_cleaned.endswith('```'):
                        content_cleaned = content_cleaned[:-3]
                    content_cleaned = content_cleaned.strip()
                    
                    logger.info(f"  ã€æ¸…ç†åå†…å®¹ã€‘: {content_cleaned[:200]}...")
                    
                    # å¦‚æœè¿”å›å†…å®¹ä¸æ˜¯JSONæ ¼å¼ï¼Œç›´æ¥æå–ä¸­æ–‡/ä¿„æ–‡
                    if not (content_cleaned.startswith('{') and content_cleaned.endswith('}')):
                        logger.info(f"  ã€è§£æç­–ç•¥ã€‘: éJSONæ ¼å¼ï¼Œç›´æ¥æå–æ–‡æœ¬")
                        # æå–ä¸­æ–‡æˆ–ä¿„æ–‡
                        match = re.search(r'[\u4e00-\u9fff\u0400-\u04FF]+', content_cleaned)
                        if match:
                            extracted = match.group(0)
                            logger.info(f"  ã€æå–æˆåŠŸã€‘: {extracted}")
                            return extracted
                        # å¦‚æœæ²¡æœ‰ä¸­ä¿„æ–‡ï¼Œè¿”å›æ¸…ç†åçš„å†…å®¹
                        cleaned = re.sub(r'["\'{}\[\]:,]', '', content_cleaned).strip()
                        if cleaned:
                            logger.info(f"  ã€æ¸…ç†åç»“æœã€‘: {cleaned[:50]}")
                            return cleaned[:50]  # é™åˆ¶é•¿åº¦
                        logger.warning(f"  ã€è­¦å‘Šã€‘: æ— æ³•ä»éJSONå†…å®¹ä¸­æå–æœ‰æ•ˆæ–‡æœ¬")
                        return ""
                    
                    # å°è¯•è§£æJSON
                    logger.info(f"  ã€è§£æç­–ç•¥ã€‘: å°è¯•JSONè§£æ")
                    data = json.loads(content_cleaned)
                    logger.info(f"  ã€JSONè§£ææˆåŠŸã€‘: keys={list(data.keys())}")
                    
                    # å¦‚æœæ˜¯å®Œæ•´çš„ä¸»é¢˜åˆ†æJSONï¼Œç›´æ¥è¿”å›æ•´ä¸ªå­—å…¸
                    if 'topic_label' in data or 'core_meaning' in data or 'typical_discourse' in data:
                        logger.info(f"  ã€è¿”å›å®Œæ•´JSONã€‘: åŒ…å«{len(data)}ä¸ªå­—æ®µ")
                        return data
                    
                    # å¦åˆ™å°è¯•å¤šç§å¯èƒ½çš„é”®åæå–æ ‡ç­¾
                    for key in ['topic_label', 'label', 'æ ‡ç­¾', 'ä¸»é¢˜æ ‡ç­¾', 'name', 'title']:
                        if key in data:
                            label = data[key]
                            if label and isinstance(label, str) and label.strip():
                                logger.info(f"  ã€æ ‡ç­¾æå–æˆåŠŸã€‘: key='{key}', value='{label.strip()}'")
                                return label.strip()
                    
                    # å¦‚æœæ‰¾ä¸åˆ°é¢„æœŸçš„é”®ï¼Œå°è¯•è·å–ä»»ä½•å­—ç¬¦ä¸²å€¼
                    logger.info(f"  ã€å°è¯•å¤‡ç”¨æå–ã€‘: ä»æ‰€æœ‰å€¼ä¸­æŸ¥æ‰¾")
                    for k, v in data.items():
                        if isinstance(v, str) and v.strip():
                            logger.info(f"  ã€ä½¿ç”¨å¤‡ç”¨keyã€‘: key='{k}', value='{v.strip()}'")
                            return v.strip()
                    
                    # JSONè§£ææˆåŠŸä½†æ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾
                    logger.warning(f"  ã€è­¦å‘Šã€‘: JSONä¸­æœªæ‰¾åˆ°æœ‰æ•ˆæ ‡ç­¾")
                    logger.warning(f"  ã€JSONå†…å®¹ã€‘: {data}")
                    
                except json.JSONDecodeError as je:
                    logger.warning(f"  ã€JSONè§£æå¤±è´¥ã€‘: {str(je)}")
                    logger.warning(f"  ã€åŸå§‹å†…å®¹ã€‘: {content}")
                    # JSONè§£æå¤±è´¥ï¼Œå°è¯•ç›´æ¥æå–ä¸­æ–‡/ä¿„æ–‡
                    logger.info(f"  ã€é™çº§ç­–ç•¥ã€‘: ç›´æ¥æå–ä¸­æ–‡/ä¿„æ–‡å­—ç¬¦")
                    match = re.search(r'[\u4e00-\u9fff\u0400-\u04FF]+', content)
                    if match:
                        extracted = match.group(0)
                        logger.info(f"  ã€æå–æˆåŠŸã€‘: {extracted}")
                        return extracted
                    
                except KeyError as ke:
                    logger.error(f"  ã€KeyErrorå¼‚å¸¸ã€‘: {str(ke)}")
                    logger.error(f"  ã€å¼‚å¸¸è¯¦æƒ…ã€‘: key={repr(ke.args[0])}")
                    logger.error(f"  ã€JSONæ•°æ®ã€‘: {data if 'data' in locals() else 'N/A'}")
                    logger.error(f"  ã€åŸå§‹å†…å®¹ã€‘: {content}")
                    
                except Exception as parse_err:
                    logger.warning(f"  ã€è§£æå¼‚å¸¸ã€‘: {type(parse_err).__name__}: {parse_err}")
                    logger.warning(f"  ã€åŸå§‹å†…å®¹ã€‘: {content}")
                    import traceback
                    logger.debug(f"  ã€å¼‚å¸¸å †æ ˆã€‘:\n{traceback.format_exc()}")

                # æœ€åçš„é™çº§ç­–ç•¥ï¼šç›´æ¥ä»æ–‡æœ¬ä¸­æå–
                logger.info(f"  ã€æœ€ç»ˆé™çº§ã€‘: å°è¯•æå–ä»»ä½•æœ‰æ•ˆæ–‡æœ¬")
                # æå–ä¸­æ–‡
                match = re.search(r'[\u4e00-\u9fff]+', content)
                if match:
                    extracted = match.group(0)
                    logger.info(f"  ã€ä¸­æ–‡æå–ã€‘: {extracted}")
                    return extracted
                
                # æå–ä¿„æ–‡
                match = re.search(r'[\u0400-\u04FF]+', content)
                if match:
                    extracted = match.group(0)
                    logger.info(f"  ã€ä¿„æ–‡æå–ã€‘: {extracted}")
                    return extracted
                
                # è¿”å›æ¸…ç†åçš„å†…å®¹
                if content:
                    cleaned = re.sub(r'["\'{}\[\]:,]', '', content).strip()
                    if cleaned:
                        logger.info(f"  ã€æ¸…ç†åè¿”å›ã€‘: {cleaned[:50]}")
                        return cleaned[:50]
                
                logger.warning(f"  ã€å¤±è´¥ã€‘: æ— æ³•ä»è¿”å›å†…å®¹ä¸­æå–ä»»ä½•æœ‰æ•ˆæ ‡ç­¾")
                return ""
                
            except Exception as exc:
                # è¯¦ç»†é”™è¯¯ä¿¡æ¯
                error_type = type(exc).__name__
                error_msg = str(exc)
                
                logger.error(f"  ã€APIè¯·æ±‚å¼‚å¸¸ã€‘ (å°è¯• {attempt + 1}/{max_retries})")
                logger.error(f"     ç±»å‹: {error_type}")
                logger.error(f"     ä¿¡æ¯: {error_msg}")
                
                # æ‰“å°å®Œæ•´å †æ ˆ
                import traceback
                logger.debug(f"     å †æ ˆ:\n{traceback.format_exc()}")
                
                # ç‰¹æ®Šé”™è¯¯æç¤º
                if "429" in error_msg or "rate" in error_msg.lower():
                    logger.warning(f"     â†’ APIé€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…åé‡è¯•")
                elif "401" in error_msg or "auth" in error_msg.lower():
                    logger.error(f"     â†’ APIè®¤è¯å¤±è´¥ï¼Œæ£€æŸ¥API_KEY")
                elif "timeout" in error_msg.lower():
                    logger.warning(f"     â†’ è¯·æ±‚è¶…æ—¶ï¼Œè€ƒè™‘å¢åŠ timeoutè®¾ç½®")
                
                if attempt == max_retries - 1:
                    logger.error(f"  ã€æœ€ç»ˆå¤±è´¥ã€‘: AIæ ‡ç­¾è¯·æ±‚å¤±è´¥ (å·²é‡è¯•{max_retries}æ¬¡)")
                    return ""
                
                wait_time = 2 ** attempt
                logger.info(f"     ç­‰å¾… {wait_time}ç§’ åé‡è¯•...")
                time.sleep(wait_time)

        return ""
    
    def _save_enhanced_topic_summary(self,
                                   topic_model: BERTopic,
                                   enhanced_topics: Optional[Dict] = None,
                                   composition_df: Optional[pd.DataFrame] = None):
        """ä¿å­˜å¢å¼ºçš„ä¸»é¢˜æ‘˜è¦ï¼ˆç›´æ¥ä¿å­˜ä¸ºä¸»é¢˜æ‘˜è¦è¡¨.csvï¼‰"""
        topic_info = topic_model.get_topic_info()
        
        # æ·»åŠ åŸºç¡€å…³é”®è¯ï¼ˆTop_Wordsåˆ—ï¼Œç”¨äºAIæ ‡ç­¾å’ŒåŸºæœ¬æŸ¥çœ‹ï¼‰
        topic_info['Top_Words'] = topic_info['Topic'].apply(
            lambda x: ', '.join([word for word, _ in topic_model.get_topic(x)[:5]])
            if x != -1 else 'Outlier'
        )
        
        # æ·»åŠ å¢å¼ºçš„å…³é”®è¯
        if enhanced_topics:
            topic_info['Enhanced_Keywords'] = topic_info['Topic'].apply(
                lambda x: ', '.join([word for word, _ in enhanced_topics.get(x, [])[:5]])
                if x in enhanced_topics else ''
            )
        
        # æ·»åŠ è¯­è¨€æ„æˆä¿¡æ¯
        if composition_df is not None and not composition_df.empty:
            # åˆ›å»ºè¯­è¨€ä¿¡æ¯å­—å…¸
            lang_info = {}
            for _, row in composition_df.iterrows():
                topic_id = row['Topic']
                lang_info[topic_id] = {
                    'Dominant_Language': row.get('Dominant_Language', ''),
                    'Topic_Type': row.get('Topic_Type', ''),
                    'Chinese_Percentage': row.get('Chinese_Percentage', 0),
                    'Russian_Percentage': row.get('Russian_Percentage', 0),
                    'English_Percentage': row.get('English_Percentage', 0)
                }
            
            # æ·»åŠ åˆ°ä¸»é¢˜ä¿¡æ¯ä¸­
            topic_info['Dominant_Language'] = topic_info['Topic'].apply(
                lambda x: lang_info.get(x, {}).get('Dominant_Language', '')
            )
            topic_info['Topic_Type'] = topic_info['Topic'].apply(
                lambda x: lang_info.get(x, {}).get('Topic_Type', '')
            )
            topic_info['Chinese_Pct'] = topic_info['Topic'].apply(
                lambda x: lang_info.get(x, {}).get('Chinese_Percentage', 0)
            )
            topic_info['Russian_Pct'] = topic_info['Topic'].apply(
                lambda x: lang_info.get(x, {}).get('Russian_Percentage', 0)
            )
            topic_info['English_Pct'] = topic_info['Topic'].apply(
                lambda x: lang_info.get(x, {}).get('English_Percentage', 0)
            )
        
        # ç›´æ¥ä¿å­˜ä¸ºä¸»é¢˜æ‘˜è¦è¡¨ï¼ˆè¦†ç›–åŸºç¡€ç‰ˆï¼‰
        summary_path = Path(self.results_paths['summary_file'])
        summary_path.parent.mkdir(parents=True, exist_ok=True)
        topic_info.to_csv(summary_path, index=False, encoding='utf-8-sig')
        logger.info(f"  âœ“ å¢å¼ºä¸»é¢˜æ‘˜è¦å·²ä¿å­˜: {summary_path}")
    
    def _generate_comprehensive_report(self,
                                     topic_model: BERTopic,
                                     cross_lingual_results: Dict,
                                     evolution_results: Dict,
                                     academic_charts: Dict):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        report_path = Path(self.results_paths.get('analysis_report', str(Path(self.results_paths['output_dir']) / '4-ä¸»é¢˜åˆ†ææŠ¥å‘Š.txt')))
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write("BERTopic å¢å¼ºåˆ†æç»¼åˆæŠ¥å‘Š\n")
                f.write("=" * 80 + "\n\n")
                
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                # åŸºç¡€ç»Ÿè®¡
                topic_info = topic_model.get_topic_info()
                n_topics = len(topic_info) - 1
                f.write(f"ğŸ“Š åŸºç¡€ç»Ÿè®¡ä¿¡æ¯:\n")
                f.write(f"  è¯†åˆ«ä¸»é¢˜æ•°é‡: {n_topics} ä¸ª\n")
                f.write(f"  æ€»æ–‡æ¡£æ•°é‡: {topic_info['Count'].sum()} ä¸ª\n\n")
                
                # è·¨è¯­è¨€åˆ†æç»“æœ
                if cross_lingual_results.get('summary'):
                    summary = cross_lingual_results['summary']
                    f.write(f"ğŸŒ è·¨è¯­è¨€åˆ†æç»“æœ:\n")
                    f.write(f"  æ€»æ–‡æ¡£æ•°: {summary['total_documents']}\n")
                    f.write(f"  åˆ†æä¸»é¢˜æ•°: {summary['total_topics']}\n")
                    
                    lang_dist = summary['language_distribution']
                    f.write(f"  è¯­è¨€åˆ†å¸ƒ:\n")
                    f.write(f"    ä¸­æ–‡æ–‡æ¡£: {lang_dist['chinese']} ({lang_dist['chinese']/summary['total_documents']*100:.1f}%)\n")
                    f.write(f"    ä¿„æ–‡æ–‡æ¡£: {lang_dist['russian']} ({lang_dist['russian']/summary['total_documents']*100:.1f}%)\n")
                    f.write(f"    è‹±æ–‡æ–‡æ¡£: {lang_dist['english']} ({lang_dist['english']/summary['total_documents']*100:.1f}%)\n")
                    
                    f.write(f"  å…³é”®æ´å¯Ÿ:\n")
                    for insight in summary['key_insights']:
                        f.write(f"    â€¢ {insight}\n")
                    f.write("\n")
                
                # åŠ¨æ€æ¼”åŒ–åˆ†æç»“æœ
                if evolution_results.get('summary'):
                    evo_summary = evolution_results['summary']
                    f.write(f"ğŸ• åŠ¨æ€æ¼”åŒ–åˆ†æç»“æœ:\n")
                    f.write(f"  åˆ†ææ—¶é—´æ®µ: {evo_summary['analysis_period']}\n")
                    f.write(f"  æ—¶é—´ç‚¹æ•°é‡: {evo_summary['time_points']}\n")
                    f.write(f"  æ¼”åŒ–ä¸»é¢˜æ•°: {evo_summary['total_topics']}\n")
                    
                    if evolution_results.get('evolution_patterns'):
                        patterns = evolution_results['evolution_patterns']
                        f.write(f"  æ¼”åŒ–æ¨¡å¼:\n")
                        f.write(f"    ä¸Šå‡è¶‹åŠ¿ä¸»é¢˜: {len(patterns.get('rising_topics', []))} ä¸ª\n")
                        f.write(f"    ä¸‹é™è¶‹åŠ¿ä¸»é¢˜: {len(patterns.get('declining_topics', []))} ä¸ª\n")
                        f.write(f"    ç¨³å®šä¸»é¢˜: {len(patterns.get('stable_topics', []))} ä¸ª\n")
                        f.write(f"    æ³¢åŠ¨ä¸»é¢˜: {len(patterns.get('volatile_topics', []))} ä¸ª\n")
                    f.write("\n")
                
                # ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶
                if academic_charts:
                    f.write(f"ğŸ¨ ç”Ÿæˆçš„å­¦æœ¯çº§å›¾è¡¨:\n")
                    for chart_name, chart_path in academic_charts.items():
                        if chart_path:
                            f.write(f"  {chart_name}: {chart_path}\n")
                    f.write("\n")
                
                f.write("=" * 80 + "\n")
                f.write("æŠ¥å‘Šç»“æŸ\n")
                f.write("=" * 80 + "\n")
            
            logger.info(f"  âœ“ ç»¼åˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            
        except Exception as e:
            logger.error(f"  âœ— ç”Ÿæˆç»¼åˆæŠ¥å‘Šå¤±è´¥: {e}")
    
    def _create_embedding_model(self) -> SentenceTransformer:
        """åˆ›å»ºåµŒå…¥æ¨¡å‹"""
        model_name = self.model_params['embedding_model']
        logger.info(f"  â€¢ åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
        return SentenceTransformer(model_name)
    
    def _create_umap_model(self) -> UMAP:
        """åˆ›å»ºUMAPé™ç»´æ¨¡å‹"""
        params = self.model_params['umap_params']
        return UMAP(
            n_neighbors=params['n_neighbors'],
            n_components=params['n_components'],
            min_dist=params['min_dist'],
            metric=params['metric'],
            random_state=params.get('random_state', self.random_seed)
        )
    
    def _create_hdbscan_model(self) -> HDBSCAN:
        """åˆ›å»ºHDBSCANèšç±»æ¨¡å‹"""
        params = self.model_params['hdbscan_params']
        return HDBSCAN(
            min_cluster_size=params['min_cluster_size'],
            min_samples=params['min_samples'],
            metric=params['metric'],
            cluster_selection_method=params['cluster_selection_method'],
            prediction_data=params['prediction_data']
        )
    
    def _create_vectorizer(self) -> CountVectorizer:
        """åˆ›å»ºå¢å¼ºçš„å‘é‡åŒ–å™¨ï¼Œé›†æˆå¤šè¯­è¨€å¤„ç†å’Œä¸“å®¶çº§å…³é”®è¯æå–"""
        # æ£€æŸ¥é…ç½®ï¼Œå†³å®šä½¿ç”¨å“ªç§å‘é‡åŒ–å™¨
        # æ–¹æ¡ˆAï¼šexpert_keywordsï¼ˆæ·±åº¦è¯æ€§åˆ†æï¼Œé«˜è´¨é‡ï¼Œé€‚åˆå­¦æœ¯ï¼‰
        # æ–¹æ¡ˆBï¼šmultilingual_preprocessorï¼ˆè½»é‡åˆ†è¯ï¼Œå¿«é€Ÿï¼‰
        advanced_config = self.config.get('bertopic_params', {})
        use_expert = advanced_config.get('use_expert_keywords', True)
        
        if use_expert:
            # æ–¹æ¡ˆAï¼šä½¿ç”¨ä¸“å®¶çº§å…³é”®è¯æå–å™¨ï¼ˆåŸºäºspaCyè¯æ€§æ ‡æ³¨ï¼‰
            enhanced_vectorizer = self.expert_extractor.create_enhanced_vectorizer()
            logger.info("  âœ“ åˆ›å»ºä¸“å®¶çº§å¢å¼ºå‘é‡åŒ–å™¨ï¼ˆè¯æ€§æ ‡æ³¨æ¨¡å¼ï¼‰")
        else:
            # æ–¹æ¡ˆBï¼šä½¿ç”¨è½»é‡çº§å¤šè¯­è¨€å‘é‡åŒ–å™¨
            enhanced_vectorizer = self.multilingual_vectorizer.create_vectorizer()
            logger.info("  âœ“ åˆ›å»ºè½»é‡çº§å¤šè¯­è¨€å‘é‡åŒ–å™¨")
        
        return enhanced_vectorizer
    
    def _create_representation_model(self):
        """åˆ›å»ºè¡¨ç¤ºæ¨¡å‹ï¼ˆSOTAï¼šç»„åˆå¤šç§æ–¹æ³•ï¼‰"""
        # KeyBERTé£æ ¼çš„å…³é”®è¯æå–
        keybert = KeyBERTInspired()
        
        # æœ€å¤§è¾¹é™…ç›¸å…³æ€§
        mmr = MaximalMarginalRelevance(diversity=0.3)
        
        # ç»„åˆå¤šç§è¡¨ç¤ºæ–¹æ³•
        return [keybert, mmr]
    
    def _save_model(self, topic_model: BERTopic) -> None:
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        import traceback
        model_dir = Path(self.results_paths['model_dir'])
        model_dir.mkdir(exist_ok=True, parents=True)
        model_path = model_dir / 'bertopic_model'
        
        try:
            # å°è¯•å®Œæ•´ä¿å­˜ï¼ˆåŒ…æ‹¬c-TF-IDFé…ç½®ï¼‰
            topic_model.save(str(model_path), serialization="safetensors", save_ctfidf=True)
            logger.info(f"  âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        except TypeError as e:
            # å¦‚æœé‡åˆ°JSONåºåˆ—åŒ–é”™è¯¯ï¼ˆnumpyç±»å‹ï¼‰ï¼Œå°è¯•ä¸ä¿å­˜c-TF-IDFé…ç½®
            if "is not JSON serializable" in str(e):
                logger.warning(f"  âš  ä¿å­˜c-TF-IDFé…ç½®å¤±è´¥ï¼ˆnumpyç±»å‹é—®é¢˜ï¼‰ï¼Œå°è¯•ç®€åŒ–ä¿å­˜...")
                try:
                    topic_model.save(str(model_path), serialization="safetensors", save_ctfidf=False)
                    logger.info(f"  âœ“ æ¨¡å‹å·²ä¿å­˜ï¼ˆä¸å«c-TF-IDFé…ç½®ï¼‰: {model_path}")
                except Exception as e2:
                    logger.warning(f"  âš  æ¨¡å‹ä¿å­˜å¤±è´¥ï¼Œç»§ç»­åˆ†ææµç¨‹: {e2}")
                    logger.debug(f"è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
            else:
                logger.warning(f"  âš  æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
                logger.debug(f"è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
        except Exception as e:
            logger.warning(f"  âš  æ¨¡å‹ä¿å­˜å¤±è´¥ï¼Œç»§ç»­åˆ†ææµç¨‹: {type(e).__name__}: {e}")
            logger.debug(f"è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
    
    def _get_nr_topics(self) -> Optional[int]:
        """è·å–ä¸»é¢˜æ•°é‡è®¾ç½®"""
        nr_topics = self.model_params['nr_topics']
        if nr_topics == "auto" or nr_topics is None:
            return None
        return int(nr_topics)